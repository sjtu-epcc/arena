
------------------------------------------------------------------
- (1/3) Profiling wide_resnet_1B with batch size: 256...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal_prune_no_prof/wide_resnet_1B_256.pkl`, creating it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
[TMP] The range of stage num is (min #stage -> max #stage): 2 -> 4
[I] Coarsening pipeline layers from 16 to 4...
[I] Coarsened scheme: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2))
[TMP] The range of device num per stage is (min #gpu -> max #gpu): 1 -> 2
- Profiling for submesh 2 (2, 2):

[TMP] Since submesh device num (4) > max gpu num per stage (2) or < min gpu num per stage (1), skip compiling and profiling on pruned stages and forge infeasible profiling results for them ...
--------------------------------------------------

- Profiling for submesh 1 (1, 2):
[TMP] Layer indices [0, 1, 2, 3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [0, 1, 2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
[TMP] Layer indices [8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.059, peak_memory=1.868 GB, invar_size=0.452 GB, outvar_size=0.598 GB, temp_buffer_size=0.818 GB, available_memory=35.446 GB)
result[(0, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.148, peak_memory=1.854 GB, invar_size=0.233 GB, outvar_size=0.838 GB, temp_buffer_size=0.784 GB, available_memory=35.446 GB)
result[(0, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.189, peak_memory=2.859 GB, invar_size=1.497 GB, outvar_size=0.447 GB, temp_buffer_size=1.362 GB, available_memory=35.242 GB)
result[(0, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.073, peak_memory=1.868 GB, invar_size=0.452 GB, outvar_size=0.598 GB, temp_buffer_size=0.818 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.328, peak_memory=3.099 GB, invar_size=1.294 GB, outvar_size=0.224 GB, temp_buffer_size=1.805 GB, available_memory=35.446 GB)
result[(0, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.189, peak_memory=2.889 GB, invar_size=1.527 GB, outvar_size=0.447 GB, temp_buffer_size=1.362 GB, available_memory=35.242 GB)
result[(4, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.074, peak_memory=1.955 GB, invar_size=1.150 GB, outvar_size=0.329 GB, temp_buffer_size=0.476 GB, available_memory=35.446 GB)
result[(4, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=1.359 GB, invar_size=0.665 GB, outvar_size=0.389 GB, temp_buffer_size=0.305 GB, available_memory=35.446 GB)
result[(4, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.238, peak_memory=4.219 GB, invar_size=2.569 GB, outvar_size=1.150 GB, temp_buffer_size=1.590 GB, available_memory=35.242 GB)
result[(4, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.069, peak_memory=1.544 GB, invar_size=0.796 GB, outvar_size=0.329 GB, temp_buffer_size=0.419 GB, available_memory=35.242 GB)
result[(4, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.231, peak_memory=2.349 GB, invar_size=1.598 GB, outvar_size=0.605 GB, temp_buffer_size=0.691 GB, available_memory=35.446 GB)
result[(4, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.211, peak_memory=2.595 GB, invar_size=1.860 GB, outvar_size=0.796 GB, temp_buffer_size=0.675 GB, available_memory=35.242 GB)
result[(8, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.175, peak_memory=4.471 GB, invar_size=3.330 GB, outvar_size=0.180 GB, temp_buffer_size=0.962 GB, available_memory=35.446 GB)
result[(8, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.121, peak_memory=2.481 GB, invar_size=1.710 GB, outvar_size=0.210 GB, temp_buffer_size=0.561 GB, available_memory=35.446 GB)
result[(8, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.298, peak_memory=4.496 GB, invar_size=3.569 GB, outvar_size=1.680 GB, temp_buffer_size=0.897 GB, available_memory=35.446 GB)
result[(8, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.558, peak_memory=11.821 GB, invar_size=6.808 GB, outvar_size=3.329 GB, temp_buffer_size=4.984 GB, available_memory=35.242 GB)
result[(8, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.121, peak_memory=2.481 GB, invar_size=1.710 GB, outvar_size=0.210 GB, temp_buffer_size=0.561 GB, available_memory=35.446 GB)
result[(8, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.302, peak_memory=4.499 GB, invar_size=3.569 GB, outvar_size=1.680 GB, temp_buffer_size=0.900 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 81.01 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
[TMP] Layer indices [1, 2, 3] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.050, peak_memory=1.772 GB, invar_size=0.099 GB, outvar_size=0.778 GB, temp_buffer_size=0.895 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.050, peak_memory=1.772 GB, invar_size=0.099 GB, outvar_size=0.778 GB, temp_buffer_size=0.895 GB, available_memory=35.242 GB)
result[(4, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.048, peak_memory=1.463 GB, invar_size=0.477 GB, outvar_size=0.419 GB, temp_buffer_size=0.567 GB, available_memory=35.446 GB)
result[(4, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.048, peak_memory=1.463 GB, invar_size=0.477 GB, outvar_size=0.419 GB, temp_buffer_size=0.567 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.158, peak_memory=3.140 GB, invar_size=0.966 GB, outvar_size=0.090 GB, temp_buffer_size=2.173 GB, available_memory=35.446 GB)
result[(8, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.054, peak_memory=1.372 GB, invar_size=0.793 GB, outvar_size=0.240 GB, temp_buffer_size=0.339 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.173, peak_memory=3.140 GB, invar_size=0.966 GB, outvar_size=0.090 GB, temp_buffer_size=2.173 GB, available_memory=35.242 GB)
result[(4, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.155, peak_memory=2.293 GB, invar_size=1.253 GB, outvar_size=0.477 GB, temp_buffer_size=0.921 GB, available_memory=35.242 GB)
result[(4, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.141, peak_memory=2.293 GB, invar_size=1.253 GB, outvar_size=0.477 GB, temp_buffer_size=0.921 GB, available_memory=35.446 GB)
result[(8, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.372 GB, invar_size=0.793 GB, outvar_size=0.240 GB, temp_buffer_size=0.339 GB, available_memory=35.242 GB)
result[(8, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.144, peak_memory=2.555 GB, invar_size=1.764 GB, outvar_size=0.793 GB, temp_buffer_size=0.730 GB, available_memory=35.446 GB)
result[(8, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.160, peak_memory=2.555 GB, invar_size=1.764 GB, outvar_size=0.793 GB, temp_buffer_size=0.730 GB, available_memory=35.242 GB)
result[(12, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.152, peak_memory=3.769 GB, invar_size=2.627 GB, outvar_size=0.120 GB, temp_buffer_size=1.022 GB, available_memory=35.446 GB)
result[(12, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.151, peak_memory=3.769 GB, invar_size=2.627 GB, outvar_size=0.120 GB, temp_buffer_size=1.022 GB, available_memory=35.446 GB)
result[(12, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.377, peak_memory=6.984 GB, invar_size=5.312 GB, outvar_size=2.626 GB, temp_buffer_size=1.612 GB, available_memory=35.242 GB)
result[(12, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.376, peak_memory=6.984 GB, invar_size=5.312 GB, outvar_size=2.626 GB, temp_buffer_size=1.612 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 46.11 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 1, 0) has been pruned...
[TMP] Stage (0, 6, 1, 1) has been pruned...
[TMP] Stage (0, 6, 1, 2) has been pruned...
[TMP] Stage (0, 8, 1, 0) has been pruned...
[TMP] Stage (0, 8, 1, 1) has been pruned...
[TMP] Stage (0, 8, 1, 2) has been pruned...
[TMP] Stage (0, 15, 2, 0) has been pruned...
[TMP] Stage (0, 15, 2, 1) has been pruned...
[TMP] Stage (0, 15, 2, 2) has been pruned...
[TMP] Stage (1, 3, 0, 0) has been pruned...
[TMP] Stage (1, 3, 0, 1) has been pruned...
[TMP] Stage (1, 4, 0, 0) has been pruned...
[TMP] Stage (1, 4, 0, 1) has been pruned...
[TMP] Stage (1, 6, 1, 0) has been pruned...
[TMP] Stage (1, 6, 1, 1) has been pruned...
[TMP] Stage (1, 6, 1, 2) has been pruned...
[TMP] Stage (1, 7, 1, 0) has been pruned...
[TMP] Stage (1, 7, 1, 1) has been pruned...
[TMP] Stage (1, 7, 1, 2) has been pruned...
[TMP] Stage (1, 8, 1, 0) has been pruned...
[TMP] Stage (1, 8, 1, 1) has been pruned...
[TMP] Stage (1, 8, 1, 2) has been pruned...
[TMP] Stage (1, 9, 1, 0) has been pruned...
[TMP] Stage (1, 9, 1, 1) has been pruned...
[TMP] Stage (1, 9, 1, 2) has been pruned...
[TMP] Stage (2, 4, 0, 0) has been pruned...
[TMP] Stage (2, 4, 0, 1) has been pruned...
[TMP] Stage (2, 5, 0, 0) has been pruned...
[TMP] Stage (2, 5, 0, 1) has been pruned...
[TMP] Stage (2, 7, 1, 0) has been pruned...
[TMP] Stage (2, 7, 1, 1) has been pruned...
[TMP] Stage (2, 7, 1, 2) has been pruned...
[TMP] Stage (2, 8, 1, 0) has been pruned...
[TMP] Stage (2, 8, 1, 1) has been pruned...
[TMP] Stage (2, 8, 1, 2) has been pruned...
[TMP] Stage (2, 9, 1, 0) has been pruned...
[TMP] Stage (2, 9, 1, 1) has been pruned...
[TMP] Stage (2, 9, 1, 2) has been pruned...
[TMP] Stage (2, 10, 1, 0) has been pruned...
[TMP] Stage (2, 10, 1, 1) has been pruned...
[TMP] Stage (2, 10, 1, 2) has been pruned...
[TMP] Stage (3, 5, 0, 0) has been pruned...
[TMP] Stage (3, 5, 0, 1) has been pruned...
[TMP] Stage (3, 6, 0, 0) has been pruned...
[TMP] Stage (3, 6, 0, 1) has been pruned...
[TMP] Stage (3, 9, 1, 0) has been pruned...
[TMP] Stage (3, 9, 1, 1) has been pruned...
[TMP] Stage (3, 9, 1, 2) has been pruned...
[TMP] Stage (3, 10, 1, 0) has been pruned...
[TMP] Stage (3, 10, 1, 1) has been pruned...
[TMP] Stage (3, 10, 1, 2) has been pruned...
[TMP] Stage (3, 11, 1, 0) has been pruned...
[TMP] Stage (3, 11, 1, 1) has been pruned...
[TMP] Stage (3, 11, 1, 2) has been pruned...
[TMP] Stage (4, 10, 1, 0) has been pruned...
[TMP] Stage (4, 10, 1, 1) has been pruned...
[TMP] Stage (4, 10, 1, 2) has been pruned...
[TMP] Stage (4, 12, 1, 0) has been pruned...
[TMP] Stage (4, 12, 1, 1) has been pruned...
[TMP] Stage (4, 12, 1, 2) has been pruned...
[TMP] Stage (5, 7, 0, 0) has been pruned...
[TMP] Stage (5, 7, 0, 1) has been pruned...
[TMP] Stage (5, 8, 0, 0) has been pruned...
[TMP] Stage (5, 8, 0, 1) has been pruned...
[TMP] Stage (5, 11, 1, 0) has been pruned...
[TMP] Stage (5, 11, 1, 1) has been pruned...
[TMP] Stage (5, 11, 1, 2) has been pruned...
[TMP] Stage (5, 12, 1, 0) has been pruned...
[TMP] Stage (5, 12, 1, 1) has been pruned...
[TMP] Stage (5, 12, 1, 2) has been pruned...
[TMP] Stage (5, 13, 1, 0) has been pruned...
[TMP] Stage (5, 13, 1, 1) has been pruned...
[TMP] Stage (5, 13, 1, 2) has been pruned...
[TMP] Stage (5, 14, 1, 0) has been pruned...
[TMP] Stage (5, 14, 1, 1) has been pruned...
[TMP] Stage (5, 14, 1, 2) has been pruned...
[TMP] Stage (6, 9, 0, 0) has been pruned...
[TMP] Stage (6, 9, 0, 1) has been pruned...
[TMP] Stage (6, 12, 1, 0) has been pruned...
[TMP] Stage (6, 12, 1, 1) has been pruned...
[TMP] Stage (6, 12, 1, 2) has been pruned...
[TMP] Stage (6, 13, 1, 0) has been pruned...
[TMP] Stage (6, 13, 1, 1) has been pruned...
[TMP] Stage (6, 13, 1, 2) has been pruned...
[TMP] Stage (6, 14, 1, 0) has been pruned...
[TMP] Stage (6, 14, 1, 1) has been pruned...
[TMP] Stage (6, 14, 1, 2) has been pruned...
[TMP] Stage (6, 15, 1, 0) has been pruned...
[TMP] Stage (6, 15, 1, 1) has been pruned...
[TMP] Stage (6, 15, 1, 2) has been pruned...
[TMP] Stage (7, 10, 0, 0) has been pruned...
[TMP] Stage (7, 10, 0, 1) has been pruned...
[TMP] Stage (7, 11, 0, 0) has been pruned...
[TMP] Stage (7, 11, 0, 1) has been pruned...
[TMP] Stage (7, 13, 1, 0) has been pruned...
[TMP] Stage (7, 13, 1, 1) has been pruned...
[TMP] Stage (7, 13, 1, 2) has been pruned...
[TMP] Stage (7, 14, 1, 0) has been pruned...
[TMP] Stage (7, 14, 1, 1) has been pruned...
[TMP] Stage (7, 14, 1, 2) has been pruned...
[TMP] Stage (7, 15, 1, 0) has been pruned...
[TMP] Stage (7, 15, 1, 1) has been pruned...
[TMP] Stage (7, 15, 1, 2) has been pruned...
[TMP] Stage (8, 12, 0, 0) has been pruned...
[TMP] Stage (8, 12, 0, 1) has been pruned...
[TMP] Stage (8, 14, 1, 0) has been pruned...
[TMP] Stage (8, 14, 1, 1) has been pruned...
[TMP] Stage (8, 14, 1, 2) has been pruned...
[TMP] Stage (9, 12, 0, 0) has been pruned...
[TMP] Stage (9, 12, 0, 1) has been pruned...
[TMP] Stage (9, 13, 0, 0) has been pruned...
[TMP] Stage (9, 13, 0, 1) has been pruned...
[TMP] Stage (9, 15, 1, 0) has been pruned...
[TMP] Stage (9, 15, 1, 1) has been pruned...
[TMP] Stage (9, 15, 1, 2) has been pruned...
[TMP] Stage (10, 13, 0, 0) has been pruned...
[TMP] Stage (10, 13, 0, 1) has been pruned...
[TMP] Stage (10, 14, 0, 0) has been pruned...
[TMP] Stage (10, 14, 0, 1) has been pruned...
[TMP] Stage (11, 14, 0, 0) has been pruned...
[TMP] Stage (11, 14, 0, 1) has been pruned...
[TMP] Stage (11, 15, 0, 0) has been pruned...
[TMP] Stage (11, 15, 0, 1) has been pruned...
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (1, 2)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 153.32 s
compilation time breakdown: {'stage-construction': '130.48', 'stage-construction-dp': '1.39', 'stage-construction-compilation': '10.38', 'stage-construction-profiling': '106.69'}
 - Compile (worker): 7.70 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 103.23 s

[23.586312770843506, 12.57194995880127, 12.651071071624756, 12.724809408187866, 12.556348085403442, 12.60491943359375, 12.420146703720093]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 65.501 s.
 - Average e2e iteration time: 13.100000381469727 s.
 - Total local training time: 62.95700454711914 s.
 - Average local iteration time: 12.5910005569458 s.
 - Max allocated memory among devices: 11.455 GB.
 - Compilation times:  {'stage-construction': 130.47631001472473, 'stage-construction-dp': 1.386612892150879, 'stage-construction-compilation': 10.382184505462646, 'stage-construction-profiling': 106.69408178329468}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_2_d`: 12.591460227966309
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal_prune_no_prof/wide_resnet_1B_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling wide_resnet_1B with batch size: 512...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal_prune_no_prof/wide_resnet_1B_512.pkl`, creating it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
[TMP] The range of stage num is (min #stage -> max #stage): 2 -> 4
[I] Coarsening pipeline layers from 16 to 4...
[I] Coarsened scheme: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2))
[TMP] The range of device num per stage is (min #gpu -> max #gpu): 1 -> 2
- Profiling for submesh 2 (2, 2):

[TMP] Since submesh device num (4) > max gpu num per stage (2) or < min gpu num per stage (1), skip compiling and profiling on pruned stages and forge infeasible profiling results for them ...
--------------------------------------------------

- Profiling for submesh 1 (1, 2):
[TMP] Layer indices [0, 1, 2, 3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [0, 1, 2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
[TMP] Layer indices [8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.110, peak_memory=2.768 GB, invar_size=0.456 GB, outvar_size=1.196 GB, temp_buffer_size=1.115 GB, available_memory=35.242 GB)
result[(0, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.312, peak_memory=4.511 GB, invar_size=2.100 GB, outvar_size=0.447 GB, temp_buffer_size=2.412 GB, available_memory=35.446 GB)
result[(0, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.305, peak_memory=3.358 GB, invar_size=0.242 GB, outvar_size=1.675 GB, temp_buffer_size=1.442 GB, available_memory=35.242 GB)
result[(0, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.768 GB, invar_size=0.456 GB, outvar_size=1.196 GB, temp_buffer_size=1.115 GB, available_memory=35.242 GB)
result[(0, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.324, peak_memory=4.502 GB, invar_size=2.159 GB, outvar_size=0.447 GB, temp_buffer_size=2.343 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.638, peak_memory=5.739 GB, invar_size=2.140 GB, outvar_size=0.224 GB, temp_buffer_size=3.599 GB, available_memory=35.446 GB)
result[(4, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.103, peak_memory=2.515 GB, invar_size=1.210 GB, outvar_size=0.658 GB, temp_buffer_size=0.647 GB, available_memory=35.242 GB)
result[(4, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.320, peak_memory=4.588 GB, invar_size=2.957 GB, outvar_size=1.210 GB, temp_buffer_size=1.511 GB, available_memory=35.446 GB)
result[(4, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.178, peak_memory=2.245 GB, invar_size=0.785 GB, outvar_size=0.778 GB, temp_buffer_size=0.682 GB, available_memory=35.242 GB)
result[(4, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.114, peak_memory=2.515 GB, invar_size=1.210 GB, outvar_size=0.658 GB, temp_buffer_size=0.647 GB, available_memory=35.242 GB)
result[(4, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.413, peak_memory=3.545 GB, invar_size=2.107 GB, outvar_size=0.665 GB, temp_buffer_size=1.318 GB, available_memory=35.446 GB)
result[(4, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.331, peak_memory=4.588 GB, invar_size=2.957 GB, outvar_size=1.210 GB, temp_buffer_size=1.511 GB, available_memory=35.242 GB)
result[(8, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.208, peak_memory=4.801 GB, invar_size=3.360 GB, outvar_size=0.359 GB, temp_buffer_size=1.083 GB, available_memory=35.446 GB)
result[(8, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.175, peak_memory=2.823 GB, invar_size=1.770 GB, outvar_size=0.419 GB, temp_buffer_size=0.634 GB, available_memory=35.446 GB)
result[(8, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.645, peak_memory=12.061 GB, invar_size=7.017 GB, outvar_size=3.359 GB, temp_buffer_size=4.984 GB, available_memory=35.242 GB)
result[(8, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.446, peak_memory=5.021 GB, invar_size=3.838 GB, outvar_size=1.709 GB, temp_buffer_size=1.123 GB, available_memory=35.446 GB)
result[(8, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.170, peak_memory=3.390 GB, invar_size=2.168 GB, outvar_size=0.419 GB, temp_buffer_size=0.803 GB, available_memory=35.242 GB)
result[(8, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.438, peak_memory=6.211 GB, invar_size=4.694 GB, outvar_size=2.167 GB, temp_buffer_size=1.458 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 104.60 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
[TMP] Layer indices [1, 2, 3] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=3.396 GB, invar_size=0.108 GB, outvar_size=1.555 GB, temp_buffer_size=1.733 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=3.396 GB, invar_size=0.108 GB, outvar_size=1.555 GB, temp_buffer_size=1.733 GB, available_memory=35.242 GB)
result[(4, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.086, peak_memory=2.528 GB, invar_size=0.597 GB, outvar_size=0.838 GB, temp_buffer_size=1.094 GB, available_memory=35.446 GB)
result[(4, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.086, peak_memory=2.528 GB, invar_size=0.597 GB, outvar_size=0.838 GB, temp_buffer_size=1.094 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=6.065 GB, invar_size=1.753 GB, outvar_size=0.090 GB, temp_buffer_size=4.312 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.316, peak_memory=6.065 GB, invar_size=1.753 GB, outvar_size=0.090 GB, temp_buffer_size=4.312 GB, available_memory=35.242 GB)
result[(8, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.082, peak_memory=1.919 GB, invar_size=0.853 GB, outvar_size=0.479 GB, temp_buffer_size=0.587 GB, available_memory=35.446 GB)
result[(4, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.270, peak_memory=3.949 GB, invar_size=1.791 GB, outvar_size=0.597 GB, temp_buffer_size=1.919 GB, available_memory=35.242 GB)
result[(8, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=1.919 GB, invar_size=0.853 GB, outvar_size=0.479 GB, temp_buffer_size=0.587 GB, available_memory=35.446 GB)
result[(8, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.248, peak_memory=3.243 GB, invar_size=2.064 GB, outvar_size=0.852 GB, temp_buffer_size=1.059 GB, available_memory=35.242 GB)
result[(4, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.250, peak_memory=3.949 GB, invar_size=1.791 GB, outvar_size=0.597 GB, temp_buffer_size=1.919 GB, available_memory=35.446 GB)
result[(8, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.255, peak_memory=3.243 GB, invar_size=2.064 GB, outvar_size=0.852 GB, temp_buffer_size=1.059 GB, available_memory=35.242 GB)
result[(12, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.190, peak_memory=4.092 GB, invar_size=2.687 GB, outvar_size=0.240 GB, temp_buffer_size=1.166 GB, available_memory=35.446 GB)
result[(12, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.189, peak_memory=4.092 GB, invar_size=2.687 GB, outvar_size=0.240 GB, temp_buffer_size=1.166 GB, available_memory=35.446 GB)
result[(12, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.493, peak_memory=7.418 GB, invar_size=5.492 GB, outvar_size=2.686 GB, temp_buffer_size=1.807 GB, available_memory=35.242 GB)
result[(12, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.500, peak_memory=7.418 GB, invar_size=5.492 GB, outvar_size=2.686 GB, temp_buffer_size=1.807 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 59.84 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 1, 0) has been pruned...
[TMP] Stage (0, 6, 1, 1) has been pruned...
[TMP] Stage (0, 6, 1, 2) has been pruned...
[TMP] Stage (0, 8, 1, 0) has been pruned...
[TMP] Stage (0, 8, 1, 1) has been pruned...
[TMP] Stage (0, 8, 1, 2) has been pruned...
[TMP] Stage (0, 15, 2, 0) has been pruned...
[TMP] Stage (0, 15, 2, 1) has been pruned...
[TMP] Stage (0, 15, 2, 2) has been pruned...
[TMP] Stage (1, 3, 0, 0) has been pruned...
[TMP] Stage (1, 3, 0, 1) has been pruned...
[TMP] Stage (1, 4, 0, 0) has been pruned...
[TMP] Stage (1, 4, 0, 1) has been pruned...
[TMP] Stage (1, 6, 1, 0) has been pruned...
[TMP] Stage (1, 6, 1, 1) has been pruned...
[TMP] Stage (1, 6, 1, 2) has been pruned...
[TMP] Stage (1, 7, 1, 0) has been pruned...
[TMP] Stage (1, 7, 1, 1) has been pruned...
[TMP] Stage (1, 7, 1, 2) has been pruned...
[TMP] Stage (1, 8, 1, 0) has been pruned...
[TMP] Stage (1, 8, 1, 1) has been pruned...
[TMP] Stage (1, 8, 1, 2) has been pruned...
[TMP] Stage (1, 9, 1, 0) has been pruned...
[TMP] Stage (1, 9, 1, 1) has been pruned...
[TMP] Stage (1, 9, 1, 2) has been pruned...
[TMP] Stage (2, 4, 0, 0) has been pruned...
[TMP] Stage (2, 4, 0, 1) has been pruned...
[TMP] Stage (2, 5, 0, 0) has been pruned...
[TMP] Stage (2, 5, 0, 1) has been pruned...
[TMP] Stage (2, 7, 1, 0) has been pruned...
[TMP] Stage (2, 7, 1, 1) has been pruned...
[TMP] Stage (2, 7, 1, 2) has been pruned...
[TMP] Stage (2, 8, 1, 0) has been pruned...
[TMP] Stage (2, 8, 1, 1) has been pruned...
[TMP] Stage (2, 8, 1, 2) has been pruned...
[TMP] Stage (2, 9, 1, 0) has been pruned...
[TMP] Stage (2, 9, 1, 1) has been pruned...
[TMP] Stage (2, 9, 1, 2) has been pruned...
[TMP] Stage (2, 10, 1, 0) has been pruned...
[TMP] Stage (2, 10, 1, 1) has been pruned...
[TMP] Stage (2, 10, 1, 2) has been pruned...
[TMP] Stage (3, 5, 0, 0) has been pruned...
[TMP] Stage (3, 5, 0, 1) has been pruned...
[TMP] Stage (3, 6, 0, 0) has been pruned...
[TMP] Stage (3, 6, 0, 1) has been pruned...
[TMP] Stage (3, 9, 1, 0) has been pruned...
[TMP] Stage (3, 9, 1, 1) has been pruned...
[TMP] Stage (3, 9, 1, 2) has been pruned...
[TMP] Stage (3, 10, 1, 0) has been pruned...
[TMP] Stage (3, 10, 1, 1) has been pruned...
[TMP] Stage (3, 10, 1, 2) has been pruned...
[TMP] Stage (3, 11, 1, 0) has been pruned...
[TMP] Stage (3, 11, 1, 1) has been pruned...
[TMP] Stage (3, 11, 1, 2) has been pruned...
[TMP] Stage (4, 10, 1, 0) has been pruned...
[TMP] Stage (4, 10, 1, 1) has been pruned...
[TMP] Stage (4, 10, 1, 2) has been pruned...
[TMP] Stage (4, 12, 1, 0) has been pruned...
[TMP] Stage (4, 12, 1, 1) has been pruned...
[TMP] Stage (4, 12, 1, 2) has been pruned...
[TMP] Stage (5, 7, 0, 0) has been pruned...
[TMP] Stage (5, 7, 0, 1) has been pruned...
[TMP] Stage (5, 8, 0, 0) has been pruned...
[TMP] Stage (5, 8, 0, 1) has been pruned...
[TMP] Stage (5, 11, 1, 0) has been pruned...
[TMP] Stage (5, 11, 1, 1) has been pruned...
[TMP] Stage (5, 11, 1, 2) has been pruned...
[TMP] Stage (5, 12, 1, 0) has been pruned...
[TMP] Stage (5, 12, 1, 1) has been pruned...
[TMP] Stage (5, 12, 1, 2) has been pruned...
[TMP] Stage (5, 13, 1, 0) has been pruned...
[TMP] Stage (5, 13, 1, 1) has been pruned...
[TMP] Stage (5, 13, 1, 2) has been pruned...
[TMP] Stage (5, 14, 1, 0) has been pruned...
[TMP] Stage (5, 14, 1, 1) has been pruned...
[TMP] Stage (5, 14, 1, 2) has been pruned...
[TMP] Stage (6, 9, 0, 0) has been pruned...
[TMP] Stage (6, 9, 0, 1) has been pruned...
[TMP] Stage (6, 12, 1, 0) has been pruned...
[TMP] Stage (6, 12, 1, 1) has been pruned...
[TMP] Stage (6, 12, 1, 2) has been pruned...
[TMP] Stage (6, 13, 1, 0) has been pruned...
[TMP] Stage (6, 13, 1, 1) has been pruned...
[TMP] Stage (6, 13, 1, 2) has been pruned...
[TMP] Stage (6, 14, 1, 0) has been pruned...
[TMP] Stage (6, 14, 1, 1) has been pruned...
[TMP] Stage (6, 14, 1, 2) has been pruned...
