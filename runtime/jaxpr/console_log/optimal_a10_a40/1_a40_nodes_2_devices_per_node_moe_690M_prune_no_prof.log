
------------------------------------------------------------------
- (1/3) Profiling moe_690M with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal_prune_no_prof/moe_690M_256.pkl`, updating/rewriting it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f7d96330df0>
    dtype = float16
    bias_init = zeros
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
[TMP] The range of stage num is (min #stage -> max #stage): 1 -> 2
[I] Coarsening pipeline layers from 16 to 4...
[I] Coarsened scheme: [[0, 1, 2], [3, 4, 5, 6], [7, 8, 9, 10, 11, 12], [13, 14, 15]]
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2))
[TMP] The range of device num per stage is (min #gpu -> max #gpu): 1 -> 2
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.082, peak_memory=2.222 GB, invar_size=1.260 GB, outvar_size=0.474 GB, temp_buffer_size=0.488 GB, available_memory=35.242 GB)
result[(0, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.224, peak_memory=4.522 GB, invar_size=3.121 GB, outvar_size=1.324 GB, temp_buffer_size=1.401 GB, available_memory=35.242 GB)
result[(0, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=1.972 GB, invar_size=0.630 GB, outvar_size=0.853 GB, temp_buffer_size=0.488 GB, available_memory=35.242 GB)
result[(0, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.275, peak_memory=4.026 GB, invar_size=2.177 GB, outvar_size=0.662 GB, temp_buffer_size=1.849 GB, available_memory=35.242 GB)
result[(0, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.080, peak_memory=1.911 GB, invar_size=0.698 GB, outvar_size=0.724 GB, temp_buffer_size=0.488 GB, available_memory=35.242 GB)
result[(0, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.221, peak_memory=3.660 GB, invar_size=2.247 GB, outvar_size=0.761 GB, temp_buffer_size=1.413 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 65.42 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
[TMP] Layer indices [0, 1, 2, 3, 4, 5] violates the layer coarsening rules, pruned.
[TMP] Layer indices [0, 1, 2, 3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [0, 1, 2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
[TMP] Layer indices [8, 9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.086, peak_memory=2.176 GB, invar_size=0.667 GB, outvar_size=0.532 GB, temp_buffer_size=0.977 GB, available_memory=35.242 GB)
result[(0, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.077, peak_memory=2.176 GB, invar_size=0.667 GB, outvar_size=0.532 GB, temp_buffer_size=0.977 GB, available_memory=35.242 GB)
result[(0, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.228, peak_memory=4.645 GB, invar_size=1.866 GB, outvar_size=0.667 GB, temp_buffer_size=2.779 GB, available_memory=35.242 GB)
result[(3, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.095, peak_memory=2.097 GB, invar_size=0.689 GB, outvar_size=0.658 GB, temp_buffer_size=0.750 GB, available_memory=35.242 GB)
result[(0, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.213, peak_memory=4.645 GB, invar_size=1.866 GB, outvar_size=0.667 GB, temp_buffer_size=2.779 GB, available_memory=35.242 GB)
result[(3, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=2.097 GB, invar_size=0.689 GB, outvar_size=0.658 GB, temp_buffer_size=0.750 GB, available_memory=35.242 GB)
result[(3, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.251, peak_memory=5.214 GB, invar_size=1.988 GB, outvar_size=0.689 GB, temp_buffer_size=3.179 GB, available_memory=35.242 GB)
result[(7, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.050, peak_memory=1.829 GB, invar_size=0.641 GB, outvar_size=0.439 GB, temp_buffer_size=0.750 GB, available_memory=35.242 GB)
result[(3, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.239, peak_memory=5.214 GB, invar_size=1.988 GB, outvar_size=0.689 GB, temp_buffer_size=3.179 GB, available_memory=35.242 GB)
result[(7, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.190, peak_memory=4.976 GB, invar_size=1.776 GB, outvar_size=0.704 GB, temp_buffer_size=3.152 GB, available_memory=35.242 GB)
result[(7, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.829 GB, invar_size=0.641 GB, outvar_size=0.439 GB, temp_buffer_size=0.750 GB, available_memory=35.242 GB)
result[(7, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.188, peak_memory=4.976 GB, invar_size=1.776 GB, outvar_size=0.704 GB, temp_buffer_size=3.152 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 44.10 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 5, 0, 0) has been pruned...
[TMP] Stage (0, 5, 0, 1) has been pruned...
[TMP] Stage (0, 7, 0, 0) has been pruned...
[TMP] Stage (0, 7, 0, 1) has been pruned...
[TMP] Stage (0, 8, 0, 0) has been pruned...
[TMP] Stage (0, 8, 0, 1) has been pruned...
[TMP] Stage (1, 6, 0, 0) has been pruned...
[TMP] Stage (1, 6, 0, 1) has been pruned...
[TMP] Stage (1, 7, 0, 0) has been pruned...
[TMP] Stage (1, 7, 0, 1) has been pruned...
[TMP] Stage (1, 8, 0, 0) has been pruned...
[TMP] Stage (1, 8, 0, 1) has been pruned...
[TMP] Stage (1, 9, 0, 0) has been pruned...
[TMP] Stage (1, 9, 0, 1) has been pruned...
[TMP] Stage (2, 6, 0, 0) has been pruned...
[TMP] Stage (2, 6, 0, 1) has been pruned...
[TMP] Stage (2, 7, 0, 0) has been pruned...
[TMP] Stage (2, 7, 0, 1) has been pruned...
[TMP] Stage (2, 8, 0, 0) has been pruned...
[TMP] Stage (2, 8, 0, 1) has been pruned...
[TMP] Stage (2, 9, 0, 0) has been pruned...
[TMP] Stage (2, 9, 0, 1) has been pruned...
[TMP] Stage (2, 10, 0, 0) has been pruned...
[TMP] Stage (2, 10, 0, 1) has been pruned...
[TMP] Stage (2, 11, 0, 0) has been pruned...
[TMP] Stage (2, 11, 0, 1) has been pruned...
[TMP] Stage (2, 12, 0, 0) has been pruned...
[TMP] Stage (2, 12, 0, 1) has been pruned...
[TMP] Stage (3, 9, 0, 0) has been pruned...
[TMP] Stage (3, 9, 0, 1) has been pruned...
[TMP] Stage (3, 10, 0, 0) has been pruned...
[TMP] Stage (3, 10, 0, 1) has been pruned...
[TMP] Stage (3, 11, 0, 0) has been pruned...
[TMP] Stage (3, 11, 0, 1) has been pruned...
[TMP] Stage (3, 13, 0, 0) has been pruned...
[TMP] Stage (3, 13, 0, 1) has been pruned...
[TMP] Stage (4, 10, 0, 0) has been pruned...
[TMP] Stage (4, 10, 0, 1) has been pruned...
[TMP] Stage (4, 11, 0, 0) has been pruned...
[TMP] Stage (4, 11, 0, 1) has been pruned...
[TMP] Stage (4, 12, 0, 0) has been pruned...
[TMP] Stage (4, 12, 0, 1) has been pruned...
[TMP] Stage (4, 13, 0, 0) has been pruned...
[TMP] Stage (4, 13, 0, 1) has been pruned...
[TMP] Stage (4, 14, 0, 0) has been pruned...
[TMP] Stage (4, 14, 0, 1) has been pruned...
[TMP] Stage (5, 10, 0, 0) has been pruned...
[TMP] Stage (5, 10, 0, 1) has been pruned...
[TMP] Stage (5, 11, 0, 0) has been pruned...
[TMP] Stage (5, 11, 0, 1) has been pruned...
[TMP] Stage (5, 12, 0, 0) has been pruned...
[TMP] Stage (5, 12, 0, 1) has been pruned...
[TMP] Stage (5, 13, 0, 0) has been pruned...
[TMP] Stage (5, 13, 0, 1) has been pruned...
[TMP] Stage (5, 14, 0, 0) has been pruned...
[TMP] Stage (5, 14, 0, 1) has been pruned...
[TMP] Stage (6, 13, 0, 0) has been pruned...
[TMP] Stage (6, 13, 0, 1) has been pruned...
[TMP] Stage (6, 14, 0, 0) has been pruned...
[TMP] Stage (6, 14, 0, 1) has been pruned...
[TMP] Stage (6, 15, 0, 0) has been pruned...
[TMP] Stage (6, 15, 0, 1) has been pruned...
[TMP] Stage (8, 15, 0, 0) has been pruned...
[TMP] Stage (8, 15, 0, 1) has been pruned...
[TMP] Stage (9, 15, 0, 0) has been pruned...
[TMP] Stage (9, 15, 0, 1) has been pruned...
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 2)]
Result logical_mesh_shapes: [(2, 1)]
Result autosharding_option_dicts: [{}]
 - Compile (driver): 143.57 s
compilation time breakdown: {'stage-construction': '113.29', 'stage-construction-dp': '1.35', 'stage-construction-compilation': '20.52', 'stage-construction-profiling': '78.75'}
 - Compile (worker): 17.35 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 37.43 s

[7.118036508560181, 4.61364483833313, 4.634745359420776, 4.653728008270264, 4.630948305130005, 4.6224000453948975, 4.61989164352417]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 23.431 s.
 - Average e2e iteration time: 4.686000347137451 s.
 - Total local training time: 23.16200065612793 s.
 - Average local iteration time: 4.63200044631958 s.
 - Max allocated memory among devices: 15.854 GB.
 - Compilation times:  {'stage-construction': 113.28719353675842, 'stage-construction-dp': 1.3483951091766357, 'stage-construction-compilation': 20.51838517189026, 'stage-construction-profiling': 78.7467782497406}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `1_a40_1_n_2_d`: 4.63234281539917
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal_prune_no_prof/moe_690M_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling moe_690M with batch size: 512...
------------------------------------------------------------------
