
------------------------------------------------------------------
- (1/3) Profiling wide_resnet_4B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_4B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 640
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 3, 1), 0] = ModuleProfileResult(compute_cost=0.246, peak_memory=9.113 GB, invar_size=7.455 GB, outvar_size=0.479 GB, temp_buffer_size=1.179 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 1] = ModuleProfileResult(compute_cost=23.564, peak_memory=49.288 GB, invar_size=30.199 GB, outvar_size=14.905 GB, temp_buffer_size=19.089 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 0] = ModuleProfileResult(compute_cost=25.296, peak_memory=3.947 GB, invar_size=2.661 GB, outvar_size=0.598 GB, temp_buffer_size=0.688 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 1] = ModuleProfileResult(compute_cost=10.889, peak_memory=26.827 GB, invar_size=15.387 GB, outvar_size=7.454 GB, temp_buffer_size=11.440 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 1] = ModuleProfileResult(compute_cost=32.434, peak_memory=7.116 GB, invar_size=5.917 GB, outvar_size=2.659 GB, temp_buffer_size=1.199 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 0] = ModuleProfileResult(compute_cost=0.093, peak_memory=17.287 GB, invar_size=14.906 GB, outvar_size=0.389 GB, temp_buffer_size=1.992 GB, available_memory=35.242 GB)
Profiling for submesh 3 (4, 2) takes 75.75 seconds
--------------------------------------------------
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.997 GB, invar_size=0.872 GB, outvar_size=0.598 GB, temp_buffer_size=0.527 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.645 GB, invar_size=0.688 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.219, peak_memory=2.378 GB, invar_size=0.703 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.645 GB, invar_size=0.688 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.997 GB, invar_size=0.872 GB, outvar_size=0.598 GB, temp_buffer_size=0.527 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.248, peak_memory=2.589 GB, invar_size=0.795 GB, outvar_size=0.838 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.177 GB, invar_size=0.693 GB, outvar_size=0.718 GB, temp_buffer_size=0.767 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.341, peak_memory=2.731 GB, invar_size=0.616 GB, outvar_size=0.957 GB, temp_buffer_size=1.158 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.712 GB, invar_size=0.826 GB, outvar_size=0.359 GB, temp_buffer_size=0.527 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=1.733 GB, invar_size=0.772 GB, outvar_size=0.359 GB, temp_buffer_size=0.602 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.712 GB, invar_size=0.826 GB, outvar_size=0.359 GB, temp_buffer_size=0.527 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.338, peak_memory=3.935 GB, invar_size=1.645 GB, outvar_size=0.464 GB, temp_buffer_size=2.051 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.009, peak_memory=3.414 GB, invar_size=1.735 GB, outvar_size=0.688 GB, temp_buffer_size=1.440 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.009, peak_memory=3.294 GB, invar_size=1.615 GB, outvar_size=0.688 GB, temp_buffer_size=1.440 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=3.829 GB, invar_size=2.101 GB, outvar_size=0.871 GB, temp_buffer_size=1.489 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=3.829 GB, invar_size=2.101 GB, outvar_size=0.871 GB, temp_buffer_size=1.489 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.966 GB, invar_size=1.009 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.407, peak_memory=4.239 GB, invar_size=1.948 GB, outvar_size=0.555 GB, temp_buffer_size=2.051 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=1.945 GB, invar_size=0.864 GB, outvar_size=0.479 GB, temp_buffer_size=0.602 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.011, peak_memory=4.159 GB, invar_size=2.043 GB, outvar_size=0.692 GB, temp_buffer_size=1.937 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.966 GB, invar_size=1.009 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=3.203 GB, invar_size=1.770 GB, outvar_size=0.825 GB, temp_buffer_size=1.194 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.253, peak_memory=3.185 GB, invar_size=1.424 GB, outvar_size=0.532 GB, temp_buffer_size=1.522 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=3.203 GB, invar_size=1.770 GB, outvar_size=0.825 GB, temp_buffer_size=1.194 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.710 GB, invar_size=1.549 GB, outvar_size=0.419 GB, temp_buffer_size=0.743 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.558, peak_memory=4.481 GB, invar_size=1.829 GB, outvar_size=0.615 GB, temp_buffer_size=2.294 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.179, peak_memory=1.971 GB, invar_size=0.954 GB, outvar_size=0.539 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.031, peak_memory=2.229 GB, invar_size=1.231 GB, outvar_size=0.419 GB, temp_buffer_size=0.579 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.288 GB, invar_size=1.365 GB, outvar_size=0.299 GB, temp_buffer_size=0.623 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=3.746 GB, invar_size=2.256 GB, outvar_size=1.009 GB, temp_buffer_size=1.250 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=1.751 GB, invar_size=1.048 GB, outvar_size=0.299 GB, temp_buffer_size=0.404 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.187, peak_memory=1.879 GB, invar_size=0.862 GB, outvar_size=0.539 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=3.746 GB, invar_size=2.256 GB, outvar_size=1.009 GB, temp_buffer_size=1.250 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.031 GB, invar_size=2.098 GB, outvar_size=0.359 GB, temp_buffer_size=0.574 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.214, peak_memory=2.306 GB, invar_size=1.229 GB, outvar_size=0.598 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.762 GB, invar_size=3.396 GB, outvar_size=1.548 GB, temp_buffer_size=2.246 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.322, peak_memory=3.488 GB, invar_size=1.726 GB, outvar_size=0.624 GB, temp_buffer_size=1.522 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.143, peak_memory=2.309 GB, invar_size=1.292 GB, outvar_size=0.479 GB, temp_buffer_size=0.538 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.581 GB, invar_size=2.648 GB, outvar_size=0.299 GB, temp_buffer_size=0.634 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.078, peak_memory=4.231 GB, invar_size=2.821 GB, outvar_size=1.231 GB, temp_buffer_size=1.290 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.284, peak_memory=3.411 GB, invar_size=2.206 GB, outvar_size=0.834 GB, temp_buffer_size=1.085 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.024, peak_memory=5.092 GB, invar_size=2.910 GB, outvar_size=1.365 GB, temp_buffer_size=2.063 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.224, peak_memory=3.228 GB, invar_size=2.023 GB, outvar_size=0.742 GB, temp_buffer_size=1.085 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.074, peak_memory=3.745 GB, invar_size=2.335 GB, outvar_size=1.048 GB, temp_buffer_size=1.290 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.157, peak_memory=2.445 GB, invar_size=1.504 GB, outvar_size=0.419 GB, temp_buffer_size=0.523 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.060 GB, invar_size=3.197 GB, outvar_size=0.240 GB, temp_buffer_size=0.623 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.038, peak_memory=7.448 GB, invar_size=4.434 GB, outvar_size=2.098 GB, temp_buffer_size=2.893 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=2.537 GB, invar_size=1.595 GB, outvar_size=0.419 GB, temp_buffer_size=0.523 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.272, peak_memory=4.093 GB, invar_size=2.816 GB, outvar_size=1.109 GB, temp_buffer_size=1.158 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.541 GB, invar_size=1.778 GB, outvar_size=0.240 GB, temp_buffer_size=0.523 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.541 GB, invar_size=1.778 GB, outvar_size=0.240 GB, temp_buffer_size=0.523 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.049, peak_memory=9.097 GB, invar_size=5.473 GB, outvar_size=2.647 GB, temp_buffer_size=3.504 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.852 GB, invar_size=3.930 GB, outvar_size=0.299 GB, temp_buffer_size=0.623 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=4.065 GB, invar_size=2.943 GB, outvar_size=1.292 GB, temp_buffer_size=1.003 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=4.441 GB, invar_size=3.185 GB, outvar_size=1.383 GB, temp_buffer_size=1.136 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.059, peak_memory=10.636 GB, invar_size=6.512 GB, outvar_size=3.196 GB, temp_buffer_size=4.004 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.744 GB, invar_size=2.991 GB, outvar_size=0.240 GB, temp_buffer_size=0.515 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.126, peak_memory=2.967 GB, invar_size=2.145 GB, outvar_size=0.299 GB, temp_buffer_size=0.523 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.126, peak_memory=2.967 GB, invar_size=2.145 GB, outvar_size=0.299 GB, temp_buffer_size=0.523 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.239 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.414 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.167, peak_memory=4.492 GB, invar_size=3.369 GB, outvar_size=1.475 GB, temp_buffer_size=1.004 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.239 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.414 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.211, peak_memory=4.802 GB, invar_size=3.555 GB, outvar_size=1.658 GB, temp_buffer_size=1.126 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.073, peak_memory=12.894 GB, invar_size=8.037 GB, outvar_size=3.929 GB, temp_buffer_size=4.737 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.211, peak_memory=4.802 GB, invar_size=3.555 GB, outvar_size=1.658 GB, temp_buffer_size=1.126 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.537 GB, invar_size=3.723 GB, outvar_size=0.299 GB, temp_buffer_size=0.515 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=9.882 GB, invar_size=6.159 GB, outvar_size=2.990 GB, temp_buffer_size=3.663 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.126, peak_memory=2.665 GB, invar_size=1.952 GB, outvar_size=0.299 GB, temp_buffer_size=0.414 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.186, peak_memory=4.226 GB, invar_size=3.289 GB, outvar_size=1.525 GB, temp_buffer_size=0.877 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.729 GB, invar_size=2.991 GB, outvar_size=0.240 GB, temp_buffer_size=0.500 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.164 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.339 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.126, peak_memory=2.665 GB, invar_size=1.952 GB, outvar_size=0.299 GB, temp_buffer_size=0.414 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.259, peak_memory=5.595 GB, invar_size=4.348 GB, outvar_size=2.024 GB, temp_buffer_size=1.127 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.186, peak_memory=4.226 GB, invar_size=3.289 GB, outvar_size=1.525 GB, temp_buffer_size=0.877 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.259, peak_memory=5.595 GB, invar_size=4.348 GB, outvar_size=2.024 GB, temp_buffer_size=1.127 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.164 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.339 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=9.882 GB, invar_size=6.160 GB, outvar_size=2.990 GB, temp_buffer_size=3.663 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=8.628 GB, invar_size=6.507 GB, outvar_size=0.270 GB, temp_buffer_size=1.852 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.070, peak_memory=12.139 GB, invar_size=7.684 GB, outvar_size=3.723 GB, temp_buffer_size=4.395 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=7.836 GB, invar_size=5.774 GB, outvar_size=0.210 GB, temp_buffer_size=1.852 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.144, peak_memory=4.586 GB, invar_size=3.344 GB, outvar_size=0.270 GB, temp_buffer_size=0.973 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.144, peak_memory=4.586 GB, invar_size=3.344 GB, outvar_size=0.270 GB, temp_buffer_size=0.973 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.117, peak_memory=4.160 GB, invar_size=2.978 GB, outvar_size=0.210 GB, temp_buffer_size=0.973 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.186, peak_memory=4.226 GB, invar_size=3.289 GB, outvar_size=1.525 GB, temp_buffer_size=0.877 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.234, peak_memory=5.020 GB, invar_size=4.082 GB, outvar_size=1.891 GB, temp_buffer_size=0.878 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.234, peak_memory=5.020 GB, invar_size=4.082 GB, outvar_size=1.891 GB, temp_buffer_size=0.878 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.117, peak_memory=4.160 GB, invar_size=2.978 GB, outvar_size=0.210 GB, temp_buffer_size=0.973 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.186, peak_memory=4.226 GB, invar_size=3.289 GB, outvar_size=1.525 GB, temp_buffer_size=0.877 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=10.796 GB, invar_size=8.705 GB, outvar_size=0.240 GB, temp_buffer_size=1.852 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.122, peak_memory=22.999 GB, invar_size=13.221 GB, outvar_size=6.506 GB, temp_buffer_size=9.718 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.131, peak_memory=5.681 GB, invar_size=4.443 GB, outvar_size=0.240 GB, temp_buffer_size=0.999 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=20.741 GB, invar_size=11.696 GB, outvar_size=5.774 GB, temp_buffer_size=8.985 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.131, peak_memory=5.681 GB, invar_size=4.443 GB, outvar_size=0.240 GB, temp_buffer_size=0.999 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=10.004 GB, invar_size=7.972 GB, outvar_size=0.180 GB, temp_buffer_size=1.852 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=5.255 GB, invar_size=4.076 GB, outvar_size=0.180 GB, temp_buffer_size=0.999 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.198, peak_memory=8.097 GB, invar_size=6.073 GB, outvar_size=2.917 GB, temp_buffer_size=1.964 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=5.255 GB, invar_size=4.076 GB, outvar_size=0.180 GB, temp_buffer_size=0.999 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.246, peak_memory=8.949 GB, invar_size=6.866 GB, outvar_size=3.283 GB, temp_buffer_size=2.024 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.198, peak_memory=8.097 GB, invar_size=6.073 GB, outvar_size=2.917 GB, temp_buffer_size=1.964 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.246, peak_memory=8.949 GB, invar_size=6.866 GB, outvar_size=3.283 GB, temp_buffer_size=2.024 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.164, peak_memory=29.756 GB, invar_size=17.586 GB, outvar_size=8.703 GB, temp_buffer_size=12.110 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.149, peak_memory=27.499 GB, invar_size=16.061 GB, outvar_size=7.971 GB, temp_buffer_size=11.378 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=13.012 GB, invar_size=10.980 GB, outvar_size=0.180 GB, temp_buffer_size=1.852 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.227, peak_memory=11.200 GB, invar_size=9.003 GB, outvar_size=4.382 GB, temp_buffer_size=2.137 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=12.219 GB, invar_size=10.248 GB, outvar_size=0.120 GB, temp_buffer_size=1.852 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.117, peak_memory=6.759 GB, invar_size=5.581 GB, outvar_size=0.180 GB, temp_buffer_size=0.999 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.179, peak_memory=10.378 GB, invar_size=8.211 GB, outvar_size=4.016 GB, temp_buffer_size=2.107 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.117, peak_memory=6.759 GB, invar_size=5.581 GB, outvar_size=0.180 GB, temp_buffer_size=0.999 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=6.340 GB, invar_size=5.214 GB, outvar_size=0.180 GB, temp_buffer_size=0.947 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.227, peak_memory=11.200 GB, invar_size=9.003 GB, outvar_size=4.382 GB, temp_buffer_size=2.137 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.179, peak_memory=10.378 GB, invar_size=8.211 GB, outvar_size=4.016 GB, temp_buffer_size=2.107 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=6.340 GB, invar_size=5.214 GB, outvar_size=0.180 GB, temp_buffer_size=0.947 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.206, peak_memory=36.865 GB, invar_size=22.077 GB, outvar_size=10.979 GB, temp_buffer_size=14.728 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.192, peak_memory=34.607 GB, invar_size=20.553 GB, outvar_size=10.246 GB, temp_buffer_size=13.995 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.156, peak_memory=12.798 GB, invar_size=10.486 GB, outvar_size=5.153 GB, temp_buffer_size=2.252 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.204, peak_memory=13.530 GB, invar_size=11.219 GB, outvar_size=5.520 GB, temp_buffer_size=2.252 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.156, peak_memory=12.798 GB, invar_size=10.486 GB, outvar_size=5.153 GB, temp_buffer_size=2.252 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.204, peak_memory=13.530 GB, invar_size=11.219 GB, outvar_size=5.520 GB, temp_buffer_size=2.252 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.886 GB, invar_size=0.364 GB, outvar_size=0.778 GB, temp_buffer_size=0.744 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.886 GB, invar_size=0.364 GB, outvar_size=0.778 GB, temp_buffer_size=0.744 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.396, peak_memory=2.392 GB, invar_size=0.189 GB, outvar_size=1.137 GB, temp_buffer_size=1.067 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.312, peak_memory=2.428 GB, invar_size=0.524 GB, outvar_size=0.838 GB, temp_buffer_size=1.067 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=3.724 GB, invar_size=1.620 GB, outvar_size=0.359 GB, temp_buffer_size=2.104 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.177 GB, invar_size=0.693 GB, outvar_size=0.718 GB, temp_buffer_size=0.767 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.541, peak_memory=4.163 GB, invar_size=1.505 GB, outvar_size=0.180 GB, temp_buffer_size=2.658 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.852 GB, invar_size=0.509 GB, outvar_size=0.598 GB, temp_buffer_size=0.744 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=3.604 GB, invar_size=1.500 GB, outvar_size=0.359 GB, temp_buffer_size=2.104 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.852 GB, invar_size=0.509 GB, outvar_size=0.598 GB, temp_buffer_size=0.744 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=3.541 GB, invar_size=1.437 GB, outvar_size=0.509 GB, temp_buffer_size=1.924 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=3.660 GB, invar_size=1.557 GB, outvar_size=0.509 GB, temp_buffer_size=1.924 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.489, peak_memory=4.178 GB, invar_size=1.526 GB, outvar_size=0.524 GB, temp_buffer_size=2.293 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.011, peak_memory=4.039 GB, invar_size=1.923 GB, outvar_size=0.692 GB, temp_buffer_size=1.937 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 66.90 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.898 GB, invar_size=1.852 GB, outvar_size=0.240 GB, temp_buffer_size=0.807 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.348 GB, invar_size=1.302 GB, outvar_size=0.359 GB, temp_buffer_size=0.687 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.348 GB, invar_size=1.302 GB, outvar_size=0.359 GB, temp_buffer_size=0.687 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.746 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.662 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.746 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.662 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.898 GB, invar_size=1.852 GB, outvar_size=0.240 GB, temp_buffer_size=0.807 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.419 GB, invar_size=0.744 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.419 GB, invar_size=0.744 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.198 GB, invar_size=0.882 GB, outvar_size=0.479 GB, temp_buffer_size=0.837 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.198 GB, invar_size=0.882 GB, outvar_size=0.479 GB, temp_buffer_size=0.837 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.737 GB, invar_size=3.702 GB, outvar_size=1.851 GB, temp_buffer_size=1.796 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.698 GB, invar_size=2.723 GB, outvar_size=1.302 GB, temp_buffer_size=1.737 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.635 GB, invar_size=0.469 GB, outvar_size=0.957 GB, temp_buffer_size=1.208 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.533 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.709 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.117 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.427 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.533 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.709 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.698 GB, invar_size=2.723 GB, outvar_size=1.302 GB, temp_buffer_size=1.737 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.136 GB, invar_size=0.103 GB, outvar_size=0.837 GB, temp_buffer_size=1.196 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.635 GB, invar_size=0.469 GB, outvar_size=0.957 GB, temp_buffer_size=1.208 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.136 GB, invar_size=0.103 GB, outvar_size=0.837 GB, temp_buffer_size=1.196 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.117 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.427 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.414 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.117 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.427 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.117 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.427 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.414 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.414 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.324 GB, invar_size=1.763 GB, outvar_size=0.882 GB, temp_buffer_size=2.082 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.414 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.324 GB, invar_size=1.763 GB, outvar_size=0.882 GB, temp_buffer_size=2.082 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.613 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=1.204 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.009 GB, invar_size=1.033 GB, outvar_size=0.094 GB, temp_buffer_size=3.975 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.866 GB, invar_size=1.728 GB, outvar_size=0.744 GB, temp_buffer_size=2.660 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.866 GB, invar_size=1.728 GB, outvar_size=0.744 GB, temp_buffer_size=2.660 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.512 GB, invar_size=1.536 GB, outvar_size=0.469 GB, temp_buffer_size=3.617 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.414 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.613 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=1.204 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.414 GB, invar_size=1.585 GB, outvar_size=0.240 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.737 GB, invar_size=3.702 GB, outvar_size=1.851 GB, temp_buffer_size=1.796 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.583 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=1.174 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.493 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=1.085 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.583 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=1.174 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.435 GB, invar_size=4.369 GB, outvar_size=0.180 GB, temp_buffer_size=1.886 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.435 GB, invar_size=4.369 GB, outvar_size=0.180 GB, temp_buffer_size=1.886 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.633 GB, invar_size=6.567 GB, outvar_size=0.120 GB, temp_buffer_size=1.946 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.512 GB, invar_size=1.536 GB, outvar_size=0.469 GB, temp_buffer_size=3.617 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.633 GB, invar_size=6.567 GB, outvar_size=0.120 GB, temp_buffer_size=1.946 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.009 GB, invar_size=1.033 GB, outvar_size=0.094 GB, temp_buffer_size=3.975 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.493 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=1.085 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.921 GB, invar_size=5.999 GB, outvar_size=0.060 GB, temp_buffer_size=1.862 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.921 GB, invar_size=5.999 GB, outvar_size=0.060 GB, temp_buffer_size=1.862 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.493 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=1.085 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.493 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=1.085 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=12.665 GB, invar_size=8.796 GB, outvar_size=4.368 GB, temp_buffer_size=3.749 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=12.665 GB, invar_size=8.796 GB, outvar_size=4.368 GB, temp_buffer_size=3.749 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=17.212 GB, invar_size=13.131 GB, outvar_size=6.566 GB, temp_buffer_size=3.961 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=15.002 GB, invar_size=11.996 GB, outvar_size=5.998 GB, temp_buffer_size=2.947 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=15.002 GB, invar_size=11.996 GB, outvar_size=5.998 GB, temp_buffer_size=2.947 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=17.212 GB, invar_size=13.131 GB, outvar_size=6.566 GB, temp_buffer_size=3.961 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 17.00 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 2, 0) has been pruned...
[TMP] Stage (0, 6, 2, 1) has been pruned...
[TMP] Stage (0, 6, 2, 2) has been pruned...
[TMP] Stage (0, 7, 2, 0) has been pruned...
[TMP] Stage (0, 7, 2, 1) has been pruned...
[TMP] Stage (0, 7, 2, 2) has been pruned...
[TMP] Stage (0, 8, 2, 0) has been pruned...
[TMP] Stage (0, 8, 2, 1) has been pruned...
[TMP] Stage (0, 8, 2, 2) has been pruned...
[TMP] Stage (1, 6, 2, 0) has been pruned...
[TMP] Stage (1, 6, 2, 1) has been pruned...
[TMP] Stage (1, 6, 2, 2) has been pruned...
[TMP] Stage (1, 7, 2, 0) has been pruned...
[TMP] Stage (1, 7, 2, 1) has been pruned...
[TMP] Stage (1, 7, 2, 2) has been pruned...
[TMP] Stage (1, 8, 2, 0) has been pruned...
[TMP] Stage (1, 8, 2, 1) has been pruned...
[TMP] Stage (1, 8, 2, 2) has been pruned...
[TMP] Stage (1, 9, 2, 0) has been pruned...
[TMP] Stage (1, 9, 2, 1) has been pruned...
[TMP] Stage (1, 9, 2, 2) has been pruned...
[TMP] Stage (2, 7, 2, 0) has been pruned...
[TMP] Stage (2, 7, 2, 1) has been pruned...
[TMP] Stage (2, 7, 2, 2) has been pruned...
[TMP] Stage (2, 8, 2, 0) has been pruned...
[TMP] Stage (2, 8, 2, 1) has been pruned...
[TMP] Stage (2, 8, 2, 2) has been pruned...
[TMP] Stage (2, 9, 2, 0) has been pruned...
[TMP] Stage (2, 9, 2, 1) has been pruned...
[TMP] Stage (2, 9, 2, 2) has been pruned...
[TMP] Stage (2, 10, 2, 0) has been pruned...
[TMP] Stage (2, 10, 2, 1) has been pruned...
[TMP] Stage (2, 10, 2, 2) has been pruned...
[TMP] Stage (3, 8, 2, 0) has been pruned...
[TMP] Stage (3, 8, 2, 1) has been pruned...
[TMP] Stage (3, 8, 2, 2) has been pruned...
[TMP] Stage (3, 9, 2, 0) has been pruned...
[TMP] Stage (3, 9, 2, 1) has been pruned...
[TMP] Stage (3, 9, 2, 2) has been pruned...
[TMP] Stage (3, 10, 2, 0) has been pruned...
[TMP] Stage (3, 10, 2, 1) has been pruned...
[TMP] Stage (3, 10, 2, 2) has been pruned...
[TMP] Stage (3, 11, 2, 0) has been pruned...
[TMP] Stage (3, 11, 2, 1) has been pruned...
[TMP] Stage (3, 11, 2, 2) has been pruned...
[TMP] Stage (4, 10, 2, 0) has been pruned...
[TMP] Stage (4, 10, 2, 1) has been pruned...
[TMP] Stage (4, 10, 2, 2) has been pruned...
[TMP] Stage (4, 11, 2, 0) has been pruned...
[TMP] Stage (4, 11, 2, 1) has been pruned...
[TMP] Stage (4, 11, 2, 2) has been pruned...
[TMP] Stage (4, 12, 2, 0) has been pruned...
[TMP] Stage (4, 12, 2, 1) has been pruned...
[TMP] Stage (4, 12, 2, 2) has been pruned...
[TMP] Stage (5, 11, 2, 0) has been pruned...
[TMP] Stage (5, 11, 2, 1) has been pruned...
[TMP] Stage (5, 11, 2, 2) has been pruned...
[TMP] Stage (5, 12, 2, 0) has been pruned...
[TMP] Stage (5, 12, 2, 1) has been pruned...
[TMP] Stage (5, 12, 2, 2) has been pruned...
[TMP] Stage (5, 13, 2, 0) has been pruned...
[TMP] Stage (5, 13, 2, 1) has been pruned...
[TMP] Stage (5, 13, 2, 2) has been pruned...
[TMP] Stage (5, 14, 2, 0) has been pruned...
[TMP] Stage (5, 14, 2, 1) has been pruned...
[TMP] Stage (5, 14, 2, 2) has been pruned...
[TMP] Stage (6, 12, 2, 0) has been pruned...
[TMP] Stage (6, 12, 2, 1) has been pruned...
[TMP] Stage (6, 12, 2, 2) has been pruned...
[TMP] Stage (6, 13, 2, 0) has been pruned...
[TMP] Stage (6, 13, 2, 1) has been pruned...
[TMP] Stage (6, 13, 2, 2) has been pruned...
[TMP] Stage (6, 14, 2, 0) has been pruned...
[TMP] Stage (6, 14, 2, 1) has been pruned...
[TMP] Stage (6, 14, 2, 2) has been pruned...
[TMP] Stage (6, 15, 2, 0) has been pruned...
[TMP] Stage (6, 15, 2, 1) has been pruned...
[TMP] Stage (6, 15, 2, 2) has been pruned...
[TMP] Stage (7, 13, 2, 0) has been pruned...
[TMP] Stage (7, 13, 2, 1) has been pruned...
[TMP] Stage (7, 13, 2, 2) has been pruned...
[TMP] Stage (7, 14, 2, 0) has been pruned...
[TMP] Stage (7, 14, 2, 1) has been pruned...
[TMP] Stage (7, 14, 2, 2) has been pruned...
[TMP] Stage (7, 15, 2, 0) has been pruned...
[TMP] Stage (7, 15, 2, 1) has been pruned...
[TMP] Stage (7, 15, 2, 2) has been pruned...
[TMP] Stage (8, 14, 2, 0) has been pruned...
[TMP] Stage (8, 14, 2, 1) has been pruned...
[TMP] Stage (8, 14, 2, 2) has been pruned...
[TMP] Stage (8, 15, 2, 0) has been pruned...
[TMP] Stage (8, 15, 2, 1) has been pruned...
[TMP] Stage (8, 15, 2, 2) has been pruned...
[TMP] Stage (9, 15, 2, 0) has been pruned...
[TMP] Stage (9, 15, 2, 1) has been pruned...
[TMP] Stage (9, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.32<0>
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.32<0>
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.32<0>
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.32<0>
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO comm 0x3dc7b50 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO comm 0x5040550 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=4069658)[0m 
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=4069658)[0m 
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=4069658)[0m 
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO comm 0x3d306f0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=4069658)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO comm 0x93328b0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=4069657)[0m 
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=4069657)[0m 
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=4069657)[0m 
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO comm 0x4c33860 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=4069658)[0m 
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO comm 0x955ef00 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=4069657)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=4069657)[0m 
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO comm 0xa3d8640 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO comm 0x520d6e0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO comm 0x3a0b9a0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO comm 0x77060c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO comm 0x48fc350 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO comm 0x63d1270 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1531369, ip=192.168.0.18)[0m gpu3:1531369:1531369 [0] NCCL INFO comm 0x4ac25d0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO comm 0x74115d0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
 - Compile (driver): 182.90 s
compilation time breakdown: {'stage-construction': '161.95', 'stage-construction-dp': '1.37', 'stage-construction-compilation': '44.38', 'stage-construction-profiling': '82.20'}
 - Compile (worker): 4.08 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=2164575, ip=192.168.0.33)[0m gpu18:2164575:2164575 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2164574, ip=192.168.0.33)[0m gpu18:2164574:2164574 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=4069658)[0m gpu2:4069658:4069658 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=4069657)[0m gpu2:4069657:4069657 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=418393, ip=192.168.0.26)[0m gpu11:418393:418393 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=418394, ip=192.168.0.26)[0m gpu11:418394:418394 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1531368, ip=192.168.0.18)[0m gpu3:1531368:1531368 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 464.36 s

[216.71740794181824, 40.57501816749573, 40.35228133201599, 40.603955030441284, 40.68063139915466, 40.72397470474243, 40.784945011138916]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 206.152 s.
 - Average e2e iteration time: 41.230003356933594 s.
 - Total local training time: 203.14601135253906 s.
 - Average local iteration time: 40.62900161743164 s.
 - Max allocated memory among devices: 22.736 GB.
 - Compilation times:  {'stage-construction': 161.95441365242004, 'stage-construction-dp': 1.3692684173583984, 'stage-construction-compilation': 44.382909536361694, 'stage-construction-profiling': 82.1961407661438}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `4_a40_4_n_2_d`: 40.62915802001953
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_4B_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling wide_resnet_4B with batch size: 512...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal/wide_resnet_4B_512.pkl`, creating it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 640
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 3, 0), 0] = ModuleProfileResult(compute_cost=0.093, peak_memory=17.923 GB, invar_size=14.907 GB, outvar_size=0.778 GB, temp_buffer_size=2.238 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 0] = ModuleProfileResult(compute_cost=0.292, peak_memory=9.692 GB, invar_size=7.466 GB, outvar_size=0.838 GB, temp_buffer_size=1.388 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 1] = ModuleProfileResult(compute_cost=23.564, peak_memory=49.776 GB, invar_size=30.589 GB, outvar_size=14.905 GB, temp_buffer_size=19.187 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 0] = ModuleProfileResult(compute_cost=22.106, peak_memory=5.461 GB, invar_size=3.516 GB, outvar_size=0.987 GB, temp_buffer_size=0.958 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 1] = ModuleProfileResult(compute_cost=10.757, peak_memory=25.349 GB, invar_size=15.765 GB, outvar_size=7.463 GB, temp_buffer_size=9.583 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 1] = ModuleProfileResult(compute_cost=37.380, peak_memory=10.912 GB, invar_size=8.016 GB, outvar_size=3.513 GB, temp_buffer_size=2.896 GB, available_memory=35.242 GB)
Profiling for submesh 3 (4, 2) takes 47.61 seconds
--------------------------------------------------
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=0.928 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=0.928 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.752 GB, invar_size=1.111 GB, outvar_size=1.196 GB, temp_buffer_size=1.445 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.752 GB, invar_size=1.111 GB, outvar_size=1.196 GB, temp_buffer_size=1.445 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.436, peak_memory=4.531 GB, invar_size=1.182 GB, outvar_size=1.436 GB, temp_buffer_size=1.914 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.713 GB, invar_size=0.872 GB, outvar_size=1.436 GB, temp_buffer_size=1.406 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.669, peak_memory=7.405 GB, invar_size=2.841 GB, outvar_size=0.703 GB, temp_buffer_size=4.085 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.108 GB, invar_size=0.689 GB, outvar_size=1.196 GB, temp_buffer_size=1.223 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.108 GB, invar_size=0.689 GB, outvar_size=1.196 GB, temp_buffer_size=1.223 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.688 GB, invar_size=1.065 GB, outvar_size=0.718 GB, temp_buffer_size=0.906 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.493, peak_memory=4.862 GB, invar_size=1.273 GB, outvar_size=1.675 GB, temp_buffer_size=1.914 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.713 GB, invar_size=0.872 GB, outvar_size=1.436 GB, temp_buffer_size=1.406 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.624, peak_memory=4.582 GB, invar_size=0.883 GB, outvar_size=1.675 GB, temp_buffer_size=2.024 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.146 GB, invar_size=0.368 GB, outvar_size=1.555 GB, temp_buffer_size=1.223 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.009, peak_memory=5.737 GB, invar_size=2.333 GB, outvar_size=0.927 GB, temp_buffer_size=2.926 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.146 GB, invar_size=0.368 GB, outvar_size=1.555 GB, temp_buffer_size=1.223 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.688 GB, invar_size=1.065 GB, outvar_size=0.718 GB, temp_buffer_size=0.906 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.680, peak_memory=5.004 GB, invar_size=0.975 GB, outvar_size=1.914 GB, temp_buffer_size=2.116 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.804, peak_memory=7.827 GB, invar_size=3.264 GB, outvar_size=0.795 GB, temp_buffer_size=4.085 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.184, peak_memory=3.404 GB, invar_size=1.250 GB, outvar_size=0.718 GB, temp_buffer_size=1.436 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.009, peak_memory=5.977 GB, invar_size=2.572 GB, outvar_size=0.927 GB, temp_buffer_size=2.926 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.162 GB, invar_size=1.248 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=6.393 GB, invar_size=2.939 GB, outvar_size=1.111 GB, temp_buffer_size=2.975 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=6.393 GB, invar_size=2.939 GB, outvar_size=1.111 GB, temp_buffer_size=2.975 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.973, peak_memory=8.007 GB, invar_size=2.722 GB, outvar_size=0.883 GB, temp_buffer_size=4.566 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=1.108, peak_memory=8.429 GB, invar_size=3.145 GB, outvar_size=0.974 GB, temp_buffer_size=4.567 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.162 GB, invar_size=1.248 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=6.353 GB, invar_size=2.215 GB, outvar_size=0.689 GB, temp_buffer_size=3.779 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=6.592 GB, invar_size=2.454 GB, outvar_size=0.689 GB, temp_buffer_size=3.779 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=5.003 GB, invar_size=2.369 GB, outvar_size=1.065 GB, temp_buffer_size=2.156 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.592 GB, invar_size=1.668 GB, outvar_size=0.838 GB, temp_buffer_size=1.086 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.789, peak_memory=4.495 GB, invar_size=0.198 GB, outvar_size=2.273 GB, temp_buffer_size=2.024 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.498, peak_memory=5.697 GB, invar_size=2.261 GB, outvar_size=0.772 GB, temp_buffer_size=2.958 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.011, peak_memory=7.308 GB, invar_size=3.060 GB, outvar_size=0.872 GB, temp_buffer_size=3.889 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.592 GB, invar_size=1.668 GB, outvar_size=0.838 GB, temp_buffer_size=1.086 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.890 GB, invar_size=1.485 GB, outvar_size=0.598 GB, temp_buffer_size=0.807 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=1.076, peak_memory=7.940 GB, invar_size=2.650 GB, outvar_size=0.180 GB, temp_buffer_size=5.290 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=5.003 GB, invar_size=2.369 GB, outvar_size=1.065 GB, temp_buffer_size=2.156 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.011, peak_memory=7.068 GB, invar_size=2.820 GB, outvar_size=0.872 GB, temp_buffer_size=3.889 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.240, peak_memory=3.504 GB, invar_size=1.342 GB, outvar_size=0.957 GB, temp_buffer_size=1.205 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.890 GB, invar_size=1.485 GB, outvar_size=0.598 GB, temp_buffer_size=0.807 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=6.420 GB, invar_size=2.282 GB, outvar_size=0.359 GB, temp_buffer_size=4.138 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.358, peak_memory=3.135 GB, invar_size=1.102 GB, outvar_size=1.077 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=6.660 GB, invar_size=2.522 GB, outvar_size=0.359 GB, temp_buffer_size=4.138 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.644 GB, invar_size=2.218 GB, outvar_size=0.718 GB, temp_buffer_size=0.709 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.350, peak_memory=3.227 GB, invar_size=1.193 GB, outvar_size=1.077 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.052, peak_memory=3.197 GB, invar_size=1.900 GB, outvar_size=0.718 GB, temp_buffer_size=0.579 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=5.652 GB, invar_size=2.974 GB, outvar_size=1.248 GB, temp_buffer_size=2.199 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.194 GB, invar_size=2.767 GB, outvar_size=0.598 GB, temp_buffer_size=0.829 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.633, peak_memory=6.120 GB, invar_size=2.683 GB, outvar_size=0.863 GB, temp_buffer_size=2.958 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.024, peak_memory=5.631 GB, invar_size=3.328 GB, outvar_size=1.485 GB, temp_buffer_size=2.064 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.434, peak_memory=5.142 GB, invar_size=2.801 GB, outvar_size=0.862 GB, temp_buffer_size=2.102 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.489 GB, invar_size=2.084 GB, outvar_size=0.598 GB, temp_buffer_size=0.807 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.624 GB, invar_size=3.317 GB, outvar_size=0.479 GB, temp_buffer_size=0.829 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.420 GB, invar_size=3.934 GB, outvar_size=1.668 GB, temp_buffer_size=2.247 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=5.652 GB, invar_size=2.974 GB, outvar_size=1.248 GB, temp_buffer_size=2.199 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.549, peak_memory=5.325 GB, invar_size=2.984 GB, outvar_size=0.954 GB, temp_buffer_size=2.102 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.024, peak_memory=5.751 GB, invar_size=3.448 GB, outvar_size=1.485 GB, temp_buffer_size=2.063 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.539 GB, invar_size=4.053 GB, outvar_size=1.668 GB, temp_buffer_size=2.247 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.386, peak_memory=3.622 GB, invar_size=1.468 GB, outvar_size=1.197 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.253, peak_memory=3.537 GB, invar_size=1.743 GB, outvar_size=0.838 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=5.476 GB, invar_size=4.049 GB, outvar_size=0.599 GB, temp_buffer_size=0.829 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.510, peak_memory=5.995 GB, invar_size=3.653 GB, outvar_size=1.228 GB, temp_buffer_size=2.103 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=3.323 GB, invar_size=2.018 GB, outvar_size=0.479 GB, temp_buffer_size=0.827 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.038, peak_memory=8.046 GB, invar_size=4.913 GB, outvar_size=2.217 GB, temp_buffer_size=2.894 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.118 GB, invar_size=3.050 GB, outvar_size=0.479 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.430, peak_memory=5.780 GB, invar_size=3.843 GB, outvar_size=1.503 GB, temp_buffer_size=1.697 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=3.323 GB, invar_size=2.018 GB, outvar_size=0.479 GB, temp_buffer_size=0.827 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.110, peak_memory=6.896 GB, invar_size=4.398 GB, outvar_size=1.900 GB, temp_buffer_size=2.259 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.049, peak_memory=9.635 GB, invar_size=5.892 GB, outvar_size=2.767 GB, temp_buffer_size=3.504 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.351, peak_memory=5.962 GB, invar_size=4.034 GB, outvar_size=1.778 GB, temp_buffer_size=1.689 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.342, peak_memory=5.848 GB, invar_size=4.034 GB, outvar_size=1.778 GB, temp_buffer_size=1.575 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.971 GB, invar_size=3.783 GB, outvar_size=0.598 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=2.762 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.579 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.059, peak_memory=11.115 GB, invar_size=6.871 GB, outvar_size=3.316 GB, temp_buffer_size=4.005 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.194, peak_memory=6.765 GB, invar_size=4.525 GB, outvar_size=2.083 GB, temp_buffer_size=2.001 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=2.762 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.579 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.118 GB, invar_size=3.050 GB, outvar_size=0.479 GB, temp_buffer_size=0.589 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=10.241 GB, invar_size=6.459 GB, outvar_size=3.050 GB, temp_buffer_size=3.663 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.309, peak_memory=5.056 GB, invar_size=3.648 GB, outvar_size=1.585 GB, temp_buffer_size=1.288 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.073, peak_memory=13.433 GB, invar_size=8.456 GB, outvar_size=4.049 GB, temp_buffer_size=4.737 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.309, peak_memory=5.056 GB, invar_size=3.648 GB, outvar_size=1.585 GB, temp_buffer_size=1.288 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.427, peak_memory=6.815 GB, invar_size=4.886 GB, outvar_size=2.144 GB, temp_buffer_size=1.689 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=2.642 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.459 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=9.052 GB, invar_size=6.567 GB, outvar_size=0.539 GB, temp_buffer_size=1.946 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=3.809 GB, invar_size=2.384 GB, outvar_size=0.599 GB, temp_buffer_size=0.827 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=2.642 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.459 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=8.199 GB, invar_size=5.834 GB, outvar_size=0.419 GB, temp_buffer_size=1.946 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.418, peak_memory=6.693 GB, invar_size=4.886 GB, outvar_size=2.144 GB, temp_buffer_size=1.567 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=3.249 GB, invar_size=2.071 GB, outvar_size=0.598 GB, temp_buffer_size=0.579 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=3.809 GB, invar_size=2.384 GB, outvar_size=0.599 GB, temp_buffer_size=0.827 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.384, peak_memory=5.908 GB, invar_size=4.500 GB, outvar_size=1.951 GB, temp_buffer_size=1.289 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.070, peak_memory=12.558 GB, invar_size=8.043 GB, outvar_size=3.782 GB, temp_buffer_size=4.396 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.182, peak_memory=4.583 GB, invar_size=3.097 GB, outvar_size=0.419 GB, temp_buffer_size=1.067 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.182, peak_memory=4.583 GB, invar_size=3.097 GB, outvar_size=0.419 GB, temp_buffer_size=1.067 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=3.249 GB, invar_size=2.071 GB, outvar_size=0.598 GB, temp_buffer_size=0.579 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=10.241 GB, invar_size=6.459 GB, outvar_size=3.050 GB, temp_buffer_size=3.663 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.309, peak_memory=5.056 GB, invar_size=3.648 GB, outvar_size=1.585 GB, temp_buffer_size=1.288 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.384, peak_memory=5.908 GB, invar_size=4.500 GB, outvar_size=1.951 GB, temp_buffer_size=1.289 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.309, peak_memory=5.056 GB, invar_size=3.648 GB, outvar_size=1.585 GB, temp_buffer_size=1.288 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.210, peak_memory=5.070 GB, invar_size=3.464 GB, outvar_size=0.539 GB, temp_buffer_size=1.068 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.210, peak_memory=5.070 GB, invar_size=3.464 GB, outvar_size=0.539 GB, temp_buffer_size=1.068 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=11.189 GB, invar_size=8.764 GB, outvar_size=0.479 GB, temp_buffer_size=1.946 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=10.337 GB, invar_size=8.032 GB, outvar_size=0.359 GB, temp_buffer_size=1.946 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.351, peak_memory=8.819 GB, invar_size=6.432 GB, outvar_size=2.977 GB, temp_buffer_size=2.268 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.200, peak_memory=5.913 GB, invar_size=4.196 GB, outvar_size=0.479 GB, temp_buffer_size=1.238 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=21.071 GB, invar_size=11.966 GB, outvar_size=5.833 GB, temp_buffer_size=8.985 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.200, peak_memory=5.913 GB, invar_size=4.196 GB, outvar_size=0.479 GB, temp_buffer_size=1.238 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.351, peak_memory=8.819 GB, invar_size=6.432 GB, outvar_size=2.977 GB, temp_buffer_size=2.268 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.122, peak_memory=23.388 GB, invar_size=13.550 GB, outvar_size=6.566 GB, temp_buffer_size=9.718 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.427, peak_memory=9.671 GB, invar_size=7.284 GB, outvar_size=3.343 GB, temp_buffer_size=2.268 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.427, peak_memory=9.671 GB, invar_size=7.284 GB, outvar_size=3.343 GB, temp_buffer_size=2.268 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=12.493 GB, invar_size=10.307 GB, outvar_size=0.240 GB, temp_buffer_size=1.946 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=13.345 GB, invar_size=11.040 GB, outvar_size=0.359 GB, temp_buffer_size=1.946 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.229, peak_memory=6.399 GB, invar_size=4.562 GB, outvar_size=0.599 GB, temp_buffer_size=1.238 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.229, peak_memory=6.399 GB, invar_size=4.562 GB, outvar_size=0.599 GB, temp_buffer_size=1.238 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.125, peak_memory=6.880 GB, invar_size=5.334 GB, outvar_size=0.359 GB, temp_buffer_size=1.187 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.228, peak_memory=7.373 GB, invar_size=5.700 GB, outvar_size=0.479 GB, temp_buffer_size=1.194 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.125, peak_memory=6.880 GB, invar_size=5.334 GB, outvar_size=0.359 GB, temp_buffer_size=1.187 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.301, peak_memory=11.305 GB, invar_size=8.629 GB, outvar_size=4.075 GB, temp_buffer_size=2.556 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.228, peak_memory=7.373 GB, invar_size=5.700 GB, outvar_size=0.479 GB, temp_buffer_size=1.194 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.301, peak_memory=11.305 GB, invar_size=8.629 GB, outvar_size=4.075 GB, temp_buffer_size=2.556 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.149, peak_memory=27.798 GB, invar_size=16.301 GB, outvar_size=8.031 GB, temp_buffer_size=11.378 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.377, peak_memory=11.984 GB, invar_size=9.481 GB, outvar_size=4.442 GB, temp_buffer_size=2.383 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.377, peak_memory=11.984 GB, invar_size=9.481 GB, outvar_size=4.442 GB, temp_buffer_size=2.383 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.164, peak_memory=30.116 GB, invar_size=17.885 GB, outvar_size=8.763 GB, temp_buffer_size=12.111 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.273, peak_memory=13.748 GB, invar_size=10.785 GB, outvar_size=5.213 GB, temp_buffer_size=2.843 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.192, peak_memory=34.847 GB, invar_size=20.732 GB, outvar_size=10.306 GB, temp_buffer_size=13.995 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.273, peak_memory=13.748 GB, invar_size=10.785 GB, outvar_size=5.213 GB, temp_buffer_size=2.843 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.349, peak_memory=14.661 GB, invar_size=11.637 GB, outvar_size=5.579 GB, temp_buffer_size=2.904 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.349, peak_memory=14.661 GB, invar_size=11.637 GB, outvar_size=5.579 GB, temp_buffer_size=2.904 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.206, peak_memory=37.164 GB, invar_size=22.317 GB, outvar_size=11.039 GB, temp_buffer_size=14.728 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 39.62 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(5, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.759 GB, invar_size=0.845 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.047 GB, invar_size=0.845 GB, outvar_size=0.957 GB, temp_buffer_size=1.245 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.759 GB, invar_size=0.845 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.047 GB, invar_size=0.845 GB, outvar_size=0.957 GB, temp_buffer_size=1.245 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.227 GB, invar_size=4.180 GB, outvar_size=2.090 GB, temp_buffer_size=2.568 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.167 GB, invar_size=3.321 GB, outvar_size=1.541 GB, temp_buffer_size=2.367 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.390 GB, invar_size=2.168 GB, outvar_size=0.845 GB, temp_buffer_size=2.743 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.434 GB, invar_size=1.541 GB, outvar_size=0.718 GB, temp_buffer_size=1.175 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.173 GB, invar_size=2.924 GB, outvar_size=1.223 GB, temp_buffer_size=5.292 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.744 GB, invar_size=2.091 GB, outvar_size=0.479 GB, temp_buffer_size=1.175 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.573 GB, invar_size=1.223 GB, outvar_size=1.436 GB, temp_buffer_size=1.914 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.997 GB, invar_size=1.360 GB, outvar_size=0.957 GB, temp_buffer_size=1.680 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.997 GB, invar_size=1.360 GB, outvar_size=0.957 GB, temp_buffer_size=1.680 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.573 GB, invar_size=1.223 GB, outvar_size=1.436 GB, temp_buffer_size=1.914 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.390 GB, invar_size=2.168 GB, outvar_size=0.845 GB, temp_buffer_size=2.743 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.390 GB, invar_size=2.168 GB, outvar_size=0.845 GB, temp_buffer_size=2.743 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.434 GB, invar_size=1.541 GB, outvar_size=0.718 GB, temp_buffer_size=1.175 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.390 GB, invar_size=2.168 GB, outvar_size=0.845 GB, temp_buffer_size=2.743 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.167 GB, invar_size=3.321 GB, outvar_size=1.541 GB, temp_buffer_size=2.367 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.682 GB, invar_size=2.720 GB, outvar_size=1.360 GB, temp_buffer_size=4.005 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.161 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.978 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.682 GB, invar_size=2.720 GB, outvar_size=1.360 GB, temp_buffer_size=4.005 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.161 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.978 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.173 GB, invar_size=2.924 GB, outvar_size=1.223 GB, temp_buffer_size=5.292 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.744 GB, invar_size=2.091 GB, outvar_size=0.479 GB, temp_buffer_size=1.175 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.147 GB, invar_size=0.828 GB, outvar_size=1.914 GB, temp_buffer_size=2.405 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=10.776 GB, invar_size=2.852 GB, outvar_size=0.828 GB, temp_buffer_size=7.206 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.227 GB, invar_size=4.180 GB, outvar_size=2.090 GB, temp_buffer_size=2.568 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.705 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=1.818 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.705 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=1.818 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.645 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=1.758 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.466 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=1.578 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.645 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=1.758 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.466 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=1.578 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.466 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=1.578 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.179 GB, invar_size=0.112 GB, outvar_size=1.675 GB, temp_buffer_size=2.393 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.982 GB, invar_size=4.489 GB, outvar_size=0.359 GB, temp_buffer_size=2.135 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.466 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=1.578 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.179 GB, invar_size=0.112 GB, outvar_size=1.675 GB, temp_buffer_size=2.393 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=13.391 GB, invar_size=9.095 GB, outvar_size=4.488 GB, temp_buffer_size=4.056 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.982 GB, invar_size=4.489 GB, outvar_size=0.359 GB, temp_buffer_size=2.135 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.147 GB, invar_size=0.828 GB, outvar_size=1.914 GB, temp_buffer_size=2.405 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.803 GB, invar_size=1.880 GB, outvar_size=0.094 GB, temp_buffer_size=7.923 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=10.776 GB, invar_size=2.852 GB, outvar_size=0.828 GB, temp_buffer_size=7.206 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.146 GB, invar_size=6.059 GB, outvar_size=0.120 GB, temp_buffer_size=1.967 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.803 GB, invar_size=1.880 GB, outvar_size=0.094 GB, temp_buffer_size=7.923 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=13.391 GB, invar_size=9.095 GB, outvar_size=4.488 GB, temp_buffer_size=4.056 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.162 GB, invar_size=6.687 GB, outvar_size=0.240 GB, temp_buffer_size=2.237 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.162 GB, invar_size=6.687 GB, outvar_size=0.240 GB, temp_buffer_size=2.237 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=18.209 GB, invar_size=13.370 GB, outvar_size=6.685 GB, temp_buffer_size=4.599 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.146 GB, invar_size=6.059 GB, outvar_size=0.120 GB, temp_buffer_size=1.967 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=18.209 GB, invar_size=13.370 GB, outvar_size=6.685 GB, temp_buffer_size=4.599 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=15.589 GB, invar_size=12.115 GB, outvar_size=6.058 GB, temp_buffer_size=3.354 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=15.589 GB, invar_size=12.115 GB, outvar_size=6.058 GB, temp_buffer_size=3.354 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 16.95 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 2, 0) has been pruned...
[TMP] Stage (0, 6, 2, 1) has been pruned...
[TMP] Stage (0, 6, 2, 2) has been pruned...
[TMP] Stage (0, 7, 2, 0) has been pruned...
[TMP] Stage (0, 7, 2, 1) has been pruned...
[TMP] Stage (0, 7, 2, 2) has been pruned...
[TMP] Stage (0, 8, 2, 0) has been pruned...
[TMP] Stage (0, 8, 2, 1) has been pruned...
[TMP] Stage (0, 8, 2, 2) has been pruned...
[TMP] Stage (1, 6, 2, 0) has been pruned...
[TMP] Stage (1, 6, 2, 1) has been pruned...
[TMP] Stage (1, 6, 2, 2) has been pruned...
[TMP] Stage (1, 7, 2, 0) has been pruned...
[TMP] Stage (1, 7, 2, 1) has been pruned...
[TMP] Stage (1, 7, 2, 2) has been pruned...
[TMP] Stage (1, 8, 2, 0) has been pruned...
[TMP] Stage (1, 8, 2, 1) has been pruned...
[TMP] Stage (1, 8, 2, 2) has been pruned...
[TMP] Stage (1, 9, 2, 0) has been pruned...
[TMP] Stage (1, 9, 2, 1) has been pruned...
[TMP] Stage (1, 9, 2, 2) has been pruned...
[TMP] Stage (2, 7, 2, 0) has been pruned...
[TMP] Stage (2, 7, 2, 1) has been pruned...
[TMP] Stage (2, 7, 2, 2) has been pruned...
[TMP] Stage (2, 8, 2, 0) has been pruned...
[TMP] Stage (2, 8, 2, 1) has been pruned...
[TMP] Stage (2, 8, 2, 2) has been pruned...
[TMP] Stage (2, 9, 2, 0) has been pruned...
[TMP] Stage (2, 9, 2, 1) has been pruned...
[TMP] Stage (2, 9, 2, 2) has been pruned...
[TMP] Stage (2, 10, 2, 0) has been pruned...
[TMP] Stage (2, 10, 2, 1) has been pruned...
[TMP] Stage (2, 10, 2, 2) has been pruned...
[TMP] Stage (3, 8, 2, 0) has been pruned...
[TMP] Stage (3, 8, 2, 1) has been pruned...
[TMP] Stage (3, 8, 2, 2) has been pruned...
[TMP] Stage (3, 9, 2, 0) has been pruned...
[TMP] Stage (3, 9, 2, 1) has been pruned...
[TMP] Stage (3, 9, 2, 2) has been pruned...
[TMP] Stage (3, 10, 2, 0) has been pruned...
[TMP] Stage (3, 10, 2, 1) has been pruned...
[TMP] Stage (3, 10, 2, 2) has been pruned...
[TMP] Stage (3, 11, 2, 0) has been pruned...
[TMP] Stage (3, 11, 2, 1) has been pruned...
[TMP] Stage (3, 11, 2, 2) has been pruned...
[TMP] Stage (4, 10, 2, 0) has been pruned...
[TMP] Stage (4, 10, 2, 1) has been pruned...
[TMP] Stage (4, 10, 2, 2) has been pruned...
[TMP] Stage (4, 11, 2, 0) has been pruned...
[TMP] Stage (4, 11, 2, 1) has been pruned...
[TMP] Stage (4, 11, 2, 2) has been pruned...
[TMP] Stage (4, 12, 2, 0) has been pruned...
[TMP] Stage (4, 12, 2, 1) has been pruned...
[TMP] Stage (4, 12, 2, 2) has been pruned...
[TMP] Stage (5, 11, 2, 0) has been pruned...
[TMP] Stage (5, 11, 2, 1) has been pruned...
[TMP] Stage (5, 11, 2, 2) has been pruned...
[TMP] Stage (5, 12, 2, 0) has been pruned...
[TMP] Stage (5, 12, 2, 1) has been pruned...
[TMP] Stage (5, 12, 2, 2) has been pruned...
[TMP] Stage (5, 13, 2, 0) has been pruned...
[TMP] Stage (5, 13, 2, 1) has been pruned...
[TMP] Stage (5, 13, 2, 2) has been pruned...
[TMP] Stage (5, 14, 2, 0) has been pruned...
[TMP] Stage (5, 14, 2, 1) has been pruned...
[TMP] Stage (5, 14, 2, 2) has been pruned...
[TMP] Stage (6, 12, 2, 0) has been pruned...
[TMP] Stage (6, 12, 2, 1) has been pruned...
[TMP] Stage (6, 12, 2, 2) has been pruned...
[TMP] Stage (6, 13, 2, 0) has been pruned...
[TMP] Stage (6, 13, 2, 1) has been pruned...
[TMP] Stage (6, 13, 2, 2) has been pruned...
[TMP] Stage (6, 14, 2, 0) has been pruned...
[TMP] Stage (6, 14, 2, 1) has been pruned...
[TMP] Stage (6, 14, 2, 2) has been pruned...
[TMP] Stage (6, 15, 2, 0) has been pruned...
[TMP] Stage (6, 15, 2, 1) has been pruned...
[TMP] Stage (6, 15, 2, 2) has been pruned...
[TMP] Stage (7, 13, 2, 0) has been pruned...
[TMP] Stage (7, 13, 2, 1) has been pruned...
[TMP] Stage (7, 13, 2, 2) has been pruned...
[TMP] Stage (7, 14, 2, 0) has been pruned...
[TMP] Stage (7, 14, 2, 1) has been pruned...
[TMP] Stage (7, 14, 2, 2) has been pruned...
[TMP] Stage (7, 15, 2, 0) has been pruned...
[TMP] Stage (7, 15, 2, 1) has been pruned...
[TMP] Stage (7, 15, 2, 2) has been pruned...
[TMP] Stage (8, 14, 2, 0) has been pruned...
[TMP] Stage (8, 14, 2, 1) has been pruned...
[TMP] Stage (8, 14, 2, 2) has been pruned...
[TMP] Stage (8, 15, 2, 0) has been pruned...
[TMP] Stage (8, 15, 2, 1) has been pruned...
[TMP] Stage (8, 15, 2, 2) has been pruned...
[TMP] Stage (9, 15, 2, 0) has been pruned...
[TMP] Stage (9, 15, 2, 1) has been pruned...
[TMP] Stage (9, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO comm 0x3c53ea0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=427633, ip=192.168.0.26)[0m gpu11:427633:427633 [0] NCCL INFO comm 0x4f4f440 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=4083692)[0m 
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=4083692)[0m 
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=4083692)[0m 
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=427634, ip=192.168.0.26)[0m gpu11:427634:427634 [0] NCCL INFO comm 0x91bf770 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO comm 0x38c31e0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=4083691)[0m 
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=4083692)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=4083691)[0m 
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=4083691)[0m 
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO comm 0x41fcab0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=4083692)[0m 
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=4083692)[0m gpu2:4083692:4083692 [0] NCCL INFO comm 0x6916210 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.32<0>
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.32<0>
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=4083691)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=4083691)[0m 
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=4083691)[0m gpu2:4083691:4083691 [0] NCCL INFO comm 0x9adfb00 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO comm 0x3b8f6c0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.32<0>
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.32<0>
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO comm 0x4d09c30 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2173732, ip=192.168.0.33)[0m gpu18:2173732:2173732 [0] NCCL INFO comm 0x5b85da0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m 
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2173729, ip=192.168.0.33)[0m gpu18:2173729:2173729 [0] NCCL INFO comm 0x76cf990 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO comm 0x4087480 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1539442, ip=192.168.0.18)[0m gpu3:1539442:1539442 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=1539442, ip=192.168.0.18)[0m gpu3:1539442:1539442 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1539442, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1539442, ip=192.168.0.18)[0m gpu3:1539442:1539442 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1539442, ip=192.168.0.18)[0m gpu3:1539442:1539442 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=1539442, ip=192.168.0.18)[0m gpu3:1539442:1539442 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1539440, ip=192.168.0.18)[0m gpu3:1539440:1539440 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
