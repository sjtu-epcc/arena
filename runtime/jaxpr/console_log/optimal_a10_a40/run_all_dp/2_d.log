[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.13 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.37 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 29.80 s

[16.09078598022461, 11.474810123443604]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 11.523 s.
 - Average e2e iteration time: 11.523000717163086 s.
 - Total local training time: 11.475000381469727 s.
 - Average local iteration time: 11.475000381469727 s.
 - Max allocated memory among devices: 15.847 GB.
 - Compilation times:  {'stage-construction': 0.01510310173034668, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 65.82652735710144 s (GPU time = 131.65305471420288 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 11.474809646606445)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 18.58 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.74 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 57.87 s

[30.0426504611969, 23.013294458389282]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 23.061 s.
 - Average e2e iteration time: 23.06100082397461 s.
 - Total local training time: 23.01300048828125 s.
 - Average local iteration time: 23.01300048828125 s.
 - Max allocated memory among devices: 26.872 GB.
 - Compilation times:  {'stage-construction': 0.012471437454223633, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 93.92815256118774 s (GPU time = 187.8563051223755 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 23.013294219970703)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_256.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_2B_256.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 18.21 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.54 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.35 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 15.09 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 46.73 s

[25.72785520553589, 18.060665607452393]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 18.166 s.
 - Average e2e iteration time: 18.166000366210938 s.
 - Total local training time: 18.06100082397461 s.
 - Average local iteration time: 18.06100082397461 s.
 - Max allocated memory among devices: 24.614 GB.
 - Compilation times:  {'stage-construction': 0.01210165023803711, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 85.87378764152527 s (GPU time = 171.74757528305054 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 18.060665130615234)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_512.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 21.41 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 15.12 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_2B_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.62 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 14.37 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_1024.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 18.44 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.26 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_1024.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 20.30 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.69 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_2B_1024.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.72 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.40 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_760M_128.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f6fa69b6d30>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 42.19 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 19.68 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 22.01 s

[9.867190837860107, 7.552414894104004]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 8.107 s.
 - Average e2e iteration time: 8.107000350952148 s.
 - Total local training time: 7.552000522613525 s.
 - Average local iteration time: 7.552000522613525 s.
 - Max allocated memory among devices: 13.519 GB.
 - Compilation times:  {'stage-construction': 0.0204622745513916, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 90.20563077926636 s (GPU time = 180.41126155853271 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 7.552414894104004)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_760M_128.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/bert_1.3B_128.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fa85ee4dd30>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 40.86 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 18.74 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 33.01 s

[15.203979969024658, 13.00167965888977]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 13.035 s.
 - Average e2e iteration time: 13.035000801086426 s.
 - Total local training time: 13.00200080871582 s.
 - Average local iteration time: 13.00200080871582 s.
 - Max allocated memory among devices: 19.662 GB.
 - Compilation times:  {'stage-construction': 0.01681995391845703, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 99.02431893348694 s (GPU time = 198.04863786697388 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 13.001679420471191)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_1.3B_128.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/bert_2.6B_128.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f462b0d3730>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 63.27 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 25.98 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_760M_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f7ef1a63d00>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 41.98 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 19.60 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 36.38 s

[17.15232825279236, 14.793199300765991]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 15.233 s.
 - Average e2e iteration time: 15.233000755310059 s.
 - Total local training time: 14.793001174926758 s.
 - Average local iteration time: 14.793001174926758 s.
 - Max allocated memory among devices: 17.925 GB.
 - Compilation times:  {'stage-construction': 0.017323970794677734, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 103.71638107299805 s (GPU time = 207.4327621459961 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 14.79319953918457)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_760M_256.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/bert_1.3B_256.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f1f93ed5ca0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 41.55 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 19.57 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 56.61 s

[27.41523551940918, 25.160218000411987]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 25.217 s.
 - Average e2e iteration time: 25.21700096130371 s.
 - Total local training time: 25.160001754760742 s.
 - Average local iteration time: 25.160001754760742 s.
 - Max allocated memory among devices: 23.89 GB.
 - Compilation times:  {'stage-construction': 0.01677107810974121, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 124.3870210647583 s (GPU time = 248.7740421295166 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 25.16021728515625)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_1.3B_256.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/bert_2.6B_256.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fd82daa4cd0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 61.70 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 28.05 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/bert_760M_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7ff442271c70>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 43.63 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 19.64 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 64.21 s

[31.03377389907837, 28.832947731018066]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 29.268 s.
 - Average e2e iteration time: 29.268001556396484 s.
 - Total local training time: 28.8330020904541 s.
 - Average local iteration time: 28.8330020904541 s.
 - Max allocated memory among devices: 28.023 GB.
 - Compilation times:  {'stage-construction': 0.017621517181396484, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 133.8549928665161 s (GPU time = 267.7099857330322 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 28.83294677734375)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_760M_512.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/bert_1.3B_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f70843677c0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 41.94 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 19.86 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 104.16 s

[50.99272418022156, 48.774659395217896]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 48.805 s.
 - Average e2e iteration time: 48.80500411987305 s.
 - Total local training time: 48.775001525878906 s.
 - Average local iteration time: 48.775001525878906 s.
 - Max allocated memory among devices: 34.52 GB.
 - Compilation times:  {'stage-construction': 0.016681194305419922, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 172.70920181274414 s (GPU time = 345.4184036254883 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 48.774658203125)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_1.3B_512.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/bert_2.6B_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f63098dceb0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 61.02 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 27.58 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_690M_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f735aa7aca0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 23.15 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 16.41 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 13.06 s

[6.338966369628906, 4.627712249755859]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 4.653 s.
 - Average e2e iteration time: 4.653000354766846 s.
 - Total local training time: 4.628000259399414 s.
 - Average local iteration time: 4.628000259399414 s.
 - Max allocated memory among devices: 12.955 GB.
 - Compilation times:  {'stage-construction': 0.006969451904296875, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 56.69821214675903 s (GPU time = 113.39642429351807 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 4.627712249755859)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_690M_256.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/moe_1.3B_256.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f13ba092c70>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 45.27 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.94 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 22.68 s

[10.69461727142334, 8.83950924873352]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 8.868 s.
 - Average e2e iteration time: 8.868000030517578 s.
 - Total local training time: 8.84000015258789 s.
 - Average local iteration time: 8.84000015258789 s.
 - Max allocated memory among devices: 14.416 GB.
 - Compilation times:  {'stage-construction': 0.01163339614868164, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 105.05757689476013 s (GPU time = 210.11515378952026 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 8.839509010314941)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_1.3B_256.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/moe_2.4B_256.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7fdc94660490>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 45.87 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 31.08 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 32.08 s

[14.269708156585693, 12.275129795074463]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 12.307 s.
 - Average e2e iteration time: 12.307000160217285 s.
 - Total local training time: 12.27500057220459 s.
 - Average local iteration time: 12.27500057220459 s.
 - Max allocated memory among devices: 23.762 GB.
 - Compilation times:  {'stage-construction': 0.012406110763549805, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 115.84940505027771 s (GPU time = 231.69881010055542 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 12.275129318237305)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_2.4B_256.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/moe_690M_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f0c18de8c70>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 22.47 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 16.67 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 21.18 s

[10.500607252120972, 8.651630878448486]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 8.676 s.
 - Average e2e iteration time: 8.676000595092773 s.
 - Total local training time: 8.652000427246094 s.
 - Average local iteration time: 8.652000427246094 s.
 - Max allocated memory among devices: 21.934 GB.
 - Compilation times:  {'stage-construction': 0.006975650787353516, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 64.35462546348572 s (GPU time = 128.70925092697144 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 8.651630401611328)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_690M_512.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/moe_1.3B_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f51d649bc10>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 45.17 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.59 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 38.17 s

[18.37527585029602, 16.50447964668274]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 16.545 s.
 - Average e2e iteration time: 16.545000076293945 s.
 - Total local training time: 16.50400161743164 s.
 - Average local iteration time: 16.50400161743164 s.
 - Max allocated memory among devices: 18.858 GB.
 - Compilation times:  {'stage-construction': 0.01188802719116211, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 120.06433176994324 s (GPU time = 240.12866353988647 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 16.504480361938477)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_1.3B_512.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/moe_2.4B_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f30ecf5eca0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 45.72 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 31.29 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 52.34 s

[24.501442193984985, 22.33166480064392]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 22.364 s.
 - Average e2e iteration time: 22.36400032043457 s.
 - Total local training time: 22.332000732421875 s.
 - Average local iteration time: 22.332000732421875 s.
 - Max allocated memory among devices: 29.514 GB.
 - Compilation times:  {'stage-construction': 0.011414527893066406, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 135.86389660835266 s (GPU time = 271.7277932167053 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 22.3316650390625)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_2.4B_512.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/moe_690M_1024.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7fe61fe16d30>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 22.56 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 16.68 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/moe_1.3B_1024.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f5b8e32ec70>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 44.45 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.63 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 69.20 s

[33.96594977378845, 32.04610824584961]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 32.076 s.
 - Average e2e iteration time: 32.07600021362305 s.
 - Total local training time: 32.04600143432617 s.
 - Average local iteration time: 32.04600143432617 s.
 - Max allocated memory among devices: 30.045 GB.
 - Compilation times:  {'stage-construction': 0.011703014373779297, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False


[I] The e2e profiling overhead with estimation is 149.70296669006348 s (GPU time = 299.40593338012695 s)
[TMP] Current profiling results of key `1_a40_1_n_2_d`: [((1, 2, 1), 32.04610824584961)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_1.3B_1024.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/moe_2.4B_1024.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7feab679ec10>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(1, 2)]
    - 'submesh_logical_shapes': [(2, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 44.60 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.92 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
