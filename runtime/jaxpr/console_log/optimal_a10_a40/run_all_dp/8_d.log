[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 25.83 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 51.90 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 128.48 s

[116.41706252098083, 7.332882642745972]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 7.407 s.
 - Average e2e iteration time: 7.407000541687012 s.
 - Total local training time: 7.333000183105469 s.
 - Average local iteration time: 7.333000183105469 s.
 - Max allocated memory among devices: 8.484 GB.
 - Compilation times:  {'stage-construction': 0.012169122695922852, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 216.54137253761292 (GPU time = 1732.3309803009033) s.

[I] The e2e profiling overhead with estimation is 217.36188960075378 s (GPU time = 1738.8951168060303 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 7.332882881164551)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 18.66 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.66 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 44.27 s

[20.640785217285156, 15.18916654586792]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 15.302 s.
 - Average e2e iteration time: 15.302000999450684 s.
 - Total local training time: 15.189001083374023 s.
 - Average local iteration time: 15.189001083374023 s.
 - Max allocated memory among devices: 15.907 GB.
 - Compilation times:  {'stage-construction': 0.012083053588867188, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 79.68877983093262 (GPU time = 637.5102386474609) s.

[I] The e2e profiling overhead with estimation is 80.38014459609985 s (GPU time = 643.0411567687988 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 15.189167022705078)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_2B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 20.15 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.82 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 103.52 s

[35.50578999519348, 30.138373613357544]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 30.223 s.
 - Average e2e iteration time: 30.22300148010254 s.
 - Total local training time: 30.13800048828125 s.
 - Average local iteration time: 30.13800048828125 s.
 - Max allocated memory among devices: 29.3 GB.
 - Compilation times:  {'stage-construction': 0.012138605117797852, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 140.9682161808014 (GPU time = 1127.7457294464111) s.

[I] The e2e profiling overhead with estimation is 141.64962244033813 s (GPU time = 1133.196979522705 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 30.13837432861328)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_2B_256.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_4B_256.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 640
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 18.74 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.73 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_6.8B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 16
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.92 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.64 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 262.88 s

[]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 0.144 s.
 - Average e2e iteration time: 0.14400000870227814 s.
 - Total local training time: 0.0 s.
 - Average local iteration time: nan s.
 - Max allocated memory among devices: 35.265 GB.
 - Compilation times:  {'stage-construction': 0.011812925338745117, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 302.918194770813 (GPU time = 2423.345558166504) s.

[I] The e2e profiling overhead with estimation is 303.7007977962494 s (GPU time = 2429.606382369995 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), nan)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_6.8B_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 20.20 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.13 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 27.97 s

[14.89900803565979, 9.17064881324768]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 9.26 s.
 - Average e2e iteration time: 9.260000228881836 s.
 - Total local training time: 9.171000480651855 s.
 - Average local iteration time: 9.171000480651855 s.
 - Max allocated memory among devices: 10.701 GB.
 - Compilation times:  {'stage-construction': 0.012195348739624023, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 65.12654137611389 (GPU time = 521.0123310089111) s.

[I] The e2e profiling overhead with estimation is 65.93201470375061 s (GPU time = 527.4561176300049 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 9.170648574829102)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 20.02 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.01 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 48.49 s

[23.874366760253906, 18.343697547912598]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 18.434 s.
 - Average e2e iteration time: 18.43400001525879 s.
 - Total local training time: 18.34400177001953 s.
 - Average local iteration time: 18.34400177001953 s.
 - Max allocated memory among devices: 19.078 GB.
 - Compilation times:  {'stage-construction': 0.012169361114501953, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 85.42199921607971 (GPU time = 683.3759937286377) s.

[I] The e2e profiling overhead with estimation is 86.1529688835144 s (GPU time = 689.2237510681152 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 18.34369659423828)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_2B_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.14 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.86 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 104.34 s

[42.17993688583374, 36.68032383918762]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 36.781 s.
 - Average e2e iteration time: 36.781002044677734 s.
 - Total local training time: 36.68000030517578 s.
 - Average local iteration time: 36.68000030517578 s.
 - Max allocated memory among devices: 33.72 GB.
 - Compilation times:  {'stage-construction': 0.012253046035766602, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 140.91769313812256 (GPU time = 1127.3415451049805) s.

[I] The e2e profiling overhead with estimation is 141.63044142723083 s (GPU time = 1133.0435314178467 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 36.68032455444336)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_2B_512.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_4B_512.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 640
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.62 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.83 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_6.8B_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 16
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.50 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 12.86 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 233.55 s

[]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 0.185 s.
 - Average e2e iteration time: 0.1850000023841858 s.
 - Total local training time: 0.0 s.
 - Average local iteration time: nan s.
 - Max allocated memory among devices: 35.3 GB.
 - Compilation times:  {'stage-construction': 0.012309074401855469, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 273.220844745636 (GPU time = 2185.766757965088) s.

[I] The e2e profiling overhead with estimation is 274.03738617897034 s (GPU time = 2192.2990894317627 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), nan)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_6.8B_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_1024.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 18.97 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.02 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 35.70 s

[18.413541793823242, 12.814952850341797]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 13.066 s.
 - Average e2e iteration time: 13.066000938415527 s.
 - Total local training time: 12.815000534057617 s.
 - Average local iteration time: 12.815000534057617 s.
 - Max allocated memory among devices: 15.155 GB.
 - Compilation times:  {'stage-construction': 0.01223611831665039, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 71.24458718299866 (GPU time = 569.9566974639893) s.

[I] The e2e profiling overhead with estimation is 72.0647337436676 s (GPU time = 576.5178699493408 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 12.814952850341797)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_500M_1024.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_1024.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.50 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.30 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 62.13 s

[30.736149072647095, 24.708619356155396]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 24.94 s.
 - Average e2e iteration time: 24.940000534057617 s.
 - Total local training time: 24.709001541137695 s.
 - Average local iteration time: 24.709001541137695 s.
 - Max allocated memory among devices: 25.467 GB.
 - Compilation times:  {'stage-construction': 0.012206554412841797, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 98.98171544075012 (GPU time = 791.853723526001) s.

[I] The e2e profiling overhead with estimation is 99.69664978981018 s (GPU time = 797.5731983184814 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 24.708620071411133)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_1B_1024.pkl`...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_2B_1024.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.42 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.16 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Profiling results not found in `./jaxpr/prof_log/ground_truth/wide_resnet_4B_1024.pkl`, creating it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 640
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.45 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.19 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/wide_resnet_6.8B_1024.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 16
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 19.17 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 13.15 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 252.88 s

[]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 0.347 s.
 - Average e2e iteration time: 0.34700000286102295 s.
 - Total local training time: 0.0 s.
 - Average local iteration time: nan s.
 - Max allocated memory among devices: 35.372 GB.
 - Compilation times:  {'stage-construction': 0.011772871017456055, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 292.9154679775238 (GPU time = 2343.3237438201904) s.

[I] The e2e profiling overhead with estimation is 293.7237479686737 s (GPU time = 2349.7899837493896 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), nan)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/wide_resnet_6.8B_1024.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_760M_128.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7efe76bd0be0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 42.30 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 17.93 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 17.72 s

[5.4859559535980225, 2.8875060081481934]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 2.936 s.
 - Average e2e iteration time: 2.936000108718872 s.
 - Total local training time: 2.888000249862671 s.
 - Average local iteration time: 2.888000249862671 s.
 - Max allocated memory among devices: 6.902 GB.
 - Compilation times:  {'stage-construction': 0.017388105392456055, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 83.09065771102905 (GPU time = 664.7252616882324) s.

[I] The e2e profiling overhead with estimation is 83.8169367313385 s (GPU time = 670.535493850708 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 2.8875060081481934)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_760M_128.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_1.3B_128.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f5848540c70>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 41.88 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 18.73 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 22.30 s

[7.295701742172241, 4.779717445373535]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 5.241 s.
 - Average e2e iteration time: 5.241000175476074 s.
 - Total local training time: 4.78000020980835 s.
 - Average local iteration time: 4.78000020980835 s.
 - Max allocated memory among devices: 10.882 GB.
 - Compilation times:  {'stage-construction': 0.016645193099975586, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 89.01798462867737 (GPU time = 712.143877029419) s.

[I] The e2e profiling overhead with estimation is 89.82809472084045 s (GPU time = 718.6247577667236 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 4.779717445373535)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_1.3B_128.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_2.6B_128.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f071dec7cd0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 62.77 s
compilation time breakdown: {'stage-construction': '0.03', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 24.54 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 35.13 s

[10.486240863800049, 8.06771469116211]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 8.125 s.
 - Average e2e iteration time: 8.125 s.
 - Total local training time: 8.068000793457031 s.
 - Average local iteration time: 8.068000793457031 s.
 - Max allocated memory among devices: 20.645 GB.
 - Compilation times:  {'stage-construction': 0.02550029754638672, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 130.31867575645447 (GPU time = 1042.5494060516357) s.

[I] The e2e profiling overhead with estimation is 131.06008386611938 s (GPU time = 1048.480670928955 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 8.06771469116211)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_2.6B_128.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_6.7B_128.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f6aea5e3c40>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 64.58 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 23.67 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_760M_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] No local Ray cluster is found, constructing a new one.
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_1.3B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] No local Ray cluster is found, constructing a new one.
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_2.6B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] No local Ray cluster is found, constructing a new one.
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_6.7B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] No local Ray cluster is found, constructing a new one.
