
------------------------------------------------------------------
- (1/3) Profiling wide_resnet_500M with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_500M_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1), (4, 1))
- Profiling for submesh 2 (4, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages

------------------------------------------------------------------
- (2/3) Profiling wide_resnet_500M with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_500M_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1), (4, 1))
- Profiling for submesh 2 (4, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages

------------------------------------------------------------------
- (3/3) Profiling wide_resnet_500M with batch size: 1024...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_500M_1024.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1), (4, 1))
- Profiling for submesh 2 (4, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
