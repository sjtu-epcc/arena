
------------------------------------------------------------------
- (1/3) Profiling bert_760M with batch size: 128...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal_no_prof/bert_760M_128.pkl`, updating/rewriting it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f4a66b88c70>
    dtype = float16
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.125, peak_memory=1.650 GB, invar_size=0.799 GB, outvar_size=0.070 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.124, peak_memory=1.650 GB, invar_size=0.799 GB, outvar_size=0.070 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.336, peak_memory=7.023 GB, invar_size=1.668 GB, outvar_size=0.799 GB, temp_buffer_size=5.355 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.112, peak_memory=1.065 GB, invar_size=0.674 GB, outvar_size=0.070 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.337, peak_memory=7.023 GB, invar_size=1.668 GB, outvar_size=0.799 GB, temp_buffer_size=5.355 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.112, peak_memory=1.065 GB, invar_size=0.674 GB, outvar_size=0.070 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.314, peak_memory=6.757 GB, invar_size=1.395 GB, outvar_size=0.674 GB, temp_buffer_size=5.338 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.148, peak_memory=1.299 GB, invar_size=0.885 GB, outvar_size=0.094 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.314, peak_memory=6.757 GB, invar_size=1.395 GB, outvar_size=0.674 GB, temp_buffer_size=5.338 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.148, peak_memory=1.299 GB, invar_size=0.885 GB, outvar_size=0.094 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.412, peak_memory=7.820 GB, invar_size=1.841 GB, outvar_size=0.885 GB, temp_buffer_size=5.955 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.112, peak_memory=1.065 GB, invar_size=0.674 GB, outvar_size=0.070 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.412, peak_memory=7.820 GB, invar_size=1.841 GB, outvar_size=0.885 GB, temp_buffer_size=5.955 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.112, peak_memory=1.065 GB, invar_size=0.674 GB, outvar_size=0.070 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.313, peak_memory=7.024 GB, invar_size=1.395 GB, outvar_size=0.674 GB, temp_buffer_size=5.605 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.072, peak_memory=0.813 GB, invar_size=0.446 GB, outvar_size=0.047 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.313, peak_memory=7.024 GB, invar_size=1.395 GB, outvar_size=0.674 GB, temp_buffer_size=5.605 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.072, peak_memory=0.813 GB, invar_size=0.446 GB, outvar_size=0.047 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.334, peak_memory=6.075 GB, invar_size=1.604 GB, outvar_size=0.790 GB, temp_buffer_size=4.448 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.334, peak_memory=6.075 GB, invar_size=1.604 GB, outvar_size=0.790 GB, temp_buffer_size=4.448 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 92.91 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 5, 1, 0) has been pruned...
[TMP] Stage (0, 5, 1, 1) has been pruned...
Result forward_stage_layer_ids: [[0, 1, 2], [3, 4, 5]]
Result mesh_shapes: [(1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 128.20 s
compilation time breakdown: {'stage-construction': '96.54', 'stage-construction-dp': '1.39', 'stage-construction-compilation': '5.92', 'stage-construction-profiling': '64.95'}
 - Compile (worker): 10.60 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 67.56 s

[10.065425157546997, 7.818325042724609, 7.821063756942749, 7.8720784187316895, 7.810714960098267, 7.810504913330078, 7.810505390167236]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 41.097 s.
 - Average e2e iteration time: 8.219000816345215 s.
 - Total local training time: 39.125 s.
 - Average local iteration time: 7.825000286102295 s.
 - Max allocated memory among devices: 12.543 GB.
 - Compilation times:  {'stage-construction': 96.53566884994507, 'stage-construction-dp': 1.3942890167236328, 'stage-construction-compilation': 5.919636487960815, 'stage-construction-profiling': 64.9459023475647}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 7.8249735832214355
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal_no_prof/bert_760M_128.pkl`...

------------------------------------------------------------------
- (2/3) Profiling bert_760M with batch size: 256...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal_no_prof/bert_760M_256.pkl`, creating it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f1fbb9fc670>
    dtype = float16
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.247, peak_memory=2.502 GB, invar_size=0.799 GB, outvar_size=0.141 GB, temp_buffer_size=1.563 GB, available_memory=35.242 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.247, peak_memory=2.502 GB, invar_size=0.799 GB, outvar_size=0.141 GB, temp_buffer_size=1.563 GB, available_memory=35.242 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.660, peak_memory=12.443 GB, invar_size=1.738 GB, outvar_size=0.799 GB, temp_buffer_size=10.705 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.225, peak_memory=1.479 GB, invar_size=0.698 GB, outvar_size=0.141 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.663, peak_memory=12.443 GB, invar_size=1.738 GB, outvar_size=0.799 GB, temp_buffer_size=10.705 GB, available_memory=35.242 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.227, peak_memory=1.479 GB, invar_size=0.698 GB, outvar_size=0.141 GB, temp_buffer_size=0.641 GB, available_memory=35.242 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.623, peak_memory=12.209 GB, invar_size=1.489 GB, outvar_size=0.698 GB, temp_buffer_size=10.673 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.297, peak_memory=1.737 GB, invar_size=0.909 GB, outvar_size=0.188 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.627, peak_memory=12.209 GB, invar_size=1.489 GB, outvar_size=0.698 GB, temp_buffer_size=10.673 GB, available_memory=35.242 GB)
result[(1, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.299, peak_memory=1.737 GB, invar_size=0.909 GB, outvar_size=0.188 GB, temp_buffer_size=0.641 GB, available_memory=35.242 GB)
result[(1, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.816, peak_memory=13.908 GB, invar_size=1.958 GB, outvar_size=0.909 GB, temp_buffer_size=11.902 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.225, peak_memory=1.479 GB, invar_size=0.698 GB, outvar_size=0.141 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.818, peak_memory=13.908 GB, invar_size=1.958 GB, outvar_size=0.909 GB, temp_buffer_size=11.902 GB, available_memory=35.242 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.225, peak_memory=1.479 GB, invar_size=0.698 GB, outvar_size=0.141 GB, temp_buffer_size=0.641 GB, available_memory=35.242 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.623, peak_memory=12.741 GB, invar_size=1.489 GB, outvar_size=0.698 GB, temp_buffer_size=11.205 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.144, peak_memory=1.204 GB, invar_size=0.469 GB, outvar_size=0.094 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.623, peak_memory=12.741 GB, invar_size=1.489 GB, outvar_size=0.698 GB, temp_buffer_size=11.205 GB, available_memory=35.242 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.144, peak_memory=1.204 GB, invar_size=0.469 GB, outvar_size=0.094 GB, temp_buffer_size=0.641 GB, available_memory=35.242 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.674, peak_memory=10.617 GB, invar_size=1.674 GB, outvar_size=0.814 GB, temp_buffer_size=8.896 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.675, peak_memory=10.617 GB, invar_size=1.674 GB, outvar_size=0.814 GB, temp_buffer_size=8.896 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 112.26 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 5, 1, 0) has been pruned...
[TMP] Stage (0, 5, 1, 1) has been pruned...
Result forward_stage_layer_ids: [[0, 1, 2], [3, 4, 5]]
Result mesh_shapes: [(1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 148.63 s
compilation time breakdown: {'stage-construction': '115.98', 'stage-construction-dp': '1.35', 'stage-construction-compilation': '4.43', 'stage-construction-profiling': '87.43'}
 - Compile (worker): 11.48 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 125.22 s

[25.692795991897583, 15.576943159103394, 15.482115745544434, 15.625986576080322, 15.539236545562744, 15.544379949569702, 15.514899253845215]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 81.23 s.
 - Average e2e iteration time: 16.246000289916992 s.
 - Total local training time: 77.70700073242188 s.
 - Average local iteration time: 15.541000366210938 s.
 - Max allocated memory among devices: 18.692 GB.
 - Compilation times:  {'stage-construction': 115.98184275627136, 'stage-construction-dp': 1.3534791469573975, 'stage-construction-compilation': 4.430923223495483, 'stage-construction-profiling': 87.43353199958801}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 15.5413236618042
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal_no_prof/bert_760M_256.pkl`...

------------------------------------------------------------------
- (3/3) Profiling bert_760M with batch size: 512...
------------------------------------------------------------------
