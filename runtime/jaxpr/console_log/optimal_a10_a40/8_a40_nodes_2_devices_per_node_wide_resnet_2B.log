
------------------------------------------------------------------
- (1/3) Profiling wide_resnet_2B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_2B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.762 GB, invar_size=0.050 GB, outvar_size=0.293 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.762 GB, invar_size=0.050 GB, outvar_size=0.293 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=0.911 GB, invar_size=0.032 GB, outvar_size=0.377 GB, temp_buffer_size=0.502 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.785 GB, invar_size=0.389 GB, outvar_size=0.046 GB, temp_buffer_size=1.395 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.205, peak_memory=2.287 GB, invar_size=0.432 GB, outvar_size=0.023 GB, temp_buffer_size=1.855 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.966 GB, invar_size=0.563 GB, outvar_size=0.084 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.153 GB, invar_size=0.760 GB, outvar_size=0.084 GB, temp_buffer_size=0.310 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.079, peak_memory=1.840 GB, invar_size=1.041 GB, outvar_size=0.479 GB, temp_buffer_size=0.715 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.077, peak_memory=1.780 GB, invar_size=1.041 GB, outvar_size=0.479 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.784 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.257 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.680 GB, invar_size=1.562 GB, outvar_size=0.760 GB, temp_buffer_size=1.077 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.101 GB, invar_size=0.760 GB, outvar_size=0.084 GB, temp_buffer_size=0.257 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.784 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.257 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.427 GB, invar_size=0.885 GB, outvar_size=0.401 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.427 GB, invar_size=0.885 GB, outvar_size=0.401 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.760 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.233 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.680 GB, invar_size=1.562 GB, outvar_size=0.760 GB, temp_buffer_size=1.077 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.760 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.233 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.101 GB, invar_size=0.760 GB, outvar_size=0.084 GB, temp_buffer_size=0.257 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.445 GB, invar_size=0.886 GB, outvar_size=0.401 GB, temp_buffer_size=0.518 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.445 GB, invar_size=0.886 GB, outvar_size=0.401 GB, temp_buffer_size=0.518 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.760 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.233 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.760 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.233 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.680 GB, invar_size=1.562 GB, outvar_size=0.760 GB, temp_buffer_size=1.077 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.101 GB, invar_size=0.760 GB, outvar_size=0.084 GB, temp_buffer_size=0.257 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.362 GB, invar_size=0.886 GB, outvar_size=0.401 GB, temp_buffer_size=0.434 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.760 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.233 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.362 GB, invar_size=0.886 GB, outvar_size=0.401 GB, temp_buffer_size=0.434 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.680 GB, invar_size=1.562 GB, outvar_size=0.760 GB, temp_buffer_size=1.077 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.760 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.233 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.114 GB, invar_size=2.124 GB, outvar_size=0.063 GB, temp_buffer_size=0.927 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.362 GB, invar_size=0.886 GB, outvar_size=0.401 GB, temp_buffer_size=0.434 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.362 GB, invar_size=0.886 GB, outvar_size=0.401 GB, temp_buffer_size=0.434 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.044, peak_memory=1.685 GB, invar_size=1.125 GB, outvar_size=0.063 GB, temp_buffer_size=0.497 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.040, peak_memory=7.995 GB, invar_size=4.268 GB, outvar_size=2.124 GB, temp_buffer_size=3.685 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.044, peak_memory=1.685 GB, invar_size=1.125 GB, outvar_size=0.063 GB, temp_buffer_size=0.497 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.170 GB, invar_size=3.201 GB, outvar_size=0.042 GB, temp_buffer_size=0.927 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.071, peak_memory=3.318 GB, invar_size=2.250 GB, outvar_size=1.083 GB, temp_buffer_size=1.026 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.016, peak_memory=2.244 GB, invar_size=1.664 GB, outvar_size=0.042 GB, temp_buffer_size=0.538 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.071, peak_memory=3.318 GB, invar_size=2.250 GB, outvar_size=1.083 GB, temp_buffer_size=1.026 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=11.037 GB, invar_size=6.401 GB, outvar_size=3.201 GB, temp_buffer_size=4.594 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.016, peak_memory=2.244 GB, invar_size=1.664 GB, outvar_size=0.042 GB, temp_buffer_size=0.538 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.867 GB, invar_size=2.947 GB, outvar_size=0.021 GB, temp_buffer_size=0.898 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.052, peak_memory=4.474 GB, invar_size=3.284 GB, outvar_size=1.621 GB, temp_buffer_size=1.147 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.019, peak_memory=1.983 GB, invar_size=1.484 GB, outvar_size=0.021 GB, temp_buffer_size=0.478 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.052, peak_memory=4.474 GB, invar_size=3.284 GB, outvar_size=1.621 GB, temp_buffer_size=1.147 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.055, peak_memory=10.277 GB, invar_size=5.894 GB, outvar_size=2.947 GB, temp_buffer_size=4.362 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.019, peak_memory=1.983 GB, invar_size=1.484 GB, outvar_size=0.021 GB, temp_buffer_size=0.478 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.034, peak_memory=3.801 GB, invar_size=2.968 GB, outvar_size=1.484 GB, temp_buffer_size=0.812 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.034, peak_memory=3.801 GB, invar_size=2.968 GB, outvar_size=1.484 GB, temp_buffer_size=0.812 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.033, peak_memory=0.721 GB, invar_size=0.257 GB, outvar_size=0.168 GB, temp_buffer_size=0.296 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.808 GB, invar_size=0.365 GB, outvar_size=0.168 GB, temp_buffer_size=0.275 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.706 GB, invar_size=0.263 GB, outvar_size=0.168 GB, temp_buffer_size=0.275 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.706 GB, invar_size=0.263 GB, outvar_size=0.168 GB, temp_buffer_size=0.275 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.244 GB, invar_size=0.610 GB, outvar_size=0.263 GB, temp_buffer_size=0.550 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.102, peak_memory=1.343 GB, invar_size=0.515 GB, outvar_size=0.174 GB, temp_buffer_size=0.745 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.768 GB, invar_size=0.814 GB, outvar_size=0.365 GB, temp_buffer_size=0.787 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=1.915 GB, invar_size=0.700 GB, outvar_size=0.266 GB, temp_buffer_size=1.047 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.598 GB, invar_size=0.263 GB, outvar_size=0.168 GB, temp_buffer_size=0.168 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.074 GB, invar_size=0.605 GB, outvar_size=0.126 GB, temp_buffer_size=0.344 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.033, peak_memory=0.676 GB, invar_size=0.257 GB, outvar_size=0.168 GB, temp_buffer_size=0.251 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.598 GB, invar_size=0.263 GB, outvar_size=0.168 GB, temp_buffer_size=0.168 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=0.966 GB, invar_size=0.563 GB, outvar_size=0.084 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=0.811 GB, invar_size=0.449 GB, outvar_size=0.126 GB, temp_buffer_size=0.236 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.070, peak_memory=0.972 GB, invar_size=0.428 GB, outvar_size=0.209 GB, temp_buffer_size=0.335 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.302 GB, invar_size=0.874 GB, outvar_size=0.084 GB, temp_buffer_size=0.344 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.244 GB, invar_size=0.610 GB, outvar_size=0.263 GB, temp_buffer_size=0.550 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.102, peak_memory=1.343 GB, invar_size=0.515 GB, outvar_size=0.174 GB, temp_buffer_size=0.745 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.100, peak_memory=1.654 GB, invar_size=0.897 GB, outvar_size=0.344 GB, temp_buffer_size=0.673 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.245 GB, invar_size=0.610 GB, outvar_size=0.263 GB, temp_buffer_size=0.550 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.245 GB, invar_size=0.610 GB, outvar_size=0.263 GB, temp_buffer_size=0.550 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=2.244 GB, invar_size=1.250 GB, outvar_size=0.604 GB, temp_buffer_size=0.909 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=3.003 GB, invar_size=1.747 GB, outvar_size=0.874 GB, temp_buffer_size=1.173 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.048, peak_memory=1.798 GB, invar_size=0.981 GB, outvar_size=0.449 GB, temp_buffer_size=0.733 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.808 GB, invar_size=0.365 GB, outvar_size=0.168 GB, temp_buffer_size=0.275 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.023 GB, invar_size=0.434 GB, outvar_size=0.168 GB, temp_buffer_size=0.422 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.884 GB, invar_size=0.298 GB, outvar_size=0.251 GB, temp_buffer_size=0.335 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.134, peak_memory=1.489 GB, invar_size=0.400 GB, outvar_size=0.419 GB, temp_buffer_size=0.670 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=1.119 GB, invar_size=0.278 GB, outvar_size=0.335 GB, temp_buffer_size=0.505 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.939 GB, invar_size=0.180 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.939 GB, invar_size=0.180 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.684 GB, invar_size=0.730 GB, outvar_size=0.365 GB, temp_buffer_size=0.787 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.884 GB, invar_size=0.298 GB, outvar_size=0.251 GB, temp_buffer_size=0.335 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=1.810 GB, invar_size=0.679 GB, outvar_size=0.298 GB, temp_buffer_size=0.963 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.188, peak_memory=2.483 GB, invar_size=0.884 GB, outvar_size=0.233 GB, temp_buffer_size=1.432 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.973 GB, invar_size=0.568 GB, outvar_size=0.180 GB, temp_buffer_size=1.279 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=1.893 GB, invar_size=0.763 GB, outvar_size=0.298 GB, temp_buffer_size=0.963 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.973 GB, invar_size=0.568 GB, outvar_size=0.180 GB, temp_buffer_size=1.279 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.377 GB, invar_size=0.557 GB, outvar_size=0.046 GB, temp_buffer_size=1.820 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=0.262, peak_memory=2.492 GB, invar_size=0.640 GB, outvar_size=0.278 GB, temp_buffer_size=1.601 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 54.23 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.530 GB, invar_size=0.357 GB, outvar_size=0.335 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.906 GB, invar_size=0.715 GB, outvar_size=0.357 GB, temp_buffer_size=1.856 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.844 GB, invar_size=0.257 GB, outvar_size=0.168 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.844 GB, invar_size=0.257 GB, outvar_size=0.168 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.844 GB, invar_size=0.257 GB, outvar_size=0.168 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.844 GB, invar_size=0.257 GB, outvar_size=0.168 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.530 GB, invar_size=0.357 GB, outvar_size=0.335 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.657 GB, invar_size=0.514 GB, outvar_size=0.257 GB, temp_buffer_size=0.975 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.490 GB, invar_size=0.514 GB, outvar_size=0.257 GB, temp_buffer_size=0.808 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.906 GB, invar_size=0.715 GB, outvar_size=0.357 GB, temp_buffer_size=1.856 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.657 GB, invar_size=0.514 GB, outvar_size=0.257 GB, temp_buffer_size=0.975 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.657 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.573 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.604 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.657 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.657 GB, invar_size=0.514 GB, outvar_size=0.257 GB, temp_buffer_size=0.975 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.573 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.604 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.657 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.573 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.604 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.868 GB, invar_size=0.443 GB, outvar_size=0.084 GB, temp_buffer_size=0.341 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.434 GB, invar_size=1.478 GB, outvar_size=0.042 GB, temp_buffer_size=0.914 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.434 GB, invar_size=1.478 GB, outvar_size=0.042 GB, temp_buffer_size=0.914 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.573 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.604 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.573 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.604 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.844 GB, invar_size=0.257 GB, outvar_size=0.168 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.467 GB, invar_size=1.533 GB, outvar_size=0.000 GB, temp_buffer_size=0.935 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.573 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.604 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.844 GB, invar_size=0.257 GB, outvar_size=0.168 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.430 GB, invar_size=2.955 GB, outvar_size=1.478 GB, temp_buffer_size=1.433 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.467 GB, invar_size=1.533 GB, outvar_size=0.000 GB, temp_buffer_size=0.935 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.430 GB, invar_size=2.955 GB, outvar_size=1.478 GB, temp_buffer_size=1.433 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.414 GB, invar_size=3.023 GB, outvar_size=1.532 GB, temp_buffer_size=1.349 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.490 GB, invar_size=0.514 GB, outvar_size=0.257 GB, temp_buffer_size=0.808 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.657 GB, invar_size=0.514 GB, outvar_size=0.257 GB, temp_buffer_size=0.975 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.414 GB, invar_size=3.023 GB, outvar_size=1.532 GB, temp_buffer_size=1.349 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.657 GB, invar_size=0.885 GB, outvar_size=0.443 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.35 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 7, 3, 0) has been pruned...
[TMP] Stage (2, 7, 3, 1) has been pruned...
[TMP] Stage (2, 7, 3, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (3, 5, 2, 0) has been pruned...
[TMP] Stage (3, 5, 2, 1) has been pruned...
[TMP] Stage (3, 5, 2, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (5, 7, 2, 0) has been pruned...
[TMP] Stage (5, 7, 2, 1) has been pruned...
[TMP] Stage (5, 7, 2, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4], [5], [6, 7], [8], [9], [10], [11], [12, 13], [14], [15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO comm 0x35aa510 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO comm 0x484cac0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO comm 0x7baad10 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO comm 0x89f0040 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO comm 0x3a1f9c0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO comm 0x98b4400 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO comm 0x991d540 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO comm 0x8f74050 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO comm 0x6d796f0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO comm 0x4a4bc60 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO comm 0x474b1e0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO comm 0x7411e30 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO comm 0x74e4d40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO comm 0x6793b10 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO comm 0xa51fb00 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO comm 0x4f0b2e0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO comm 0xa77ceb0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO comm 0xa456c00 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO comm 0x825aa80 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO comm 0x38af0f0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO comm 0x42e4340 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO comm 0x66a6c80 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO comm 0x4f08660 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO comm 0x6df4b30 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO comm 0x38feed0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO comm 0x7cfba20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO comm 0x57b20d0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO comm 0x78e53c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=580396)[0m 
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=580396)[0m 
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=580396)[0m 
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO comm 0x35977a0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO comm 0x9c4ac40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=580396)[0m 
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO comm 0x69edcc0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO comm 0x9c536e0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=580396)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=580396)[0m 
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=580395)[0m 
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=580395)[0m 
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=580395)[0m 
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO comm 0x6145410 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=580395)[0m gpu2:580395:580395 [0] NCCL INFO comm 0x4561140 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
 - Compile (driver): 120.42 s
compilation time breakdown: {'stage-construction': '67.16', 'stage-construction-dp': '1.57', 'stage-construction-compilation': '6.12', 'stage-construction-profiling': '48.15'}
 - Compile (worker): 35.70 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439895 [1] NCCL INFO comm 0x7f9650ca2fd0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO comm 0x7f96887d86e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439893 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1439831, ip=192.168.0.32)[0m gpu17:1439831:1439831 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948416 [1] NCCL INFO comm 0x7f90119fa200 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO comm 0x7f9027805fc0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948414 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=948225, ip=192.168.0.26)[0m gpu11:948225:948225 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2117582, ip=192.168.0.18)[0m gpu3:2117582:2117582 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2117581, ip=192.168.0.18)[0m gpu3:2117581:2117581 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608268 [1] NCCL INFO comm 0x7f8987b32ac0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO comm 0x7f8993094a20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608266 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3608119, ip=192.168.0.39)[0m gpu24:3608119:3608119 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2308767, ip=192.168.0.27)[0m gpu12:2308767:2308767 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2308771, ip=192.168.0.27)[0m gpu12:2308771:2308771 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2516640, ip=192.168.0.38)[0m gpu23:2516640:2516640 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2516639, ip=192.168.0.38)[0m gpu23:2516639:2516639 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO comm 0x7f33f8ed8a00 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283702 [1] NCCL INFO comm 0x7f340a1f8e50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283700 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3283596, ip=192.168.0.31)[0m gpu16:3283596:3283596 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=580396)[0m gpu2:580396:580396 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 579.57 s

[472.21564960479736, 17.705631494522095, 17.543245553970337, 17.487486839294434, 17.38881778717041, 17.359957695007324, 17.233312129974365]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 88.5 s.
 - Average e2e iteration time: 17.700000762939453 s.
 - Total local training time: 87.01300048828125 s.
 - Average local iteration time: 17.402999877929688 s.
 - Max allocated memory among devices: 10.383 GB.
 - Compilation times:  {'stage-construction': 67.15564632415771, 'stage-construction-dp': 1.572178602218628, 'stage-construction-compilation': 6.122750997543335, 'stage-construction-profiling': 48.15325355529785}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 17.402563095092773
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_2B_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling wide_resnet_2B with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_2B_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.412 GB, invar_size=0.688 GB, outvar_size=0.251 GB, temp_buffer_size=0.473 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.017 GB, invar_size=0.347 GB, outvar_size=0.335 GB, temp_buffer_size=0.335 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.017 GB, invar_size=0.347 GB, outvar_size=0.335 GB, temp_buffer_size=0.335 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.262 GB, invar_size=0.425 GB, outvar_size=0.335 GB, temp_buffer_size=0.502 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.157, peak_memory=2.207 GB, invar_size=0.529 GB, outvar_size=0.670 GB, temp_buffer_size=1.008 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.479 GB, invar_size=0.055 GB, outvar_size=0.586 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.819 GB, invar_size=0.305 GB, outvar_size=0.670 GB, temp_buffer_size=0.843 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.041 GB, invar_size=0.862 GB, outvar_size=0.347 GB, temp_buffer_size=1.012 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=2.109 GB, invar_size=0.769 GB, outvar_size=0.335 GB, temp_buffer_size=1.005 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.041 GB, invar_size=0.862 GB, outvar_size=0.347 GB, temp_buffer_size=1.012 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.266, peak_memory=2.912 GB, invar_size=0.735 GB, outvar_size=0.837 GB, temp_buffer_size=1.340 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.638 GB, invar_size=0.465 GB, outvar_size=0.502 GB, temp_buffer_size=0.670 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.041 GB, invar_size=0.862 GB, outvar_size=0.347 GB, temp_buffer_size=1.012 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.454 GB, invar_size=0.533 GB, outvar_size=0.335 GB, temp_buffer_size=0.586 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.394 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.202, peak_memory=2.474 GB, invar_size=0.849 GB, outvar_size=0.257 GB, temp_buffer_size=1.457 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.136, peak_memory=2.615 GB, invar_size=1.292 GB, outvar_size=0.562 GB, temp_buffer_size=1.155 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.430 GB, invar_size=0.730 GB, outvar_size=0.168 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.194, peak_memory=1.800 GB, invar_size=0.041 GB, outvar_size=0.754 GB, temp_buffer_size=1.005 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.638 GB, invar_size=0.465 GB, outvar_size=0.502 GB, temp_buffer_size=0.670 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.430 GB, invar_size=0.730 GB, outvar_size=0.168 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.479 GB, invar_size=0.055 GB, outvar_size=0.586 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.113 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.136, peak_memory=1.684 GB, invar_size=0.595 GB, outvar_size=0.419 GB, temp_buffer_size=0.670 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.598 GB, invar_size=0.958 GB, outvar_size=0.168 GB, temp_buffer_size=0.473 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.819 GB, invar_size=0.305 GB, outvar_size=0.670 GB, temp_buffer_size=0.843 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.412 GB, invar_size=0.688 GB, outvar_size=0.251 GB, temp_buffer_size=0.473 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.815 GB, invar_size=1.029 GB, outvar_size=0.305 GB, temp_buffer_size=2.535 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=3.231 GB, invar_size=1.914 GB, outvar_size=0.957 GB, temp_buffer_size=1.149 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.113 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.419 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.289 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=2.555 GB, invar_size=1.292 GB, outvar_size=0.562 GB, temp_buffer_size=1.095 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.137 GB, invar_size=0.347 GB, outvar_size=0.335 GB, temp_buffer_size=0.455 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.137 GB, invar_size=0.347 GB, outvar_size=0.335 GB, temp_buffer_size=0.455 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.657 GB, invar_size=1.022 GB, outvar_size=0.046 GB, temp_buffer_size=3.636 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.454 GB, invar_size=0.533 GB, outvar_size=0.335 GB, temp_buffer_size=0.586 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.202, peak_memory=2.474 GB, invar_size=0.849 GB, outvar_size=0.257 GB, temp_buffer_size=1.457 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=3.527 GB, invar_size=1.265 GB, outvar_size=0.465 GB, temp_buffer_size=1.926 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.307 GB, invar_size=0.425 GB, outvar_size=0.335 GB, temp_buffer_size=0.547 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=0.990 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.296 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=0.990 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.296 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.107, peak_memory=1.981 GB, invar_size=1.053 GB, outvar_size=0.443 GB, temp_buffer_size=0.844 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.848 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=1.078 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.289 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=0.990 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.296 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.374, peak_memory=4.828 GB, invar_size=1.638 GB, outvar_size=0.400 GB, temp_buffer_size=2.856 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.193, peak_memory=2.567 GB, invar_size=1.274 GB, outvar_size=0.428 GB, temp_buffer_size=1.125 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.860 GB, invar_size=1.065 GB, outvar_size=0.532 GB, temp_buffer_size=1.460 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=3.359 GB, invar_size=1.098 GB, outvar_size=0.465 GB, temp_buffer_size=1.926 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=0.990 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.296 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=0.522, peak_memory=4.921 GB, invar_size=1.226 GB, outvar_size=0.529 GB, temp_buffer_size=3.192 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=2.668 GB, invar_size=1.460 GB, outvar_size=0.688 GB, temp_buffer_size=1.041 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.408, peak_memory=4.515 GB, invar_size=0.818 GB, outvar_size=0.023 GB, temp_buffer_size=3.697 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.289 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=0.990 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.296 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.107, peak_memory=1.981 GB, invar_size=1.053 GB, outvar_size=0.443 GB, temp_buffer_size=0.844 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.285 GB, invar_size=2.166 GB, outvar_size=0.126 GB, temp_buffer_size=0.993 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=0.990 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.296 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=2.794 GB, invar_size=1.544 GB, outvar_size=0.688 GB, temp_buffer_size=1.083 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.848 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=1.077 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.107, peak_memory=1.981 GB, invar_size=1.053 GB, outvar_size=0.443 GB, temp_buffer_size=0.844 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.107, peak_memory=1.981 GB, invar_size=1.053 GB, outvar_size=0.443 GB, temp_buffer_size=0.844 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=2.065 GB, invar_size=1.209 GB, outvar_size=0.210 GB, temp_buffer_size=0.646 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.848 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=1.077 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.815 GB, invar_size=1.029 GB, outvar_size=0.305 GB, temp_buffer_size=2.535 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.107, peak_memory=1.837 GB, invar_size=1.053 GB, outvar_size=0.443 GB, temp_buffer_size=0.700 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.107, peak_memory=1.837 GB, invar_size=1.053 GB, outvar_size=0.443 GB, temp_buffer_size=0.700 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.464 GB, invar_size=0.687 GB, outvar_size=0.046 GB, temp_buffer_size=2.777 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.107, peak_memory=1.837 GB, invar_size=1.053 GB, outvar_size=0.443 GB, temp_buffer_size=0.700 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=2.065 GB, invar_size=1.209 GB, outvar_size=0.210 GB, temp_buffer_size=0.646 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.848 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=1.077 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.107, peak_memory=1.837 GB, invar_size=1.053 GB, outvar_size=0.443 GB, temp_buffer_size=0.700 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.513 GB, invar_size=1.747 GB, outvar_size=0.084 GB, temp_buffer_size=0.682 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.320 GB, invar_size=3.243 GB, outvar_size=0.084 GB, temp_buffer_size=0.993 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.146 GB, invar_size=1.547 GB, outvar_size=0.042 GB, temp_buffer_size=0.556 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.513 GB, invar_size=1.747 GB, outvar_size=0.084 GB, temp_buffer_size=0.682 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.041 GB, invar_size=0.862 GB, outvar_size=0.347 GB, temp_buffer_size=1.012 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.945 GB, invar_size=2.968 GB, outvar_size=0.042 GB, temp_buffer_size=0.935 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.146 GB, invar_size=1.547 GB, outvar_size=0.042 GB, temp_buffer_size=0.556 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.253, peak_memory=3.589 GB, invar_size=1.202 GB, outvar_size=0.434 GB, temp_buffer_size=2.052 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.027 GB, invar_size=1.232 GB, outvar_size=0.532 GB, temp_buffer_size=1.460 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.040, peak_memory=8.142 GB, invar_size=4.373 GB, outvar_size=2.166 GB, temp_buffer_size=3.685 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.118, peak_memory=3.705 GB, invar_size=2.459 GB, outvar_size=1.125 GB, temp_buffer_size=1.163 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.070, peak_memory=4.232 GB, invar_size=3.052 GB, outvar_size=1.505 GB, temp_buffer_size=1.138 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.070, peak_memory=4.232 GB, invar_size=3.052 GB, outvar_size=1.505 GB, temp_buffer_size=1.138 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=10.340 GB, invar_size=5.936 GB, outvar_size=2.968 GB, temp_buffer_size=4.362 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=11.163 GB, invar_size=6.485 GB, outvar_size=3.242 GB, temp_buffer_size=4.594 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.118, peak_memory=3.705 GB, invar_size=2.459 GB, outvar_size=1.125 GB, temp_buffer_size=1.163 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.096, peak_memory=4.742 GB, invar_size=3.410 GB, outvar_size=1.663 GB, temp_buffer_size=1.248 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.096, peak_memory=4.742 GB, invar_size=3.410 GB, outvar_size=1.663 GB, temp_buffer_size=1.248 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 18.10 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.037 GB, invar_size=0.692 GB, outvar_size=0.670 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.597 GB, invar_size=0.425 GB, outvar_size=0.335 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.597 GB, invar_size=0.425 GB, outvar_size=0.335 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.753 GB, invar_size=1.385 GB, outvar_size=0.692 GB, temp_buffer_size=3.698 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.597 GB, invar_size=0.425 GB, outvar_size=0.335 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.357 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=1.137 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.037 GB, invar_size=0.692 GB, outvar_size=0.670 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.597 GB, invar_size=0.425 GB, outvar_size=0.335 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.746 GB, invar_size=0.849 GB, outvar_size=0.425 GB, temp_buffer_size=1.562 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.081 GB, invar_size=0.849 GB, outvar_size=0.425 GB, temp_buffer_size=1.896 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.081 GB, invar_size=0.849 GB, outvar_size=0.425 GB, temp_buffer_size=1.896 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.081 GB, invar_size=0.849 GB, outvar_size=0.425 GB, temp_buffer_size=1.897 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.753 GB, invar_size=1.385 GB, outvar_size=0.692 GB, temp_buffer_size=3.698 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.358 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=1.138 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=0.970 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.357 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=1.137 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=0.970 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=0.970 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.358 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=1.138 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.161 GB, invar_size=0.527 GB, outvar_size=0.168 GB, temp_buffer_size=0.467 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.570 GB, invar_size=1.520 GB, outvar_size=0.084 GB, temp_buffer_size=0.966 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=0.970 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.570 GB, invar_size=1.520 GB, outvar_size=0.084 GB, temp_buffer_size=0.966 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=0.970 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.582 GB, invar_size=1.575 GB, outvar_size=0.000 GB, temp_buffer_size=1.008 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.053 GB, outvar_size=0.526 GB, temp_buffer_size=0.970 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.744 GB, invar_size=3.039 GB, outvar_size=1.519 GB, temp_buffer_size=1.621 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.744 GB, invar_size=3.039 GB, outvar_size=1.519 GB, temp_buffer_size=1.621 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.582 GB, invar_size=1.575 GB, outvar_size=0.000 GB, temp_buffer_size=1.008 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.597 GB, invar_size=0.425 GB, outvar_size=0.335 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.597 GB, invar_size=0.425 GB, outvar_size=0.335 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.602 GB, invar_size=3.065 GB, outvar_size=1.574 GB, temp_buffer_size=1.454 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.602 GB, invar_size=3.065 GB, outvar_size=1.574 GB, temp_buffer_size=1.454 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.746 GB, invar_size=0.849 GB, outvar_size=0.425 GB, temp_buffer_size=1.562 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.081 GB, invar_size=0.849 GB, outvar_size=0.425 GB, temp_buffer_size=1.897 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.82 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 7, 3, 0) has been pruned...
[TMP] Stage (2, 7, 3, 1) has been pruned...
[TMP] Stage (2, 7, 3, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (3, 5, 2, 0) has been pruned...
[TMP] Stage (3, 5, 2, 1) has been pruned...
[TMP] Stage (3, 5, 2, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (5, 7, 2, 0) has been pruned...
[TMP] Stage (5, 7, 2, 1) has been pruned...
[TMP] Stage (5, 7, 2, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4], [5], [6, 7], [8], [9], [10], [11], [12, 13], [14], [15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO comm 0x4b256d0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO comm 0x4e0f330 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO comm 0x820e6b0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO comm 0x6b9c0e0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=591468)[0m 
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=591468)[0m 
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=591468)[0m 
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO comm 0xa92e130 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO comm 0x51bda20 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=591468)[0m 
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO comm 0xaa0c040 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=591467)[0m 
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO comm 0xadec8c0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=591468)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=591467)[0m 
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=591467)[0m 
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO comm 0x3cc84a0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=591467)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=591468)[0m 
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO comm 0x83318a0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=591467)[0m 
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO comm 0x484f9c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO comm 0x9907ac0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=591467)[0m 
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO comm 0x6e37700 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO comm 0x68dd270 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO comm 0x4206770 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO comm 0xa66dfb0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO comm 0x97544f0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO comm 0xa74bec0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO comm 0x4b465d0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO comm 0x7553ce0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO comm 0xa38b250 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO comm 0x3f188e0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO comm 0x4f815f0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO comm 0x962a000 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO comm 0x45ce730 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO comm 0xa4ec5e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO comm 0x99d13c0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO comm 0xa33ee20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO comm 0x3bd4f40 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO comm 0xa898900 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO comm 0x69c5b00 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO comm 0xa901a40 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1445691, ip=192.168.0.32)[0m gpu17:1445691:1445691 [0] NCCL INFO comm 0x351dbc0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO comm 0x683a7e0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 56.08 s
compilation time breakdown: {'stage-construction': '31.25', 'stage-construction-dp': '1.48', 'stage-construction-compilation': '6.63', 'stage-construction-profiling': '11.61'}
 - Compile (worker): 3.94 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO comm 0x7f26647a2b90 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285411 [1] NCCL INFO comm 0x7f26401404d0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285409 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3285344, ip=192.168.0.31)[0m gpu16:3285344:3285344 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316354 [1] NCCL INFO comm 0x7fdf5e2186c0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO comm 0x7fdf7e95d7a0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316352 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2316248, ip=192.168.0.27)[0m gpu12:2316248:2316248 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=591468)[0m gpu2:591468:591468 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=591467)[0m gpu2:591467:591467 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124385 [1] NCCL INFO comm 0x7f44f283f6a0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO comm 0x7f452210df00 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124383 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2124288, ip=192.168.0.18)[0m gpu3:2124288:2124288 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=953068, ip=192.168.0.26)[0m gpu11:953068:953068 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=953067, ip=192.168.0.26)[0m gpu11:953067:953067 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3614477, ip=192.168.0.39)[0m gpu24:3614477:3614477 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3614475, ip=192.168.0.39)[0m gpu24:3614475:3614475 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519198 [1] NCCL INFO comm 0x7f8e69fee140 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO comm 0x7f8e7dde00c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519196 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2519092, ip=192.168.0.38)[0m gpu23:2519092:2519092 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1445692, ip=192.168.0.32)[0m gpu17:1445692:1445692 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 258.94 s

[79.30770421028137, 29.57808780670166, 30.525776624679565, 29.275541305541992, 28.771398782730103, 28.804374933242798, 28.33207416534424]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 148.535 s.
 - Average e2e iteration time: 29.707000732421875 s.
 - Total local training time: 145.70899963378906 s.
 - Average local iteration time: 29.14200210571289 s.
 - Max allocated memory among devices: 13.808 GB.
 - Compilation times:  {'stage-construction': 31.254591703414917, 'stage-construction-dp': 1.480586051940918, 'stage-construction-compilation': 6.6318323612213135, 'stage-construction-profiling': 11.605401277542114}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 29.141834259033203
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_2B_512.pkl`...

------------------------------------------------------------------
- (3/3) Profiling wide_resnet_2B with batch size: 1024...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_2B_1024.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.911 GB, invar_size=0.064 GB, outvar_size=1.172 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.854 GB, invar_size=0.515 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.911 GB, invar_size=0.064 GB, outvar_size=1.172 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.854 GB, invar_size=0.515 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.172 GB, invar_size=0.856 GB, outvar_size=0.503 GB, temp_buffer_size=0.814 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.387, peak_memory=3.576 GB, invar_size=0.059 GB, outvar_size=1.507 GB, temp_buffer_size=2.010 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.124, peak_memory=2.435 GB, invar_size=0.760 GB, outvar_size=0.670 GB, temp_buffer_size=1.005 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.662 GB, invar_size=0.868 GB, outvar_size=0.670 GB, temp_buffer_size=1.125 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=2.405 GB, invar_size=1.065 GB, outvar_size=0.335 GB, temp_buffer_size=1.005 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.179, peak_memory=4.118 GB, invar_size=1.439 GB, outvar_size=0.670 GB, temp_buffer_size=2.010 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.145 GB, invar_size=0.800 GB, outvar_size=1.005 GB, temp_buffer_size=1.340 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.577 GB, invar_size=0.556 GB, outvar_size=1.340 GB, temp_buffer_size=1.681 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.577 GB, invar_size=0.556 GB, outvar_size=1.340 GB, temp_buffer_size=1.681 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.145 GB, invar_size=0.800 GB, outvar_size=1.005 GB, temp_buffer_size=1.340 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.530, peak_memory=5.760 GB, invar_size=1.405 GB, outvar_size=1.675 GB, temp_buffer_size=2.680 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.314, peak_memory=4.385 GB, invar_size=1.032 GB, outvar_size=1.340 GB, temp_buffer_size=2.013 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.172 GB, invar_size=0.856 GB, outvar_size=0.503 GB, temp_buffer_size=0.814 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.125 GB, outvar_size=0.335 GB, temp_buffer_size=0.730 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.269, peak_memory=3.108 GB, invar_size=0.930 GB, outvar_size=0.837 GB, temp_buffer_size=1.340 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.814, peak_memory=8.971 GB, invar_size=1.589 GB, outvar_size=0.023 GB, temp_buffer_size=7.382 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=1.711 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.682 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=6.823 GB, invar_size=1.282 GB, outvar_size=0.046 GB, temp_buffer_size=5.540 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.747 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.526 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.632 GB, invar_size=1.364 GB, outvar_size=0.514 GB, temp_buffer_size=1.933 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.125 GB, outvar_size=0.335 GB, temp_buffer_size=0.730 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.402, peak_memory=4.735 GB, invar_size=1.519 GB, outvar_size=0.425 GB, temp_buffer_size=2.881 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.632 GB, invar_size=1.364 GB, outvar_size=0.514 GB, temp_buffer_size=1.933 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.632 GB, invar_size=1.364 GB, outvar_size=0.514 GB, temp_buffer_size=1.933 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=4.392 GB, invar_size=2.249 GB, outvar_size=1.125 GB, temp_buffer_size=1.808 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.747 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.526 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=1.532 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.503 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.645 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=4.367 GB, invar_size=2.046 GB, outvar_size=0.856 GB, temp_buffer_size=1.986 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=6.374 GB, invar_size=1.935 GB, outvar_size=0.800 GB, temp_buffer_size=3.769 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=3.877 GB, invar_size=1.879 GB, outvar_size=0.856 GB, temp_buffer_size=1.663 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.645 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=5.211 GB, invar_size=1.735 GB, outvar_size=0.867 GB, temp_buffer_size=2.806 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.645 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.207, peak_memory=3.112 GB, invar_size=1.388 GB, outvar_size=0.526 GB, temp_buffer_size=1.557 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=6.709 GB, invar_size=2.270 GB, outvar_size=0.800 GB, temp_buffer_size=3.769 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=7.500 GB, invar_size=1.950 GB, outvar_size=0.556 GB, temp_buffer_size=5.047 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.378, peak_memory=4.574 GB, invar_size=2.028 GB, outvar_size=0.595 GB, temp_buffer_size=2.211 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.645 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=7.500 GB, invar_size=1.950 GB, outvar_size=0.556 GB, temp_buffer_size=5.047 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.001, peak_memory=9.178 GB, invar_size=1.952 GB, outvar_size=0.046 GB, temp_buffer_size=7.226 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.645 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=1.532 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.503 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.745, peak_memory=9.518 GB, invar_size=3.145 GB, outvar_size=0.735 GB, temp_buffer_size=5.703 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=1.042, peak_memory=9.778 GB, invar_size=2.399 GB, outvar_size=1.032 GB, temp_buffer_size=6.375 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.392 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.286 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.260, peak_memory=4.308 GB, invar_size=1.795 GB, outvar_size=0.730 GB, temp_buffer_size=2.178 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=4.560 GB, invar_size=2.417 GB, outvar_size=1.125 GB, temp_buffer_size=1.808 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.645 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=1.532 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.503 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.392 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.286 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.662 GB, invar_size=0.868 GB, outvar_size=0.670 GB, temp_buffer_size=1.125 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.626 GB, invar_size=2.250 GB, outvar_size=0.251 GB, temp_buffer_size=1.125 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.392 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.286 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.207, peak_memory=3.112 GB, invar_size=1.388 GB, outvar_size=0.526 GB, temp_buffer_size=1.557 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.392 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.286 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.207, peak_memory=2.800 GB, invar_size=1.388 GB, outvar_size=0.526 GB, temp_buffer_size=1.245 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.046 GB, invar_size=0.515 GB, outvar_size=0.670 GB, temp_buffer_size=0.861 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.147, peak_memory=2.657 GB, invar_size=1.376 GB, outvar_size=0.419 GB, temp_buffer_size=0.862 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.046 GB, invar_size=0.515 GB, outvar_size=0.670 GB, temp_buffer_size=0.861 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.225 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.119 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.124, peak_memory=2.479 GB, invar_size=0.760 GB, outvar_size=0.670 GB, temp_buffer_size=1.050 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.225 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.119 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.133, peak_memory=2.836 GB, invar_size=1.556 GB, outvar_size=0.419 GB, temp_buffer_size=0.862 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.225 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.119 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.207, peak_memory=2.800 GB, invar_size=1.388 GB, outvar_size=0.526 GB, temp_buffer_size=1.245 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.619 GB, invar_size=3.327 GB, outvar_size=0.168 GB, temp_buffer_size=1.125 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.102 GB, invar_size=3.010 GB, outvar_size=0.084 GB, temp_buffer_size=1.008 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.225 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.119 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.016 GB, invar_size=1.915 GB, outvar_size=0.168 GB, temp_buffer_size=0.933 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.016 GB, invar_size=1.915 GB, outvar_size=0.168 GB, temp_buffer_size=0.933 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=2.397 GB, invar_size=1.631 GB, outvar_size=0.084 GB, temp_buffer_size=0.682 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=2.397 GB, invar_size=1.631 GB, outvar_size=0.084 GB, temp_buffer_size=0.682 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.040, peak_memory=8.435 GB, invar_size=4.582 GB, outvar_size=2.249 GB, temp_buffer_size=3.685 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.402, peak_memory=4.735 GB, invar_size=1.519 GB, outvar_size=0.425 GB, temp_buffer_size=2.881 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=11.510 GB, invar_size=6.652 GB, outvar_size=3.326 GB, temp_buffer_size=4.690 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.130, peak_memory=4.611 GB, invar_size=3.178 GB, outvar_size=1.547 GB, temp_buffer_size=1.350 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.211, peak_memory=4.715 GB, invar_size=2.836 GB, outvar_size=1.208 GB, temp_buffer_size=1.711 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=10.465 GB, invar_size=6.020 GB, outvar_size=3.010 GB, temp_buffer_size=4.362 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.632 GB, invar_size=1.364 GB, outvar_size=0.514 GB, temp_buffer_size=1.933 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.168, peak_memory=5.639 GB, invar_size=3.661 GB, outvar_size=1.747 GB, temp_buffer_size=1.810 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.168, peak_memory=5.639 GB, invar_size=3.661 GB, outvar_size=1.747 GB, temp_buffer_size=1.810 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.130, peak_memory=4.611 GB, invar_size=3.178 GB, outvar_size=1.547 GB, temp_buffer_size=1.350 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=4.822 GB, invar_size=3.195 GB, outvar_size=1.388 GB, temp_buffer_size=1.460 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=5.546 GB, invar_size=2.070 GB, outvar_size=0.867 GB, temp_buffer_size=2.806 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.502, peak_memory=6.939 GB, invar_size=2.207 GB, outvar_size=0.769 GB, temp_buffer_size=4.062 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 17.56 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.052 GB, invar_size=1.362 GB, outvar_size=1.340 GB, temp_buffer_size=3.350 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.105 GB, invar_size=0.760 GB, outvar_size=0.670 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=11.448 GB, invar_size=2.725 GB, outvar_size=1.362 GB, temp_buffer_size=7.383 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.105 GB, invar_size=0.760 GB, outvar_size=0.670 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.105 GB, invar_size=0.760 GB, outvar_size=0.670 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.781 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=2.058 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.105 GB, invar_size=0.760 GB, outvar_size=0.670 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.052 GB, invar_size=1.362 GB, outvar_size=1.340 GB, temp_buffer_size=3.350 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.259 GB, invar_size=1.519 GB, outvar_size=0.760 GB, temp_buffer_size=3.069 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.928 GB, invar_size=1.519 GB, outvar_size=0.760 GB, temp_buffer_size=3.739 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.928 GB, invar_size=1.519 GB, outvar_size=0.760 GB, temp_buffer_size=3.739 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=11.448 GB, invar_size=2.725 GB, outvar_size=1.362 GB, temp_buffer_size=7.383 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.929 GB, invar_size=1.519 GB, outvar_size=0.760 GB, temp_buffer_size=3.739 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.781 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=2.058 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.782 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=2.060 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.782 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=2.060 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.447 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=1.725 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.447 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=1.725 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.795 GB, invar_size=0.694 GB, outvar_size=0.335 GB, temp_buffer_size=0.766 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.447 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=1.725 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.604 GB, outvar_size=0.168 GB, temp_buffer_size=1.071 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.447 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=1.725 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.604 GB, outvar_size=0.168 GB, temp_buffer_size=1.071 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.447 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=1.725 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.813 GB, invar_size=1.658 GB, outvar_size=0.000 GB, temp_buffer_size=1.154 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.105 GB, invar_size=0.760 GB, outvar_size=0.670 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.105 GB, invar_size=0.760 GB, outvar_size=0.670 GB, temp_buffer_size=1.675 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.372 GB, invar_size=3.206 GB, outvar_size=1.603 GB, temp_buffer_size=1.998 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.447 GB, invar_size=1.388 GB, outvar_size=0.694 GB, temp_buffer_size=1.725 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.372 GB, invar_size=3.206 GB, outvar_size=1.603 GB, temp_buffer_size=1.998 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.813 GB, invar_size=1.658 GB, outvar_size=0.000 GB, temp_buffer_size=1.154 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.979 GB, invar_size=3.148 GB, outvar_size=1.658 GB, temp_buffer_size=1.663 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.979 GB, invar_size=3.148 GB, outvar_size=1.658 GB, temp_buffer_size=1.663 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.929 GB, invar_size=1.519 GB, outvar_size=0.760 GB, temp_buffer_size=3.739 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.259 GB, invar_size=1.519 GB, outvar_size=0.760 GB, temp_buffer_size=3.069 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.77 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 7, 3, 0) has been pruned...
[TMP] Stage (2, 7, 3, 1) has been pruned...
[TMP] Stage (2, 7, 3, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (3, 5, 2, 0) has been pruned...
[TMP] Stage (3, 5, 2, 1) has been pruned...
[TMP] Stage (3, 5, 2, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (5, 7, 2, 0) has been pruned...
[TMP] Stage (5, 7, 2, 1) has been pruned...
[TMP] Stage (5, 7, 2, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4], [5], [6, 7], [8], [9], [10], [11], [12, 13], [14], [15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO comm 0x474aa40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO comm 0x465b810 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO comm 0x6983ae0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=601538)[0m 
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO comm 0x7a0e1b0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=601538)[0m 
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=601538)[0m 
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO comm 0x4f44ff0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO comm 0xaa5a5b0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m 
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO comm 0x7ff45b0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=601538)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO comm 0xa9f1470 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=601539)[0m 
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=601539)[0m 
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=601539)[0m 
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO comm 0x3747690 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=601538)[0m 
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO comm 0x6ec0b00 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=601539)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=601539)[0m 
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO comm 0x4f4f9b0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO comm 0x938bb40 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=601539)[0m 
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO comm 0x68bb420 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO comm 0x6e1d850 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO comm 0xb489920 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO comm 0x422f180 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO comm 0x62458e0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO comm 0xb4cbb60 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO comm 0x5194450 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO comm 0x6cc8670 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO comm 0x3896db0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO comm 0x7cafcc0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO comm 0x3bf5b20 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO comm 0x8dec000 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO comm 0x4731a30 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO comm 0x670af70 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO comm 0x9b2eda0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO comm 0x5f07390 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO comm 0xa9f3f40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO comm 0x4ff2020 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO comm 0xaa5d080 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO comm 0xa540c90 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO comm 0xa38c190 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3286240, ip=192.168.0.31)[0m gpu16:3286240:3286240 [0] NCCL INFO comm 0x402cb00 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 54.30 s
compilation time breakdown: {'stage-construction': '30.68', 'stage-construction-dp': '1.49', 'stage-construction-compilation': '6.50', 'stage-construction-profiling': '11.71'}
 - Compile (worker): 4.31 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322798 [1] NCCL INFO comm 0x7ee3fe4f1900 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO comm 0x7ee4182c60e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322796 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2322649, ip=192.168.0.27)[0m gpu12:2322649:2322649 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451715 [1] NCCL INFO comm 0x7f989d1a2f10 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO comm 0x7f98b887c220 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451713 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1451609, ip=192.168.0.32)[0m gpu17:1451609:1451609 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=601538)[0m gpu2:601538:601538 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=601539)[0m gpu2:601539:601539 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620653 [1] NCCL INFO comm 0x7f08fb93dc60 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO comm 0x7f0903879ae0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620651 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3620505, ip=192.168.0.39)[0m gpu24:3620505:3620505 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2520579, ip=192.168.0.38)[0m gpu23:2520579:2520579 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2520578, ip=192.168.0.38)[0m gpu23:2520578:2520578 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2129904, ip=192.168.0.18)[0m gpu3:2129904:2129904 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2129905, ip=192.168.0.18)[0m gpu3:2129905:2129905 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO comm 0x7fda4cab2630 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957714 [1] NCCL INFO comm 0x7fda2cfdc6b0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957712 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=957608, ip=192.168.0.26)[0m gpu11:957608:957608 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3286239, ip=192.168.0.31)[0m gpu16:3286239:3286239 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 432.49 s

[104.66230344772339, 55.95629858970642, 54.74226379394531, 52.38373255729675, 51.63697695732117, 51.14173126220703, 50.86933755874634]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 268.425 s.
 - Average e2e iteration time: 53.685001373291016 s.
 - Total local training time: 260.7740173339844 s.
 - Average local iteration time: 52.15500259399414 s.
 - Max allocated memory among devices: 27.207 GB.
 - Compilation times:  {'stage-construction': 30.68000626564026, 'stage-construction-dp': 1.4947140216827393, 'stage-construction-compilation': 6.504796266555786, 'stage-construction-profiling': 11.711137533187866}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 52.15481185913086
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_2B_1024.pkl`...
