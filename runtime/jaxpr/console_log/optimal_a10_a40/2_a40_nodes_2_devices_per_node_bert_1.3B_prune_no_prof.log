
------------------------------------------------------------------
- (1/3) Profiling bert_1.3B with batch size: 128...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal_prune_no_prof/bert_1.3B_128.pkl`, creating it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f7136ff0820>
    dtype = float16
) 

[I] Compiling and executing model with timeout = 3600 s...
[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
[TMP] The range of stage num is (min #stage -> max #stage): 2 -> 4
[I] Coarsening pipeline layers from 16 to 4...
[I] Coarsened scheme: [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2))
[TMP] The range of device num per stage is (min #gpu -> max #gpu): 1 -> 2
- Profiling for submesh 2 (2, 2):

[TMP] Since submesh device num (4) > max gpu num per stage (2) or < min gpu num per stage (1), skip compiling and profiling on pruned stages and forge infeasible profiling results for them ...
--------------------------------------------------

- Profiling for submesh 1 (1, 2):
[TMP] Layer indices [0, 1, 2, 3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [0, 1, 2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
[TMP] Layer indices [8, 9, 10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.111, peak_memory=1.870 GB, invar_size=1.354 GB, outvar_size=0.125 GB, temp_buffer_size=0.391 GB, available_memory=35.446 GB)
result[(0, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.171, peak_memory=1.302 GB, invar_size=0.677 GB, outvar_size=0.234 GB, temp_buffer_size=0.391 GB, available_memory=35.446 GB)
result[(0, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.305, peak_memory=4.397 GB, invar_size=2.833 GB, outvar_size=1.354 GB, temp_buffer_size=1.564 GB, available_memory=35.242 GB)
result[(0, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.129, peak_memory=1.802 GB, invar_size=1.256 GB, outvar_size=0.125 GB, temp_buffer_size=0.422 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.414, peak_memory=4.334 GB, invar_size=1.604 GB, outvar_size=0.677 GB, temp_buffer_size=2.729 GB, available_memory=35.446 GB)
result[(4, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.102, peak_memory=1.657 GB, invar_size=1.235 GB, outvar_size=0.125 GB, temp_buffer_size=0.297 GB, available_memory=35.446 GB)
result[(0, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.302, peak_memory=4.219 GB, invar_size=2.652 GB, outvar_size=1.256 GB, temp_buffer_size=1.567 GB, available_memory=35.242 GB)
result[(4, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.171, peak_memory=1.188 GB, invar_size=0.641 GB, outvar_size=0.234 GB, temp_buffer_size=0.313 GB, available_memory=35.242 GB)
result[(4, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.289, peak_memory=4.144 GB, invar_size=2.579 GB, outvar_size=1.235 GB, temp_buffer_size=1.549 GB, available_memory=35.446 GB)
result[(4, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.102, peak_memory=1.657 GB, invar_size=1.235 GB, outvar_size=0.125 GB, temp_buffer_size=0.297 GB, available_memory=35.446 GB)
result[(4, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.434, peak_memory=3.760 GB, invar_size=1.501 GB, outvar_size=0.641 GB, temp_buffer_size=2.227 GB, available_memory=35.242 GB)
result[(4, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.289, peak_memory=4.144 GB, invar_size=2.579 GB, outvar_size=1.235 GB, temp_buffer_size=1.549 GB, available_memory=35.446 GB)
result[(8, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=1.454 GB, invar_size=1.047 GB, outvar_size=0.109 GB, temp_buffer_size=0.297 GB, available_memory=35.242 GB)
result[(8, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.150, peak_memory=1.079 GB, invar_size=0.547 GB, outvar_size=0.219 GB, temp_buffer_size=0.313 GB, available_memory=35.242 GB)
result[(8, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.290, peak_memory=4.253 GB, invar_size=2.720 GB, outvar_size=1.313 GB, temp_buffer_size=1.517 GB, available_memory=35.446 GB)
result[(8, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.087, peak_memory=1.454 GB, invar_size=1.047 GB, outvar_size=0.109 GB, temp_buffer_size=0.297 GB, available_memory=35.446 GB)
result[(8, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.411, peak_memory=3.967 GB, invar_size=1.548 GB, outvar_size=0.680 GB, temp_buffer_size=2.387 GB, available_memory=35.242 GB)
result[(8, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.290, peak_memory=4.057 GB, invar_size=2.525 GB, outvar_size=1.215 GB, temp_buffer_size=1.516 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 98.50 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
[TMP] Layer indices [0, 1, 2] violates the layer coarsening rules, pruned.
[TMP] Layer indices [1, 2, 3, 4] violates the layer coarsening rules, pruned.
[TMP] Layer indices [2, 3, 4, 5] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [3, 4, 5, 6, 7] violates the layer coarsening rules, pruned.
[TMP] Layer indices [4, 5, 6] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8] violates the layer coarsening rules, pruned.
[TMP] Layer indices [5, 6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9] violates the layer coarsening rules, pruned.
[TMP] Layer indices [6, 7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10] violates the layer coarsening rules, pruned.
[TMP] Layer indices [7, 8, 9, 10, 11] violates the layer coarsening rules, pruned.
[TMP] Layer indices [8, 9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12] violates the layer coarsening rules, pruned.
[TMP] Layer indices [9, 10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [10, 11, 12, 13] violates the layer coarsening rules, pruned.
[TMP] Layer indices [10, 11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [11, 12, 13, 14] violates the layer coarsening rules, pruned.
[TMP] Layer indices [11, 12, 13, 14, 15] violates the layer coarsening rules, pruned.
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.106, peak_memory=1.604 GB, invar_size=0.698 GB, outvar_size=0.125 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.126, peak_memory=1.604 GB, invar_size=0.698 GB, outvar_size=0.125 GB, temp_buffer_size=0.781 GB, available_memory=35.242 GB)
result[(4, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.109, peak_memory=1.407 GB, invar_size=0.688 GB, outvar_size=0.125 GB, temp_buffer_size=0.594 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.275, peak_memory=5.496 GB, invar_size=1.520 GB, outvar_size=0.698 GB, temp_buffer_size=3.975 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.280, peak_memory=5.496 GB, invar_size=1.520 GB, outvar_size=0.698 GB, temp_buffer_size=3.975 GB, available_memory=35.242 GB)
result[(4, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.109, peak_memory=1.407 GB, invar_size=0.688 GB, outvar_size=0.125 GB, temp_buffer_size=0.594 GB, available_memory=35.446 GB)
result[(8, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=1.313 GB, invar_size=0.594 GB, outvar_size=0.125 GB, temp_buffer_size=0.594 GB, available_memory=35.242 GB)
result[(4, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.302, peak_memory=5.820 GB, invar_size=1.469 GB, outvar_size=0.688 GB, temp_buffer_size=4.319 GB, available_memory=35.242 GB)
result[(8, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.101, peak_memory=1.313 GB, invar_size=0.594 GB, outvar_size=0.125 GB, temp_buffer_size=0.594 GB, available_memory=35.242 GB)
result[(4, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.298, peak_memory=5.820 GB, invar_size=1.469 GB, outvar_size=0.688 GB, temp_buffer_size=4.319 GB, available_memory=35.446 GB)
result[(12, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=1.188 GB, invar_size=0.500 GB, outvar_size=0.094 GB, temp_buffer_size=0.594 GB, available_memory=35.242 GB)
result[(8, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.260, peak_memory=4.948 GB, invar_size=1.282 GB, outvar_size=0.594 GB, temp_buffer_size=3.635 GB, available_memory=35.446 GB)
result[(12, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.094, peak_memory=1.188 GB, invar_size=0.500 GB, outvar_size=0.094 GB, temp_buffer_size=0.594 GB, available_memory=35.242 GB)
result[(8, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.267, peak_memory=4.948 GB, invar_size=1.282 GB, outvar_size=0.594 GB, temp_buffer_size=3.635 GB, available_memory=35.242 GB)
result[(12, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.307, peak_memory=5.696 GB, invar_size=1.595 GB, outvar_size=0.766 GB, temp_buffer_size=4.070 GB, available_memory=35.446 GB)
result[(12, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.306, peak_memory=5.696 GB, invar_size=1.595 GB, outvar_size=0.766 GB, temp_buffer_size=4.070 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 37.92 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 2, 0, 0) has been pruned...
[TMP] Stage (0, 2, 0, 1) has been pruned...
[TMP] Stage (0, 6, 1, 0) has been pruned...
[TMP] Stage (0, 6, 1, 1) has been pruned...
[TMP] Stage (0, 6, 1, 2) has been pruned...
[TMP] Stage (0, 8, 1, 0) has been pruned...
[TMP] Stage (0, 8, 1, 1) has been pruned...
[TMP] Stage (0, 8, 1, 2) has been pruned...
[TMP] Stage (0, 15, 2, 0) has been pruned...
[TMP] Stage (0, 15, 2, 1) has been pruned...
[TMP] Stage (0, 15, 2, 2) has been pruned...
[TMP] Stage (1, 4, 0, 0) has been pruned...
[TMP] Stage (1, 4, 0, 1) has been pruned...
[TMP] Stage (1, 7, 1, 0) has been pruned...
[TMP] Stage (1, 7, 1, 1) has been pruned...
[TMP] Stage (1, 7, 1, 2) has been pruned...
[TMP] Stage (1, 8, 1, 0) has been pruned...
[TMP] Stage (1, 8, 1, 1) has been pruned...
[TMP] Stage (1, 8, 1, 2) has been pruned...
[TMP] Stage (1, 9, 1, 0) has been pruned...
[TMP] Stage (1, 9, 1, 1) has been pruned...
[TMP] Stage (1, 9, 1, 2) has been pruned...
[TMP] Stage (1, 10, 1, 0) has been pruned...
[TMP] Stage (1, 10, 1, 1) has been pruned...
[TMP] Stage (1, 10, 1, 2) has been pruned...
[TMP] Stage (2, 5, 0, 0) has been pruned...
[TMP] Stage (2, 5, 0, 1) has been pruned...
[TMP] Stage (2, 8, 1, 0) has been pruned...
[TMP] Stage (2, 8, 1, 1) has been pruned...
[TMP] Stage (2, 8, 1, 2) has been pruned...
[TMP] Stage (2, 9, 1, 0) has been pruned...
[TMP] Stage (2, 9, 1, 1) has been pruned...
[TMP] Stage (2, 9, 1, 2) has been pruned...
[TMP] Stage (2, 10, 1, 0) has been pruned...
[TMP] Stage (2, 10, 1, 1) has been pruned...
[TMP] Stage (2, 10, 1, 2) has been pruned...
[TMP] Stage (2, 11, 1, 0) has been pruned...
[TMP] Stage (2, 11, 1, 1) has been pruned...
[TMP] Stage (2, 11, 1, 2) has been pruned...
[TMP] Stage (3, 6, 0, 0) has been pruned...
[TMP] Stage (3, 6, 0, 1) has been pruned...
[TMP] Stage (3, 7, 0, 0) has been pruned...
[TMP] Stage (3, 7, 0, 1) has been pruned...
[TMP] Stage (3, 9, 1, 0) has been pruned...
[TMP] Stage (3, 9, 1, 1) has been pruned...
[TMP] Stage (3, 9, 1, 2) has been pruned...
[TMP] Stage (3, 10, 1, 0) has been pruned...
[TMP] Stage (3, 10, 1, 1) has been pruned...
[TMP] Stage (3, 10, 1, 2) has been pruned...
[TMP] Stage (3, 11, 1, 0) has been pruned...
[TMP] Stage (3, 11, 1, 1) has been pruned...
[TMP] Stage (3, 11, 1, 2) has been pruned...
[TMP] Stage (3, 12, 1, 0) has been pruned...
[TMP] Stage (3, 12, 1, 1) has been pruned...
[TMP] Stage (3, 12, 1, 2) has been pruned...
[TMP] Stage (4, 6, 0, 0) has been pruned...
[TMP] Stage (4, 6, 0, 1) has been pruned...
[TMP] Stage (4, 10, 1, 0) has been pruned...
[TMP] Stage (4, 10, 1, 1) has been pruned...
[TMP] Stage (4, 10, 1, 2) has been pruned...
[TMP] Stage (4, 12, 1, 0) has been pruned...
[TMP] Stage (4, 12, 1, 1) has been pruned...
[TMP] Stage (4, 12, 1, 2) has been pruned...
[TMP] Stage (4, 13, 1, 0) has been pruned...
[TMP] Stage (4, 13, 1, 1) has been pruned...
[TMP] Stage (4, 13, 1, 2) has been pruned...
[TMP] Stage (5, 8, 0, 0) has been pruned...
[TMP] Stage (5, 8, 0, 1) has been pruned...
[TMP] Stage (5, 9, 0, 0) has been pruned...
[TMP] Stage (5, 9, 0, 1) has been pruned...
[TMP] Stage (5, 11, 1, 0) has been pruned...
[TMP] Stage (5, 11, 1, 1) has been pruned...
[TMP] Stage (5, 11, 1, 2) has been pruned...
[TMP] Stage (5, 12, 1, 0) has been pruned...
[TMP] Stage (5, 12, 1, 1) has been pruned...
[TMP] Stage (5, 12, 1, 2) has been pruned...
[TMP] Stage (5, 13, 1, 0) has been pruned...
[TMP] Stage (5, 13, 1, 1) has been pruned...
[TMP] Stage (5, 13, 1, 2) has been pruned...
[TMP] Stage (5, 14, 1, 0) has been pruned...
[TMP] Stage (5, 14, 1, 1) has been pruned...
[TMP] Stage (5, 14, 1, 2) has been pruned...
[TMP] Stage (6, 9, 0, 0) has been pruned...
[TMP] Stage (6, 9, 0, 1) has been pruned...
[TMP] Stage (6, 10, 0, 0) has been pruned...
[TMP] Stage (6, 10, 0, 1) has been pruned...
[TMP] Stage (6, 12, 1, 0) has been pruned...
[TMP] Stage (6, 12, 1, 1) has been pruned...
[TMP] Stage (6, 12, 1, 2) has been pruned...
[TMP] Stage (6, 13, 1, 0) has been pruned...
[TMP] Stage (6, 13, 1, 1) has been pruned...
[TMP] Stage (6, 13, 1, 2) has been pruned...
[TMP] Stage (6, 14, 1, 0) has been pruned...
[TMP] Stage (6, 14, 1, 1) has been pruned...
[TMP] Stage (6, 14, 1, 2) has been pruned...
[TMP] Stage (6, 15, 1, 0) has been pruned...
[TMP] Stage (6, 15, 1, 1) has been pruned...
[TMP] Stage (6, 15, 1, 2) has been pruned...
[TMP] Stage (7, 10, 0, 0) has been pruned...
[TMP] Stage (7, 10, 0, 1) has been pruned...
[TMP] Stage (7, 11, 0, 0) has been pruned...
[TMP] Stage (7, 11, 0, 1) has been pruned...
[TMP] Stage (7, 13, 1, 0) has been pruned...
[TMP] Stage (7, 13, 1, 1) has been pruned...
[TMP] Stage (7, 13, 1, 2) has been pruned...
[TMP] Stage (7, 14, 1, 0) has been pruned...
[TMP] Stage (7, 14, 1, 1) has been pruned...
[TMP] Stage (7, 14, 1, 2) has been pruned...
[TMP] Stage (7, 15, 1, 0) has been pruned...
[TMP] Stage (7, 15, 1, 1) has been pruned...
[TMP] Stage (7, 15, 1, 2) has been pruned...
[TMP] Stage (8, 12, 0, 0) has been pruned...
[TMP] Stage (8, 12, 0, 1) has been pruned...
[TMP] Stage (8, 14, 1, 0) has been pruned...
[TMP] Stage (8, 14, 1, 1) has been pruned...
[TMP] Stage (8, 14, 1, 2) has been pruned...
[TMP] Stage (9, 12, 0, 0) has been pruned...
[TMP] Stage (9, 12, 0, 1) has been pruned...
[TMP] Stage (9, 13, 0, 0) has been pruned...
[TMP] Stage (9, 13, 0, 1) has been pruned...
[TMP] Stage (9, 15, 1, 0) has been pruned...
[TMP] Stage (9, 15, 1, 1) has been pruned...
[TMP] Stage (9, 15, 1, 2) has been pruned...
[TMP] Stage (10, 13, 0, 0) has been pruned...
[TMP] Stage (10, 13, 0, 1) has been pruned...
[TMP] Stage (10, 14, 0, 0) has been pruned...
[TMP] Stage (10, 14, 0, 1) has been pruned...
[TMP] Stage (11, 14, 0, 0) has been pruned...
[TMP] Stage (11, 14, 0, 1) has been pruned...
[TMP] Stage (11, 15, 0, 0) has been pruned...
[TMP] Stage (11, 15, 0, 1) has been pruned...
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {}]
 - Compile (driver): 193.54 s
compilation time breakdown: {'stage-construction': '142.32', 'stage-construction-dp': '1.41', 'stage-construction-compilation': '26.89', 'stage-construction-profiling': '87.41'}
 - Compile (worker): 9.85 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 61.57 s

[12.045757532119751, 7.043131351470947, 7.045001983642578, 6.985450744628906, 7.025606632232666, 6.98356556892395, 6.981491565704346]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 37.541 s.
 - Average e2e iteration time: 7.508000373840332 s.
 - Total local training time: 35.020999908447266 s.
 - Average local iteration time: 7.004000186920166 s.
 - Max allocated memory among devices: 10.117 GB.
 - Compilation times:  {'stage-construction': 142.31811928749084, 'stage-construction-dp': 1.4062726497650146, 'stage-construction-compilation': 26.894147157669067, 'stage-construction-profiling': 87.41119360923767}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_2_d`: 7.004222869873047
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal_prune_no_prof/bert_1.3B_128.pkl`...

------------------------------------------------------------------
- (2/3) Profiling bert_1.3B with batch size: 256...
------------------------------------------------------------------
