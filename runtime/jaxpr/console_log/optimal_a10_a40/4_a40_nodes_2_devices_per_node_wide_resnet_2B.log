
------------------------------------------------------------------
- (1/3) Profiling wide_resnet_2B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_2B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 3, 0), 0] = ModuleProfileResult(compute_cost=0.074, peak_memory=8.619 GB, invar_size=7.322 GB, outvar_size=0.272 GB, temp_buffer_size=1.025 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 0] = ModuleProfileResult(compute_cost=0.169, peak_memory=4.638 GB, invar_size=3.663 GB, outvar_size=0.335 GB, temp_buffer_size=0.641 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 0] = ModuleProfileResult(compute_cost=11.622, peak_memory=2.360 GB, invar_size=1.726 GB, outvar_size=0.272 GB, temp_buffer_size=0.362 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 1] = ModuleProfileResult(compute_cost=11.621, peak_memory=24.284 GB, invar_size=14.914 GB, outvar_size=7.320 GB, temp_buffer_size=9.371 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 1] = ModuleProfileResult(compute_cost=5.519, peak_memory=13.298 GB, invar_size=7.658 GB, outvar_size=3.661 GB, temp_buffer_size=5.640 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 1] = ModuleProfileResult(compute_cost=20.246, peak_memory=5.147 GB, invar_size=3.721 GB, outvar_size=1.724 GB, temp_buffer_size=1.425 GB, available_memory=35.242 GB)
Profiling for submesh 3 (4, 2) takes 64.19 seconds
--------------------------------------------------
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.058 GB, invar_size=0.388 GB, outvar_size=0.335 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.173, peak_memory=1.746 GB, invar_size=0.490 GB, outvar_size=0.586 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.255 GB, invar_size=0.477 GB, outvar_size=0.419 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.255 GB, invar_size=0.477 GB, outvar_size=0.419 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.190 GB, invar_size=0.181 GB, outvar_size=0.544 GB, temp_buffer_size=0.465 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.190 GB, invar_size=0.181 GB, outvar_size=0.544 GB, temp_buffer_size=0.465 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.058 GB, invar_size=0.388 GB, outvar_size=0.335 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.277, peak_memory=1.617 GB, invar_size=0.097 GB, outvar_size=0.796 GB, temp_buffer_size=0.724 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.154, peak_memory=1.617 GB, invar_size=0.445 GB, outvar_size=0.503 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.065 GB, invar_size=0.455 GB, outvar_size=0.251 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.065, peak_memory=1.152 GB, invar_size=0.479 GB, outvar_size=0.251 GB, temp_buffer_size=0.422 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=2.163 GB, invar_size=1.026 GB, outvar_size=0.387 GB, temp_buffer_size=0.969 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.434 GB, invar_size=0.377 GB, outvar_size=0.503 GB, temp_buffer_size=0.555 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.239, peak_memory=1.816 GB, invar_size=0.377 GB, outvar_size=0.670 GB, temp_buffer_size=0.769 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=2.366 GB, invar_size=1.206 GB, outvar_size=0.477 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=2.366 GB, invar_size=1.206 GB, outvar_size=0.477 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.065 GB, invar_size=0.455 GB, outvar_size=0.251 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.284, peak_memory=2.831 GB, invar_size=1.231 GB, outvar_size=0.322 GB, temp_buffer_size=1.433 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=2.334 GB, invar_size=0.901 GB, outvar_size=0.176 GB, temp_buffer_size=1.433 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.215 GB, invar_size=0.545 GB, outvar_size=0.335 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=2.079 GB, invar_size=0.942 GB, outvar_size=0.387 GB, temp_buffer_size=0.969 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.236, peak_memory=2.657 GB, invar_size=1.057 GB, outvar_size=0.277 GB, temp_buffer_size=1.432 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.379, peak_memory=2.836 GB, invar_size=0.981 GB, outvar_size=0.088 GB, temp_buffer_size=1.855 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.219, peak_memory=1.642 GB, invar_size=0.332 GB, outvar_size=0.586 GB, temp_buffer_size=0.724 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.171 GB, invar_size=0.287 GB, outvar_size=0.419 GB, temp_buffer_size=0.465 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.084, peak_memory=1.280 GB, invar_size=0.524 GB, outvar_size=0.335 GB, temp_buffer_size=0.422 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.418 GB, invar_size=0.985 GB, outvar_size=0.176 GB, temp_buffer_size=1.433 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.215 GB, invar_size=0.545 GB, outvar_size=0.335 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=1.948 GB, invar_size=0.993 GB, outvar_size=0.455 GB, temp_buffer_size=0.787 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.434 GB, invar_size=0.377 GB, outvar_size=0.503 GB, temp_buffer_size=0.555 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.171 GB, invar_size=0.287 GB, outvar_size=0.419 GB, temp_buffer_size=0.465 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.475 GB, invar_size=0.784 GB, outvar_size=0.293 GB, temp_buffer_size=0.398 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.176, peak_memory=2.088 GB, invar_size=0.873 GB, outvar_size=0.311 GB, temp_buffer_size=1.047 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.022, peak_memory=1.281 GB, invar_size=0.629 GB, outvar_size=0.293 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.248 GB, invar_size=0.694 GB, outvar_size=0.209 GB, temp_buffer_size=0.344 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=1.948 GB, invar_size=0.993 GB, outvar_size=0.455 GB, temp_buffer_size=0.787 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.124, peak_memory=1.230 GB, invar_size=0.518 GB, outvar_size=0.377 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=2.653 GB, invar_size=1.215 GB, outvar_size=0.377 GB, temp_buffer_size=1.313 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=1.185 GB, invar_size=0.473 GB, outvar_size=0.377 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=0.985 GB, invar_size=0.539 GB, outvar_size=0.209 GB, temp_buffer_size=0.236 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=2.570 GB, invar_size=1.131 GB, outvar_size=0.377 GB, temp_buffer_size=1.313 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.390, peak_memory=3.025 GB, invar_size=1.173 GB, outvar_size=0.377 GB, temp_buffer_size=1.601 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=2.250 GB, invar_size=1.256 GB, outvar_size=0.545 GB, temp_buffer_size=0.826 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.614 GB, invar_size=1.053 GB, outvar_size=0.251 GB, temp_buffer_size=0.310 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.147, peak_memory=1.406 GB, invar_size=0.653 GB, outvar_size=0.419 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.300 GB, invar_size=0.868 GB, outvar_size=0.287 GB, temp_buffer_size=1.307 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.884 GB, invar_size=1.323 GB, outvar_size=0.210 GB, temp_buffer_size=0.352 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.342, peak_memory=2.851 GB, invar_size=0.999 GB, outvar_size=0.332 GB, temp_buffer_size=1.601 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=1.460 GB, invar_size=0.832 GB, outvar_size=0.293 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.384 GB, invar_size=0.951 GB, outvar_size=0.287 GB, temp_buffer_size=1.307 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.019, peak_memory=1.409 GB, invar_size=0.898 GB, outvar_size=0.251 GB, temp_buffer_size=0.260 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.108, peak_memory=1.415 GB, invar_size=0.787 GB, outvar_size=0.293 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=2.962 GB, invar_size=1.777 GB, outvar_size=0.784 GB, temp_buffer_size=1.101 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=2.609 GB, invar_size=1.514 GB, outvar_size=0.694 GB, temp_buffer_size=1.011 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.103 GB, invar_size=1.592 GB, outvar_size=0.168 GB, temp_buffer_size=0.344 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=2.250 GB, invar_size=1.256 GB, outvar_size=0.545 GB, temp_buffer_size=0.826 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.154, peak_memory=1.984 GB, invar_size=1.155 GB, outvar_size=0.389 GB, temp_buffer_size=0.745 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.224, peak_memory=2.262 GB, invar_size=1.047 GB, outvar_size=0.356 GB, temp_buffer_size=1.048 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.052, peak_memory=2.345 GB, invar_size=1.508 GB, outvar_size=0.628 GB, temp_buffer_size=0.754 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.069, peak_memory=1.409 GB, invar_size=0.922 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.195, peak_memory=2.074 GB, invar_size=1.244 GB, outvar_size=0.434 GB, temp_buffer_size=0.746 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.050, peak_memory=2.082 GB, invar_size=1.245 GB, outvar_size=0.539 GB, temp_buffer_size=0.754 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.504 GB, invar_size=1.951 GB, outvar_size=0.210 GB, temp_buffer_size=0.344 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.069, peak_memory=1.409 GB, invar_size=0.922 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.775 GB, invar_size=2.274 GB, outvar_size=1.053 GB, temp_buffer_size=1.418 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.024, peak_memory=4.625 GB, invar_size=2.770 GB, outvar_size=1.322 GB, temp_buffer_size=1.771 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.088, peak_memory=1.631 GB, invar_size=1.101 GB, outvar_size=0.210 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.914 GB, invar_size=1.478 GB, outvar_size=0.168 GB, temp_buffer_size=0.268 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.069, peak_memory=1.214 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.244 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.069, peak_memory=1.214 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.244 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.030, peak_memory=5.313 GB, invar_size=3.267 GB, outvar_size=1.591 GB, temp_buffer_size=1.962 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.188, peak_memory=2.385 GB, invar_size=1.556 GB, outvar_size=0.568 GB, temp_buffer_size=0.746 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.088, peak_memory=1.631 GB, invar_size=1.101 GB, outvar_size=0.210 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.167, peak_memory=2.502 GB, invar_size=1.699 GB, outvar_size=0.703 GB, temp_buffer_size=0.719 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.112, peak_memory=2.529 GB, invar_size=1.789 GB, outvar_size=0.748 GB, temp_buffer_size=0.656 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.315 GB, invar_size=1.837 GB, outvar_size=0.210 GB, temp_buffer_size=0.268 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.903 GB, invar_size=1.478 GB, outvar_size=0.168 GB, temp_buffer_size=0.257 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.088, peak_memory=1.435 GB, invar_size=0.982 GB, outvar_size=0.210 GB, temp_buffer_size=0.244 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.088, peak_memory=1.435 GB, invar_size=0.982 GB, outvar_size=0.210 GB, temp_buffer_size=0.244 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.146, peak_memory=2.643 GB, invar_size=1.843 GB, outvar_size=0.838 GB, temp_buffer_size=0.717 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.069, peak_memory=1.161 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.191 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.043, peak_memory=3.195 GB, invar_size=2.004 GB, outvar_size=0.898 GB, temp_buffer_size=1.107 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.144, peak_memory=2.584 GB, invar_size=1.843 GB, outvar_size=0.838 GB, temp_buffer_size=0.657 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.069, peak_memory=1.161 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.191 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.317 GB, invar_size=3.201 GB, outvar_size=0.189 GB, temp_buffer_size=0.927 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=4.918 GB, invar_size=3.081 GB, outvar_size=1.478 GB, temp_buffer_size=1.795 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.130, peak_memory=2.248 GB, invar_size=1.687 GB, outvar_size=0.760 GB, temp_buffer_size=0.519 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.101, peak_memory=2.349 GB, invar_size=1.664 GB, outvar_size=0.189 GB, temp_buffer_size=0.497 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.130, peak_memory=2.248 GB, invar_size=1.687 GB, outvar_size=0.760 GB, temp_buffer_size=0.519 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.916 GB, invar_size=2.842 GB, outvar_size=0.147 GB, temp_buffer_size=0.927 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.082, peak_memory=2.128 GB, invar_size=1.485 GB, outvar_size=0.147 GB, temp_buffer_size=0.497 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.101, peak_memory=2.349 GB, invar_size=1.664 GB, outvar_size=0.189 GB, temp_buffer_size=0.497 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.082, peak_memory=2.128 GB, invar_size=1.485 GB, outvar_size=0.147 GB, temp_buffer_size=0.497 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.035, peak_memory=6.037 GB, invar_size=3.841 GB, outvar_size=1.837 GB, temp_buffer_size=2.154 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.037, peak_memory=6.431 GB, invar_size=4.026 GB, outvar_size=1.950 GB, temp_buffer_size=2.321 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=4.918 GB, invar_size=3.081 GB, outvar_size=1.478 GB, temp_buffer_size=1.795 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=5.373 GB, invar_size=4.278 GB, outvar_size=0.168 GB, temp_buffer_size=0.928 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=2.885 GB, invar_size=2.203 GB, outvar_size=0.168 GB, temp_buffer_size=0.515 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.163, peak_memory=2.668 GB, invar_size=2.088 GB, outvar_size=0.939 GB, temp_buffer_size=0.538 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.177, peak_memory=2.985 GB, invar_size=2.244 GB, outvar_size=1.017 GB, temp_buffer_size=0.657 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.163, peak_memory=2.668 GB, invar_size=2.088 GB, outvar_size=0.939 GB, temp_buffer_size=0.538 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.179, peak_memory=3.045 GB, invar_size=2.244 GB, outvar_size=1.017 GB, temp_buffer_size=0.717 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=2.885 GB, invar_size=2.203 GB, outvar_size=0.168 GB, temp_buffer_size=0.515 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.130, peak_memory=2.231 GB, invar_size=1.687 GB, outvar_size=0.760 GB, temp_buffer_size=0.501 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.054, peak_memory=10.233 GB, invar_size=5.788 GB, outvar_size=2.842 GB, temp_buffer_size=4.403 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.972 GB, invar_size=3.919 GB, outvar_size=0.126 GB, temp_buffer_size=0.927 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.130, peak_memory=2.231 GB, invar_size=1.687 GB, outvar_size=0.760 GB, temp_buffer_size=0.501 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.073, peak_memory=2.663 GB, invar_size=2.023 GB, outvar_size=0.126 GB, temp_buffer_size=0.515 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.073, peak_memory=2.663 GB, invar_size=2.023 GB, outvar_size=0.126 GB, temp_buffer_size=0.515 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=11.352 GB, invar_size=6.548 GB, outvar_size=3.201 GB, temp_buffer_size=4.762 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=4.161 GB, invar_size=3.051 GB, outvar_size=1.442 GB, temp_buffer_size=1.068 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=6.463 GB, invar_size=5.410 GB, outvar_size=0.126 GB, temp_buffer_size=0.927 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=4.161 GB, invar_size=3.051 GB, outvar_size=1.442 GB, temp_buffer_size=1.068 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=6.062 GB, invar_size=5.051 GB, outvar_size=0.084 GB, temp_buffer_size=0.927 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.082, peak_memory=3.409 GB, invar_size=2.768 GB, outvar_size=0.126 GB, temp_buffer_size=0.515 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.172, peak_memory=4.541 GB, invar_size=3.452 GB, outvar_size=1.622 GB, temp_buffer_size=1.047 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.172, peak_memory=4.541 GB, invar_size=3.452 GB, outvar_size=1.622 GB, temp_buffer_size=1.047 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.082, peak_memory=3.409 GB, invar_size=2.768 GB, outvar_size=0.126 GB, temp_buffer_size=0.515 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.081, peak_memory=14.657 GB, invar_size=8.680 GB, outvar_size=4.277 GB, temp_buffer_size=5.934 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=3.192 GB, invar_size=2.589 GB, outvar_size=0.126 GB, temp_buffer_size=0.478 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=3.192 GB, invar_size=2.589 GB, outvar_size=0.126 GB, temp_buffer_size=0.478 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.074, peak_memory=13.538 GB, invar_size=7.921 GB, outvar_size=3.919 GB, temp_buffer_size=5.575 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=5.255 GB, invar_size=4.086 GB, outvar_size=1.980 GB, temp_buffer_size=1.127 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.158, peak_memory=5.708 GB, invar_size=4.487 GB, outvar_size=2.160 GB, temp_buffer_size=1.179 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=5.255 GB, invar_size=4.086 GB, outvar_size=1.980 GB, temp_buffer_size=1.127 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.102, peak_memory=18.177 GB, invar_size=10.902 GB, outvar_size=5.409 GB, temp_buffer_size=7.233 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.158, peak_memory=5.708 GB, invar_size=4.487 GB, outvar_size=2.160 GB, temp_buffer_size=1.179 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.095, peak_memory=17.058 GB, invar_size=10.142 GB, outvar_size=5.050 GB, temp_buffer_size=6.874 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.142, peak_memory=6.846 GB, invar_size=5.577 GB, outvar_size=2.726 GB, temp_buffer_size=1.228 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.098, peak_memory=6.488 GB, invar_size=5.218 GB, outvar_size=2.546 GB, temp_buffer_size=1.228 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.098, peak_memory=6.488 GB, invar_size=5.218 GB, outvar_size=2.546 GB, temp_buffer_size=1.228 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.142, peak_memory=6.846 GB, invar_size=5.577 GB, outvar_size=2.726 GB, temp_buffer_size=1.228 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 40.78 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.598 GB, invar_size=0.958 GB, outvar_size=0.168 GB, temp_buffer_size=0.473 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.479 GB, invar_size=0.055 GB, outvar_size=0.586 GB, temp_buffer_size=0.837 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.479 GB, invar_size=0.055 GB, outvar_size=0.586 GB, temp_buffer_size=0.837 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.819 GB, invar_size=0.305 GB, outvar_size=0.670 GB, temp_buffer_size=0.843 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.329 GB, invar_size=0.688 GB, outvar_size=0.251 GB, temp_buffer_size=0.389 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.017 GB, invar_size=0.347 GB, outvar_size=0.335 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.329 GB, invar_size=0.688 GB, outvar_size=0.251 GB, temp_buffer_size=0.389 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.598 GB, invar_size=0.958 GB, outvar_size=0.168 GB, temp_buffer_size=0.473 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.082 GB, invar_size=1.914 GB, outvar_size=0.957 GB, temp_buffer_size=1.000 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.394 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.394 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.464 GB, invar_size=0.687 GB, outvar_size=0.046 GB, temp_buffer_size=2.777 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.464 GB, invar_size=0.687 GB, outvar_size=0.046 GB, temp_buffer_size=2.777 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.290 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.626 GB, invar_size=1.460 GB, outvar_size=0.688 GB, temp_buffer_size=0.999 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.005 GB, invar_size=0.862 GB, outvar_size=0.347 GB, temp_buffer_size=0.976 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.806 GB, invar_size=1.029 GB, outvar_size=0.305 GB, temp_buffer_size=2.526 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.626 GB, invar_size=1.460 GB, outvar_size=0.688 GB, temp_buffer_size=0.999 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.017 GB, invar_size=0.347 GB, outvar_size=0.335 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.137 GB, invar_size=0.347 GB, outvar_size=0.335 GB, temp_buffer_size=0.455 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.137 GB, invar_size=0.347 GB, outvar_size=0.335 GB, temp_buffer_size=0.455 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.290 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.564 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=0.793 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.454 GB, invar_size=0.533 GB, outvar_size=0.335 GB, temp_buffer_size=0.586 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.564 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=0.793 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.454 GB, invar_size=0.533 GB, outvar_size=0.335 GB, temp_buffer_size=0.586 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.082 GB, invar_size=1.914 GB, outvar_size=0.957 GB, temp_buffer_size=1.000 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.289 GB, invar_size=1.098 GB, outvar_size=0.465 GB, temp_buffer_size=1.856 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.290 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.290 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.290 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.290 GB, invar_size=0.802 GB, outvar_size=0.168 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.544 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=0.773 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.005 GB, invar_size=0.862 GB, outvar_size=0.347 GB, temp_buffer_size=0.976 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.819 GB, invar_size=0.305 GB, outvar_size=0.670 GB, temp_buffer_size=0.843 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.638 GB, invar_size=0.465 GB, outvar_size=0.503 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.638 GB, invar_size=0.465 GB, outvar_size=0.503 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.243 GB, invar_size=2.166 GB, outvar_size=0.126 GB, temp_buffer_size=0.951 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.005 GB, invar_size=0.862 GB, outvar_size=0.347 GB, temp_buffer_size=0.976 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.243 GB, invar_size=2.166 GB, outvar_size=0.126 GB, temp_buffer_size=0.951 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.005 GB, invar_size=0.862 GB, outvar_size=0.347 GB, temp_buffer_size=0.976 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.544 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=0.773 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.320 GB, invar_size=3.243 GB, outvar_size=0.084 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.824 GB, invar_size=1.065 GB, outvar_size=0.532 GB, temp_buffer_size=1.424 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.481 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=0.710 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.824 GB, invar_size=1.065 GB, outvar_size=0.532 GB, temp_buffer_size=1.424 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.481 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=0.710 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.481 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=0.710 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.320 GB, invar_size=3.243 GB, outvar_size=0.084 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.481 GB, invar_size=1.687 GB, outvar_size=0.802 GB, temp_buffer_size=0.710 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.945 GB, invar_size=2.969 GB, outvar_size=0.042 GB, temp_buffer_size=0.935 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.945 GB, invar_size=2.969 GB, outvar_size=0.042 GB, temp_buffer_size=0.935 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.384 GB, invar_size=4.373 GB, outvar_size=2.166 GB, temp_buffer_size=1.927 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.384 GB, invar_size=4.373 GB, outvar_size=2.166 GB, temp_buffer_size=1.927 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.289 GB, invar_size=1.098 GB, outvar_size=0.465 GB, temp_buffer_size=1.856 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.644 GB, invar_size=6.485 GB, outvar_size=3.242 GB, temp_buffer_size=2.075 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.806 GB, invar_size=1.029 GB, outvar_size=0.305 GB, temp_buffer_size=2.526 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.507 GB, invar_size=5.936 GB, outvar_size=2.968 GB, temp_buffer_size=1.529 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.507 GB, invar_size=5.936 GB, outvar_size=2.968 GB, temp_buffer_size=1.529 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.644 GB, invar_size=6.485 GB, outvar_size=3.242 GB, temp_buffer_size=2.075 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 17.45 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 2, 0) has been pruned...
[TMP] Stage (0, 6, 2, 1) has been pruned...
[TMP] Stage (0, 6, 2, 2) has been pruned...
[TMP] Stage (0, 7, 2, 0) has been pruned...
[TMP] Stage (0, 7, 2, 1) has been pruned...
[TMP] Stage (0, 7, 2, 2) has been pruned...
[TMP] Stage (0, 8, 2, 0) has been pruned...
[TMP] Stage (0, 8, 2, 1) has been pruned...
[TMP] Stage (0, 8, 2, 2) has been pruned...
[TMP] Stage (1, 6, 2, 0) has been pruned...
[TMP] Stage (1, 6, 2, 1) has been pruned...
[TMP] Stage (1, 6, 2, 2) has been pruned...
[TMP] Stage (1, 7, 2, 0) has been pruned...
[TMP] Stage (1, 7, 2, 1) has been pruned...
[TMP] Stage (1, 7, 2, 2) has been pruned...
[TMP] Stage (1, 8, 2, 0) has been pruned...
[TMP] Stage (1, 8, 2, 1) has been pruned...
[TMP] Stage (1, 8, 2, 2) has been pruned...
[TMP] Stage (1, 9, 2, 0) has been pruned...
[TMP] Stage (1, 9, 2, 1) has been pruned...
[TMP] Stage (1, 9, 2, 2) has been pruned...
[TMP] Stage (2, 7, 2, 0) has been pruned...
[TMP] Stage (2, 7, 2, 1) has been pruned...
[TMP] Stage (2, 7, 2, 2) has been pruned...
[TMP] Stage (2, 8, 2, 0) has been pruned...
[TMP] Stage (2, 8, 2, 1) has been pruned...
[TMP] Stage (2, 8, 2, 2) has been pruned...
[TMP] Stage (2, 9, 2, 0) has been pruned...
[TMP] Stage (2, 9, 2, 1) has been pruned...
[TMP] Stage (2, 9, 2, 2) has been pruned...
[TMP] Stage (2, 10, 2, 0) has been pruned...
[TMP] Stage (2, 10, 2, 1) has been pruned...
[TMP] Stage (2, 10, 2, 2) has been pruned...
[TMP] Stage (3, 9, 2, 0) has been pruned...
[TMP] Stage (3, 9, 2, 1) has been pruned...
[TMP] Stage (3, 9, 2, 2) has been pruned...
[TMP] Stage (3, 10, 2, 0) has been pruned...
[TMP] Stage (3, 10, 2, 1) has been pruned...
[TMP] Stage (3, 10, 2, 2) has been pruned...
[TMP] Stage (3, 11, 2, 0) has been pruned...
[TMP] Stage (3, 11, 2, 1) has been pruned...
[TMP] Stage (3, 11, 2, 2) has been pruned...
[TMP] Stage (4, 10, 2, 0) has been pruned...
[TMP] Stage (4, 10, 2, 1) has been pruned...
[TMP] Stage (4, 10, 2, 2) has been pruned...
[TMP] Stage (4, 11, 2, 0) has been pruned...
[TMP] Stage (4, 11, 2, 1) has been pruned...
[TMP] Stage (4, 11, 2, 2) has been pruned...
[TMP] Stage (4, 12, 2, 0) has been pruned...
[TMP] Stage (4, 12, 2, 1) has been pruned...
[TMP] Stage (4, 12, 2, 2) has been pruned...
[TMP] Stage (5, 11, 2, 0) has been pruned...
[TMP] Stage (5, 11, 2, 1) has been pruned...
[TMP] Stage (5, 11, 2, 2) has been pruned...
[TMP] Stage (5, 12, 2, 0) has been pruned...
[TMP] Stage (5, 12, 2, 1) has been pruned...
[TMP] Stage (5, 12, 2, 2) has been pruned...
[TMP] Stage (5, 13, 2, 0) has been pruned...
[TMP] Stage (5, 13, 2, 1) has been pruned...
[TMP] Stage (5, 13, 2, 2) has been pruned...
[TMP] Stage (5, 14, 2, 0) has been pruned...
[TMP] Stage (5, 14, 2, 1) has been pruned...
[TMP] Stage (5, 14, 2, 2) has been pruned...
[TMP] Stage (6, 12, 2, 0) has been pruned...
[TMP] Stage (6, 12, 2, 1) has been pruned...
[TMP] Stage (6, 12, 2, 2) has been pruned...
[TMP] Stage (6, 13, 2, 0) has been pruned...
[TMP] Stage (6, 13, 2, 1) has been pruned...
[TMP] Stage (6, 13, 2, 2) has been pruned...
[TMP] Stage (6, 14, 2, 0) has been pruned...
[TMP] Stage (6, 14, 2, 1) has been pruned...
[TMP] Stage (6, 14, 2, 2) has been pruned...
[TMP] Stage (6, 15, 2, 0) has been pruned...
[TMP] Stage (6, 15, 2, 1) has been pruned...
[TMP] Stage (6, 15, 2, 2) has been pruned...
[TMP] Stage (7, 13, 2, 0) has been pruned...
[TMP] Stage (7, 13, 2, 1) has been pruned...
[TMP] Stage (7, 13, 2, 2) has been pruned...
[TMP] Stage (7, 14, 2, 0) has been pruned...
[TMP] Stage (7, 14, 2, 1) has been pruned...
[TMP] Stage (7, 14, 2, 2) has been pruned...
[TMP] Stage (7, 15, 2, 0) has been pruned...
[TMP] Stage (7, 15, 2, 1) has been pruned...
[TMP] Stage (7, 15, 2, 2) has been pruned...
[TMP] Stage (8, 14, 2, 0) has been pruned...
[TMP] Stage (8, 14, 2, 1) has been pruned...
[TMP] Stage (8, 14, 2, 2) has been pruned...
[TMP] Stage (8, 15, 2, 0) has been pruned...
[TMP] Stage (8, 15, 2, 1) has been pruned...
[TMP] Stage (8, 15, 2, 2) has been pruned...
[TMP] Stage (9, 15, 2, 0) has been pruned...
[TMP] Stage (9, 15, 2, 1) has been pruned...
[TMP] Stage (9, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO comm 0x3706760 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO comm 0x4edcfb0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO comm 0x43c6f50 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO comm 0x7cd83f0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO comm 0x40d08c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO comm 0x991b450 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2563541)[0m 
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2563541)[0m 
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2563541)[0m 
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO comm 0x3945cd0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO comm 0x6be59e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2563540)[0m 
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2563541)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2563540)[0m 
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2563540)[0m 
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO comm 0x37210b0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2563541)[0m 
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO comm 0x69f2720 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2563540)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2563540)[0m 
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO comm 0x8e29ec0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO comm 0x4e486c0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1785538, ip=192.168.0.34)[0m gpu19:1785538:1785538 [0] NCCL INFO comm 0x398f1e0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO comm 0x795d7d0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 145.50 s
compilation time breakdown: {'stage-construction': '124.74', 'stage-construction-dp': '1.38', 'stage-construction-compilation': '58.48', 'stage-construction-profiling': '30.69'}
 - Compile (worker): 4.34 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1861869, ip=192.168.0.35)[0m gpu20:1861869:1861869 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1861870, ip=192.168.0.35)[0m gpu20:1861870:1861870 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3071663, ip=192.168.0.39)[0m gpu24:3071663:3071663 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3071662, ip=192.168.0.39)[0m gpu24:3071662:3071662 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2563541)[0m gpu16:2563541:2563541 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2563540)[0m gpu16:2563540:2563540 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1785539, ip=192.168.0.34)[0m gpu19:1785539:1785539 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 230.86 s

[65.61788272857666, 27.113394260406494, 27.1860249042511, 27.163394927978516, 26.90117597579956, 27.023051261901855, 27.027201652526855]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 137.049 s.
 - Average e2e iteration time: 27.410001754760742 s.
 - Total local training time: 135.30101013183594 s.
 - Average local iteration time: 27.060001373291016 s.
 - Max allocated memory among devices: 13.636 GB.
 - Compilation times:  {'stage-construction': 124.73872113227844, 'stage-construction-dp': 1.3817596435546875, 'stage-construction-compilation': 58.47652459144592, 'stage-construction-profiling': 30.69220280647278}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `4_a40_4_n_2_d`: 27.060169219970703
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_2B_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling wide_resnet_2B with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_2B_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 3, 0), 0] = ModuleProfileResult(compute_cost=0.074, peak_memory=9.065 GB, invar_size=7.323 GB, outvar_size=0.545 GB, temp_buffer_size=1.198 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 0] = ModuleProfileResult(compute_cost=0.200, peak_memory=5.042 GB, invar_size=3.668 GB, outvar_size=0.586 GB, temp_buffer_size=0.787 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 0] = ModuleProfileResult(compute_cost=8.796, peak_memory=4.638 GB, invar_size=3.255 GB, outvar_size=0.691 GB, temp_buffer_size=0.692 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 1] = ModuleProfileResult(compute_cost=11.621, peak_memory=24.605 GB, invar_size=15.187 GB, outvar_size=7.320 GB, temp_buffer_size=9.419 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 1] = ModuleProfileResult(compute_cost=5.419, peak_memory=12.683 GB, invar_size=7.919 GB, outvar_size=3.666 GB, temp_buffer_size=4.764 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 1] = ModuleProfileResult(compute_cost=13.985, peak_memory=10.460 GB, invar_size=7.198 GB, outvar_size=3.252 GB, temp_buffer_size=3.262 GB, available_memory=35.242 GB)
Profiling for submesh 3 (4, 2) takes 52.95 seconds
--------------------------------------------------
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.391 GB, invar_size=0.645 GB, outvar_size=0.837 GB, temp_buffer_size=0.909 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.895 GB, invar_size=0.555 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.391 GB, invar_size=0.645 GB, outvar_size=0.837 GB, temp_buffer_size=0.909 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.345, peak_memory=3.337 GB, invar_size=0.825 GB, outvar_size=1.172 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.074 GB, invar_size=0.185 GB, outvar_size=1.089 GB, temp_buffer_size=0.800 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.074 GB, invar_size=0.185 GB, outvar_size=1.089 GB, temp_buffer_size=0.800 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.895 GB, invar_size=0.555 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.669 GB, invar_size=0.622 GB, outvar_size=0.503 GB, temp_buffer_size=0.544 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.305, peak_memory=3.125 GB, invar_size=0.780 GB, outvar_size=1.005 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=3.934 GB, invar_size=1.612 GB, outvar_size=0.555 GB, temp_buffer_size=1.986 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.129, peak_memory=2.246 GB, invar_size=0.814 GB, outvar_size=0.503 GB, temp_buffer_size=0.930 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.562, peak_memory=5.343 GB, invar_size=2.152 GB, outvar_size=0.490 GB, temp_buffer_size=2.856 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=4.138 GB, invar_size=1.792 GB, outvar_size=0.645 GB, temp_buffer_size=2.011 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.398 GB, invar_size=0.503 GB, outvar_size=1.005 GB, temp_buffer_size=0.890 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.468, peak_memory=5.086 GB, invar_size=1.895 GB, outvar_size=0.445 GB, temp_buffer_size=2.856 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=4.138 GB, invar_size=1.792 GB, outvar_size=0.645 GB, temp_buffer_size=2.011 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.552, peak_memory=3.091 GB, invar_size=0.106 GB, outvar_size=1.591 GB, temp_buffer_size=1.394 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.669 GB, invar_size=0.622 GB, outvar_size=0.503 GB, temp_buffer_size=0.544 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=3.766 GB, invar_size=1.445 GB, outvar_size=0.555 GB, temp_buffer_size=1.986 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.753, peak_memory=5.483 GB, invar_size=1.785 GB, outvar_size=0.088 GB, temp_buffer_size=3.698 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.992 GB, invar_size=0.712 GB, outvar_size=0.670 GB, temp_buffer_size=0.610 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=3.208 GB, invar_size=1.412 GB, outvar_size=0.622 GB, temp_buffer_size=1.461 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.476, peak_memory=3.407 GB, invar_size=0.628 GB, outvar_size=1.340 GB, temp_buffer_size=1.439 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.050 GB, invar_size=0.413 GB, outvar_size=0.837 GB, temp_buffer_size=0.800 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.050 GB, invar_size=0.413 GB, outvar_size=0.837 GB, temp_buffer_size=0.800 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.398 GB, invar_size=0.503 GB, outvar_size=1.005 GB, temp_buffer_size=0.890 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.437, peak_memory=3.150 GB, invar_size=0.583 GB, outvar_size=1.172 GB, temp_buffer_size=1.394 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=4.307 GB, invar_size=1.450 GB, outvar_size=0.176 GB, temp_buffer_size=2.857 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=4.474 GB, invar_size=1.617 GB, outvar_size=0.176 GB, temp_buffer_size=2.857 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.992 GB, invar_size=0.712 GB, outvar_size=0.670 GB, temp_buffer_size=0.610 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.088 GB, invar_size=0.868 GB, outvar_size=0.586 GB, temp_buffer_size=0.634 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.775, peak_memory=5.789 GB, invar_size=2.094 GB, outvar_size=0.628 GB, temp_buffer_size=3.193 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.347, peak_memory=3.847 GB, invar_size=1.460 GB, outvar_size=0.479 GB, temp_buffer_size=2.052 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=3.208 GB, invar_size=1.412 GB, outvar_size=0.622 GB, temp_buffer_size=1.461 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.168, peak_memory=2.336 GB, invar_size=0.859 GB, outvar_size=0.670 GB, temp_buffer_size=0.807 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.088 GB, invar_size=0.868 GB, outvar_size=0.586 GB, temp_buffer_size=0.634 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.669 GB, invar_size=0.778 GB, outvar_size=0.419 GB, temp_buffer_size=0.473 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=4.837 GB, invar_size=1.926 GB, outvar_size=0.503 GB, temp_buffer_size=2.660 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.669 GB, invar_size=0.778 GB, outvar_size=0.419 GB, temp_buffer_size=0.473 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.248, peak_memory=2.064 GB, invar_size=0.640 GB, outvar_size=0.754 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.681, peak_memory=5.531 GB, invar_size=1.836 GB, outvar_size=0.583 GB, temp_buffer_size=3.192 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=4.269 GB, invar_size=1.412 GB, outvar_size=0.413 GB, temp_buffer_size=2.605 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.244, peak_memory=2.109 GB, invar_size=0.685 GB, outvar_size=0.754 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.064 GB, invar_size=1.137 GB, outvar_size=0.503 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=3.555 GB, invar_size=1.759 GB, outvar_size=0.712 GB, temp_buffer_size=1.461 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.064 GB, invar_size=1.137 GB, outvar_size=0.503 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=4.436 GB, invar_size=1.579 GB, outvar_size=0.413 GB, temp_buffer_size=2.605 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.301, peak_memory=3.324 GB, invar_size=1.699 GB, outvar_size=0.473 GB, temp_buffer_size=1.457 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.334 GB, invar_size=1.406 GB, outvar_size=0.419 GB, temp_buffer_size=0.509 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.441, peak_memory=4.105 GB, invar_size=1.717 GB, outvar_size=0.523 GB, temp_buffer_size=2.053 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.423 GB, invar_size=2.154 GB, outvar_size=0.868 GB, temp_buffer_size=1.101 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.334 GB, invar_size=1.406 GB, outvar_size=0.419 GB, temp_buffer_size=0.509 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=3.070 GB, invar_size=1.807 GB, outvar_size=0.778 GB, temp_buffer_size=1.095 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.498 GB, invar_size=1.676 GB, outvar_size=0.335 GB, temp_buffer_size=0.488 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.381, peak_memory=3.414 GB, invar_size=1.789 GB, outvar_size=0.518 GB, temp_buffer_size=1.457 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=3.555 GB, invar_size=1.759 GB, outvar_size=0.712 GB, temp_buffer_size=1.461 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=4.670 GB, invar_size=1.759 GB, outvar_size=0.503 GB, temp_buffer_size=2.660 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=3.507 GB, invar_size=2.238 GB, outvar_size=0.868 GB, temp_buffer_size=1.101 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.267, peak_memory=2.327 GB, invar_size=0.820 GB, outvar_size=0.838 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=3.153 GB, invar_size=1.891 GB, outvar_size=0.778 GB, temp_buffer_size=1.095 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.352, peak_memory=3.767 GB, invar_size=2.142 GB, outvar_size=0.652 GB, temp_buffer_size=1.458 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.176, peak_memory=2.211 GB, invar_size=0.955 GB, outvar_size=0.586 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.941 GB, invar_size=2.035 GB, outvar_size=0.419 GB, temp_buffer_size=0.488 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=3.490 GB, invar_size=2.160 GB, outvar_size=0.787 GB, temp_buffer_size=1.162 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.196 GB, invar_size=1.520 GB, outvar_size=0.335 GB, temp_buffer_size=0.341 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.084, peak_memory=1.957 GB, invar_size=1.089 GB, outvar_size=0.335 GB, temp_buffer_size=0.532 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=4.278 GB, invar_size=2.692 GB, outvar_size=1.137 GB, temp_buffer_size=1.418 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.024, peak_memory=4.948 GB, invar_size=3.063 GB, outvar_size=1.406 GB, temp_buffer_size=1.717 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=4.194 GB, invar_size=2.608 GB, outvar_size=1.137 GB, temp_buffer_size=1.418 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.084, peak_memory=1.957 GB, invar_size=1.089 GB, outvar_size=0.335 GB, temp_buffer_size=0.532 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.639 GB, invar_size=1.879 GB, outvar_size=0.419 GB, temp_buffer_size=0.341 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.084, peak_memory=1.580 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.024, peak_memory=4.948 GB, invar_size=3.063 GB, outvar_size=1.406 GB, temp_buffer_size=1.717 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.084, peak_memory=1.580 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.238, peak_memory=3.508 GB, invar_size=2.178 GB, outvar_size=0.921 GB, temp_buffer_size=1.162 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.030, peak_memory=5.648 GB, invar_size=3.518 GB, outvar_size=1.675 GB, temp_buffer_size=1.962 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.175 GB, invar_size=1.520 GB, outvar_size=0.335 GB, temp_buffer_size=0.320 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.230, peak_memory=3.442 GB, invar_size=2.178 GB, outvar_size=0.921 GB, temp_buffer_size=1.097 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.210, peak_memory=2.868 GB, invar_size=1.938 GB, outvar_size=0.802 GB, temp_buffer_size=0.845 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.210, peak_memory=2.868 GB, invar_size=1.938 GB, outvar_size=0.802 GB, temp_buffer_size=0.845 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.169 GB, invar_size=3.291 GB, outvar_size=1.520 GB, temp_buffer_size=1.795 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.084, peak_memory=1.496 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.275 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.289, peak_memory=3.945 GB, invar_size=2.620 GB, outvar_size=1.101 GB, temp_buffer_size=1.157 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.084, peak_memory=1.496 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.275 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.613 GB, invar_size=3.243 GB, outvar_size=0.377 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=2.220 GB, invar_size=1.269 GB, outvar_size=0.419 GB, temp_buffer_size=0.532 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.281, peak_memory=3.885 GB, invar_size=2.620 GB, outvar_size=1.101 GB, temp_buffer_size=1.097 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=1.843 GB, invar_size=1.065 GB, outvar_size=0.419 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.261, peak_memory=3.311 GB, invar_size=2.381 GB, outvar_size=0.981 GB, temp_buffer_size=0.846 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.171 GB, invar_size=2.884 GB, outvar_size=0.293 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.035, peak_memory=6.330 GB, invar_size=4.092 GB, outvar_size=1.879 GB, temp_buffer_size=2.154 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.037, peak_memory=6.809 GB, invar_size=4.320 GB, outvar_size=2.034 GB, temp_buffer_size=2.321 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=2.424 GB, invar_size=1.568 GB, outvar_size=0.293 GB, temp_buffer_size=0.563 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=2.220 GB, invar_size=1.269 GB, outvar_size=0.419 GB, temp_buffer_size=0.532 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=1.843 GB, invar_size=1.065 GB, outvar_size=0.419 GB, temp_buffer_size=0.359 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.169 GB, invar_size=3.291 GB, outvar_size=1.520 GB, temp_buffer_size=1.795 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.210, peak_memory=2.868 GB, invar_size=1.938 GB, outvar_size=0.802 GB, temp_buffer_size=0.845 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=2.424 GB, invar_size=1.568 GB, outvar_size=0.293 GB, temp_buffer_size=0.563 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.210, peak_memory=2.868 GB, invar_size=1.938 GB, outvar_size=0.802 GB, temp_buffer_size=0.845 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.261, peak_memory=3.311 GB, invar_size=2.381 GB, outvar_size=0.981 GB, temp_buffer_size=0.846 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.147, peak_memory=2.687 GB, invar_size=1.748 GB, outvar_size=0.377 GB, temp_buffer_size=0.563 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=5.648 GB, invar_size=4.320 GB, outvar_size=0.335 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.147, peak_memory=2.687 GB, invar_size=1.748 GB, outvar_size=0.377 GB, temp_buffer_size=0.563 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=5.206 GB, invar_size=3.961 GB, outvar_size=0.252 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.241, peak_memory=4.625 GB, invar_size=3.303 GB, outvar_size=1.484 GB, temp_buffer_size=1.239 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.054, peak_memory=10.463 GB, invar_size=5.977 GB, outvar_size=2.884 GB, temp_buffer_size=4.403 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.138, peak_memory=3.124 GB, invar_size=2.107 GB, outvar_size=0.335 GB, temp_buffer_size=0.682 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.138, peak_memory=3.124 GB, invar_size=2.107 GB, outvar_size=0.335 GB, temp_buffer_size=0.682 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=11.624 GB, invar_size=6.778 GB, outvar_size=3.243 GB, temp_buffer_size=4.762 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=5.068 GB, invar_size=3.745 GB, outvar_size=1.663 GB, temp_buffer_size=1.239 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=6.253 GB, invar_size=5.093 GB, outvar_size=0.168 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=6.696 GB, invar_size=5.452 GB, outvar_size=0.252 GB, temp_buffer_size=0.993 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=5.068 GB, invar_size=3.745 GB, outvar_size=1.663 GB, temp_buffer_size=1.239 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.241, peak_memory=4.625 GB, invar_size=3.303 GB, outvar_size=1.484 GB, temp_buffer_size=1.239 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.158, peak_memory=3.387 GB, invar_size=2.286 GB, outvar_size=0.419 GB, temp_buffer_size=0.682 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.158, peak_memory=3.387 GB, invar_size=2.286 GB, outvar_size=0.419 GB, temp_buffer_size=0.682 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.158, peak_memory=3.838 GB, invar_size=2.852 GB, outvar_size=0.335 GB, temp_buffer_size=0.651 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.119, peak_memory=3.570 GB, invar_size=2.672 GB, outvar_size=0.252 GB, temp_buffer_size=0.646 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.203, peak_memory=5.793 GB, invar_size=4.379 GB, outvar_size=2.022 GB, temp_buffer_size=1.330 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.158, peak_memory=3.838 GB, invar_size=2.852 GB, outvar_size=0.335 GB, temp_buffer_size=0.651 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.074, peak_memory=13.747 GB, invar_size=8.088 GB, outvar_size=3.960 GB, temp_buffer_size=5.576 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.254, peak_memory=6.236 GB, invar_size=4.822 GB, outvar_size=2.202 GB, temp_buffer_size=1.330 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.119, peak_memory=3.570 GB, invar_size=2.672 GB, outvar_size=0.252 GB, temp_buffer_size=0.646 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.081, peak_memory=14.908 GB, invar_size=8.890 GB, outvar_size=4.319 GB, temp_buffer_size=5.935 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.203, peak_memory=5.793 GB, invar_size=4.379 GB, outvar_size=2.022 GB, temp_buffer_size=1.330 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.254, peak_memory=6.236 GB, invar_size=4.822 GB, outvar_size=2.202 GB, temp_buffer_size=1.330 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.095, peak_memory=17.226 GB, invar_size=10.267 GB, outvar_size=5.092 GB, temp_buffer_size=6.874 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=7.285 GB, invar_size=5.870 GB, outvar_size=2.767 GB, temp_buffer_size=1.332 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.186, peak_memory=6.842 GB, invar_size=5.427 GB, outvar_size=2.588 GB, temp_buffer_size=1.332 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.186, peak_memory=6.842 GB, invar_size=5.427 GB, outvar_size=2.588 GB, temp_buffer_size=1.332 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=7.285 GB, invar_size=5.870 GB, outvar_size=2.767 GB, temp_buffer_size=1.332 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.102, peak_memory=18.386 GB, invar_size=11.069 GB, outvar_size=5.451 GB, temp_buffer_size=7.234 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 41.54 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(5, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.855 GB, invar_size=0.515 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.125 GB, outvar_size=0.335 GB, temp_buffer_size=0.730 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.088 GB, invar_size=0.856 GB, outvar_size=0.503 GB, temp_buffer_size=0.730 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.911 GB, invar_size=0.064 GB, outvar_size=1.172 GB, temp_buffer_size=1.675 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.911 GB, invar_size=0.064 GB, outvar_size=1.172 GB, temp_buffer_size=1.675 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.249 GB, invar_size=2.249 GB, outvar_size=1.125 GB, temp_buffer_size=1.664 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.088 GB, invar_size=0.856 GB, outvar_size=0.503 GB, temp_buffer_size=0.730 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.577 GB, invar_size=0.556 GB, outvar_size=1.340 GB, temp_buffer_size=1.681 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.776 GB, invar_size=1.879 GB, outvar_size=0.856 GB, temp_buffer_size=1.562 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.776 GB, invar_size=1.879 GB, outvar_size=0.856 GB, temp_buffer_size=1.562 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.823 GB, invar_size=1.282 GB, outvar_size=0.046 GB, temp_buffer_size=5.540 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.823 GB, invar_size=1.282 GB, outvar_size=0.046 GB, temp_buffer_size=5.540 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.491 GB, invar_size=1.950 GB, outvar_size=0.556 GB, temp_buffer_size=5.039 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.596 GB, invar_size=1.364 GB, outvar_size=0.514 GB, temp_buffer_size=1.897 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.046 GB, invar_size=0.515 GB, outvar_size=0.670 GB, temp_buffer_size=0.861 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.855 GB, invar_size=0.515 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.596 GB, invar_size=1.364 GB, outvar_size=0.514 GB, temp_buffer_size=1.897 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.046 GB, invar_size=0.515 GB, outvar_size=0.670 GB, temp_buffer_size=0.861 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.783 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.562 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.190 GB, invar_size=1.125 GB, outvar_size=0.335 GB, temp_buffer_size=0.730 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.662 GB, invar_size=0.868 GB, outvar_size=0.670 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.783 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.562 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.646 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.304 GB, invar_size=1.935 GB, outvar_size=0.800 GB, temp_buffer_size=3.699 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.596 GB, invar_size=1.364 GB, outvar_size=0.514 GB, temp_buffer_size=1.897 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.249 GB, invar_size=2.249 GB, outvar_size=1.125 GB, temp_buffer_size=1.664 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.646 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.646 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.596 GB, invar_size=1.364 GB, outvar_size=0.514 GB, temp_buffer_size=1.897 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.646 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.244 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.138 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.244 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.138 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.646 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.175 GB, invar_size=1.735 GB, outvar_size=0.867 GB, temp_buffer_size=2.770 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.662 GB, invar_size=0.868 GB, outvar_size=0.670 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.646 GB, invar_size=0.886 GB, outvar_size=0.335 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.244 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.138 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.145 GB, invar_size=0.800 GB, outvar_size=1.005 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.244 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=1.138 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.077 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=0.971 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.077 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=0.971 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.145 GB, invar_size=0.800 GB, outvar_size=1.005 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.077 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=0.971 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.626 GB, invar_size=2.250 GB, outvar_size=0.252 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.077 GB, invar_size=1.938 GB, outvar_size=0.885 GB, temp_buffer_size=0.971 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.175 GB, invar_size=1.735 GB, outvar_size=0.867 GB, temp_buffer_size=2.770 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.577 GB, invar_size=0.556 GB, outvar_size=1.340 GB, temp_buffer_size=1.681 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.626 GB, invar_size=2.250 GB, outvar_size=0.252 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.892 GB, invar_size=4.582 GB, outvar_size=2.249 GB, temp_buffer_size=2.142 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.892 GB, invar_size=4.582 GB, outvar_size=2.249 GB, temp_buffer_size=2.142 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.304 GB, invar_size=1.935 GB, outvar_size=0.800 GB, temp_buffer_size=3.699 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.102 GB, invar_size=3.011 GB, outvar_size=0.084 GB, temp_buffer_size=1.008 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.691 GB, invar_size=3.327 GB, outvar_size=0.168 GB, temp_buffer_size=1.196 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.491 GB, invar_size=1.950 GB, outvar_size=0.556 GB, temp_buffer_size=5.039 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.102 GB, invar_size=3.011 GB, outvar_size=0.084 GB, temp_buffer_size=1.008 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.691 GB, invar_size=3.327 GB, outvar_size=0.168 GB, temp_buffer_size=1.196 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.918 GB, invar_size=6.020 GB, outvar_size=3.010 GB, temp_buffer_size=1.815 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.918 GB, invar_size=6.020 GB, outvar_size=3.010 GB, temp_buffer_size=1.815 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.342 GB, invar_size=6.652 GB, outvar_size=3.326 GB, temp_buffer_size=2.522 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.342 GB, invar_size=6.652 GB, outvar_size=3.326 GB, temp_buffer_size=2.522 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 17.79 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 2, 0) has been pruned...
[TMP] Stage (0, 6, 2, 1) has been pruned...
[TMP] Stage (0, 6, 2, 2) has been pruned...
[TMP] Stage (0, 7, 2, 0) has been pruned...
[TMP] Stage (0, 7, 2, 1) has been pruned...
[TMP] Stage (0, 7, 2, 2) has been pruned...
[TMP] Stage (0, 8, 2, 0) has been pruned...
[TMP] Stage (0, 8, 2, 1) has been pruned...
[TMP] Stage (0, 8, 2, 2) has been pruned...
[TMP] Stage (1, 6, 2, 0) has been pruned...
[TMP] Stage (1, 6, 2, 1) has been pruned...
[TMP] Stage (1, 6, 2, 2) has been pruned...
[TMP] Stage (1, 7, 2, 0) has been pruned...
[TMP] Stage (1, 7, 2, 1) has been pruned...
[TMP] Stage (1, 7, 2, 2) has been pruned...
[TMP] Stage (1, 8, 2, 0) has been pruned...
[TMP] Stage (1, 8, 2, 1) has been pruned...
[TMP] Stage (1, 8, 2, 2) has been pruned...
[TMP] Stage (1, 9, 2, 0) has been pruned...
[TMP] Stage (1, 9, 2, 1) has been pruned...
[TMP] Stage (1, 9, 2, 2) has been pruned...
[TMP] Stage (2, 7, 2, 0) has been pruned...
[TMP] Stage (2, 7, 2, 1) has been pruned...
[TMP] Stage (2, 7, 2, 2) has been pruned...
[TMP] Stage (2, 8, 2, 0) has been pruned...
[TMP] Stage (2, 8, 2, 1) has been pruned...
[TMP] Stage (2, 8, 2, 2) has been pruned...
[TMP] Stage (2, 9, 2, 0) has been pruned...
[TMP] Stage (2, 9, 2, 1) has been pruned...
[TMP] Stage (2, 9, 2, 2) has been pruned...
[TMP] Stage (2, 10, 2, 0) has been pruned...
[TMP] Stage (2, 10, 2, 1) has been pruned...
[TMP] Stage (2, 10, 2, 2) has been pruned...
[TMP] Stage (3, 9, 2, 0) has been pruned...
[TMP] Stage (3, 9, 2, 1) has been pruned...
[TMP] Stage (3, 9, 2, 2) has been pruned...
[TMP] Stage (3, 10, 2, 0) has been pruned...
[TMP] Stage (3, 10, 2, 1) has been pruned...
[TMP] Stage (3, 10, 2, 2) has been pruned...
[TMP] Stage (3, 11, 2, 0) has been pruned...
[TMP] Stage (3, 11, 2, 1) has been pruned...
[TMP] Stage (3, 11, 2, 2) has been pruned...
[TMP] Stage (4, 10, 2, 0) has been pruned...
[TMP] Stage (4, 10, 2, 1) has been pruned...
[TMP] Stage (4, 10, 2, 2) has been pruned...
[TMP] Stage (4, 11, 2, 0) has been pruned...
[TMP] Stage (4, 11, 2, 1) has been pruned...
[TMP] Stage (4, 11, 2, 2) has been pruned...
[TMP] Stage (4, 12, 2, 0) has been pruned...
[TMP] Stage (4, 12, 2, 1) has been pruned...
[TMP] Stage (4, 12, 2, 2) has been pruned...
[TMP] Stage (5, 11, 2, 0) has been pruned...
[TMP] Stage (5, 11, 2, 1) has been pruned...
[TMP] Stage (5, 11, 2, 2) has been pruned...
[TMP] Stage (5, 12, 2, 0) has been pruned...
[TMP] Stage (5, 12, 2, 1) has been pruned...
[TMP] Stage (5, 12, 2, 2) has been pruned...
[TMP] Stage (5, 13, 2, 0) has been pruned...
[TMP] Stage (5, 13, 2, 1) has been pruned...
[TMP] Stage (5, 13, 2, 2) has been pruned...
[TMP] Stage (5, 14, 2, 0) has been pruned...
[TMP] Stage (5, 14, 2, 1) has been pruned...
[TMP] Stage (5, 14, 2, 2) has been pruned...
[TMP] Stage (6, 12, 2, 0) has been pruned...
[TMP] Stage (6, 12, 2, 1) has been pruned...
[TMP] Stage (6, 12, 2, 2) has been pruned...
[TMP] Stage (6, 13, 2, 0) has been pruned...
[TMP] Stage (6, 13, 2, 1) has been pruned...
[TMP] Stage (6, 13, 2, 2) has been pruned...
[TMP] Stage (6, 14, 2, 0) has been pruned...
[TMP] Stage (6, 14, 2, 1) has been pruned...
[TMP] Stage (6, 14, 2, 2) has been pruned...
[TMP] Stage (6, 15, 2, 0) has been pruned...
[TMP] Stage (6, 15, 2, 1) has been pruned...
[TMP] Stage (6, 15, 2, 2) has been pruned...
[TMP] Stage (7, 13, 2, 0) has been pruned...
[TMP] Stage (7, 13, 2, 1) has been pruned...
[TMP] Stage (7, 13, 2, 2) has been pruned...
[TMP] Stage (7, 14, 2, 0) has been pruned...
[TMP] Stage (7, 14, 2, 1) has been pruned...
[TMP] Stage (7, 14, 2, 2) has been pruned...
[TMP] Stage (7, 15, 2, 0) has been pruned...
[TMP] Stage (7, 15, 2, 1) has been pruned...
[TMP] Stage (7, 15, 2, 2) has been pruned...
[TMP] Stage (8, 14, 2, 0) has been pruned...
[TMP] Stage (8, 14, 2, 1) has been pruned...
[TMP] Stage (8, 14, 2, 2) has been pruned...
[TMP] Stage (8, 15, 2, 0) has been pruned...
[TMP] Stage (8, 15, 2, 1) has been pruned...
[TMP] Stage (8, 15, 2, 2) has been pruned...
[TMP] Stage (9, 15, 2, 0) has been pruned...
[TMP] Stage (9, 15, 2, 1) has been pruned...
[TMP] Stage (9, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2577126)[0m 
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2577126)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2577127)[0m 
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2577126)[0m 
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2577126)[0m 
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO comm 0x4c4f870 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2577127)[0m 
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2577127)[0m 
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO comm 0x4584ad0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2577127)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2577127)[0m 
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO comm 0x762dc60 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO comm 0x3fea850 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO comm 0x5fe0fc0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO comm 0x4004910 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO comm 0x9849050 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO comm 0x396cf90 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO comm 0x4e5e680 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO comm 0x8ec1950 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO comm 0xa3c9010 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO comm 0x504e670 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3079864, ip=192.168.0.39)[0m gpu24:3079864:3079864 [0] NCCL INFO comm 0x3fb6be0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO comm 0x7e3efe0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 136.17 s
compilation time breakdown: {'stage-construction': '114.61', 'stage-construction-dp': '1.41', 'stage-construction-compilation': '47.35', 'stage-construction-profiling': '30.60'}
 - Compile (worker): 4.40 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=2577126)[0m gpu16:2577126:2577126 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2577127)[0m gpu16:2577127:2577127 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1868600, ip=192.168.0.35)[0m gpu20:1868600:1868600 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1868599, ip=192.168.0.35)[0m gpu20:1868599:1868599 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1795688, ip=192.168.0.34)[0m gpu19:1795688:1795688 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1795689, ip=192.168.0.34)[0m gpu19:1795689:1795689 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3079865, ip=192.168.0.39)[0m gpu24:3079865:3079865 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 453.72 s

[95.612149477005, 58.869805097579956, 58.910398960113525, 58.97774147987366, 58.81490755081177, 58.87078070640564, 58.99787163734436]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 298.062 s.
 - Average e2e iteration time: 59.612003326416016 s.
 - Total local training time: 294.572021484375 s.
 - Average local iteration time: 58.91400146484375 s.
 - Max allocated memory among devices: 26.882 GB.
 - Compilation times:  {'stage-construction': 114.61135840415955, 'stage-construction-dp': 1.4054512977600098, 'stage-construction-compilation': 47.35320234298706, 'stage-construction-profiling': 30.600253582000732}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `4_a40_4_n_2_d`: 58.914344787597656
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_2B_512.pkl`...

------------------------------------------------------------------
- (3/3) Profiling wide_resnet_2B with batch size: 1024...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_2B_1024.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 3, 0), 0] = ModuleProfileResult(compute_cost=0.074, peak_memory=9.988 GB, invar_size=7.325 GB, outvar_size=1.089 GB, temp_buffer_size=1.574 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 0] = ModuleProfileResult(compute_cost=0.318, peak_memory=5.998 GB, invar_size=3.682 GB, outvar_size=1.173 GB, temp_buffer_size=1.143 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 0] = ModuleProfileResult(compute_cost=17.545, peak_memory=6.022 GB, invar_size=3.257 GB, outvar_size=1.382 GB, temp_buffer_size=1.383 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 1] = ModuleProfileResult(compute_cost=11.621, peak_memory=25.234 GB, invar_size=15.733 GB, outvar_size=7.320 GB, temp_buffer_size=9.501 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 1] = ModuleProfileResult(compute_cost=5.637, peak_memory=13.430 GB, invar_size=8.532 GB, outvar_size=3.678 GB, temp_buffer_size=4.898 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 1] = ModuleProfileResult(compute_cost=19.960, peak_memory=11.140 GB, invar_size=7.891 GB, outvar_size=3.252 GB, temp_buffer_size=3.249 GB, available_memory=35.242 GB)
Profiling for submesh 3 (4, 2) takes 52.94 seconds
--------------------------------------------------
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.234 GB, invar_size=0.980 GB, outvar_size=1.675 GB, temp_buffer_size=1.579 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.773 GB, invar_size=0.890 GB, outvar_size=1.340 GB, temp_buffer_size=1.543 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.234 GB, invar_size=0.980 GB, outvar_size=1.675 GB, temp_buffer_size=1.579 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.841 GB, invar_size=0.194 GB, outvar_size=2.177 GB, temp_buffer_size=1.470 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.841 GB, invar_size=0.194 GB, outvar_size=2.177 GB, temp_buffer_size=1.470 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=0.957 GB, outvar_size=1.005 GB, temp_buffer_size=0.879 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.688, peak_memory=6.519 GB, invar_size=1.495 GB, outvar_size=2.345 GB, temp_buffer_size=2.680 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=7.283 GB, invar_size=2.785 GB, outvar_size=0.890 GB, temp_buffer_size=3.828 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.773 GB, invar_size=0.890 GB, outvar_size=1.340 GB, temp_buffer_size=1.543 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.324 GB, invar_size=0.754 GB, outvar_size=2.010 GB, temp_buffer_size=1.560 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=1.118, peak_memory=10.368 GB, invar_size=3.994 GB, outvar_size=0.825 GB, temp_buffer_size=5.704 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=7.486 GB, invar_size=2.964 GB, outvar_size=0.980 GB, temp_buffer_size=3.852 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.609, peak_memory=6.139 GB, invar_size=1.450 GB, outvar_size=2.010 GB, temp_buffer_size=2.680 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.258, peak_memory=4.256 GB, invar_size=1.484 GB, outvar_size=1.005 GB, temp_buffer_size=1.768 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=7.486 GB, invar_size=2.964 GB, outvar_size=0.980 GB, temp_buffer_size=3.852 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=0.957 GB, outvar_size=1.005 GB, temp_buffer_size=0.879 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=1.101, peak_memory=6.040 GB, invar_size=0.124 GB, outvar_size=3.182 GB, temp_buffer_size=2.734 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=1.500, peak_memory=10.777 GB, invar_size=3.394 GB, outvar_size=0.088 GB, temp_buffer_size=7.383 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.809 GB, invar_size=0.664 GB, outvar_size=1.675 GB, temp_buffer_size=1.470 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.332 GB, invar_size=1.047 GB, outvar_size=1.340 GB, temp_buffer_size=0.945 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=5.725 GB, invar_size=2.249 GB, outvar_size=0.957 GB, temp_buffer_size=2.806 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.324 GB, invar_size=0.754 GB, outvar_size=2.010 GB, temp_buffer_size=1.560 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.809 GB, invar_size=0.664 GB, outvar_size=1.675 GB, temp_buffer_size=1.470 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=8.167 GB, invar_size=2.547 GB, outvar_size=0.176 GB, temp_buffer_size=5.620 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.931, peak_memory=9.943 GB, invar_size=3.570 GB, outvar_size=0.780 GB, temp_buffer_size=5.703 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=6.948 GB, invar_size=2.450 GB, outvar_size=0.890 GB, temp_buffer_size=3.828 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=8.502 GB, invar_size=2.882 GB, outvar_size=0.176 GB, temp_buffer_size=5.620 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.872, peak_memory=6.164 GB, invar_size=1.086 GB, outvar_size=2.345 GB, temp_buffer_size=2.734 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.332 GB, invar_size=1.047 GB, outvar_size=1.340 GB, temp_buffer_size=0.945 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.950, peak_memory=6.589 GB, invar_size=1.131 GB, outvar_size=2.680 GB, temp_buffer_size=2.778 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.332 GB, invar_size=1.035 GB, outvar_size=1.173 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.689, peak_memory=7.365 GB, invar_size=2.632 GB, outvar_size=0.814 GB, temp_buffer_size=4.063 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=1.544, peak_memory=11.316 GB, invar_size=3.936 GB, outvar_size=1.131 GB, temp_buffer_size=6.375 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=5.725 GB, invar_size=2.249 GB, outvar_size=0.957 GB, temp_buffer_size=2.806 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.332 GB, invar_size=1.035 GB, outvar_size=1.173 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=9.024 GB, invar_size=3.350 GB, outvar_size=0.754 GB, temp_buffer_size=5.171 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.336, peak_memory=4.346 GB, invar_size=1.529 GB, outvar_size=1.340 GB, temp_buffer_size=1.477 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.597 GB, invar_size=0.946 GB, outvar_size=0.838 GB, temp_buffer_size=0.814 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.597 GB, invar_size=0.946 GB, outvar_size=0.838 GB, temp_buffer_size=0.814 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=8.455 GB, invar_size=2.835 GB, outvar_size=0.664 GB, temp_buffer_size=5.117 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.979 GB, invar_size=1.305 GB, outvar_size=1.005 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.487, peak_memory=3.823 GB, invar_size=0.975 GB, outvar_size=1.507 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.482, peak_memory=3.868 GB, invar_size=1.020 GB, outvar_size=1.507 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=1.358, peak_memory=10.891 GB, invar_size=3.511 GB, outvar_size=1.086 GB, temp_buffer_size=6.375 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=8.120 GB, invar_size=2.501 GB, outvar_size=0.664 GB, temp_buffer_size=5.117 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=6.240 GB, invar_size=2.764 GB, outvar_size=1.047 GB, temp_buffer_size=2.806 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.979 GB, invar_size=1.305 GB, outvar_size=1.005 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.213 GB, invar_size=1.574 GB, outvar_size=0.838 GB, temp_buffer_size=0.802 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=8.689 GB, invar_size=3.015 GB, outvar_size=0.754 GB, temp_buffer_size=5.171 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=4.721 GB, invar_size=2.393 GB, outvar_size=0.945 GB, temp_buffer_size=1.993 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.213 GB, invar_size=1.574 GB, outvar_size=0.838 GB, temp_buffer_size=0.802 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.752, peak_memory=6.094 GB, invar_size=2.877 GB, outvar_size=0.685 GB, temp_buffer_size=2.882 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.012, peak_memory=4.893 GB, invar_size=2.560 GB, outvar_size=0.945 GB, temp_buffer_size=1.998 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=6.240 GB, invar_size=2.764 GB, outvar_size=1.047 GB, temp_buffer_size=2.806 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=5.235 GB, invar_size=2.907 GB, outvar_size=1.035 GB, temp_buffer_size=1.993 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.273 GB, invar_size=1.843 GB, outvar_size=0.670 GB, temp_buffer_size=0.760 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.526, peak_memory=4.170 GB, invar_size=1.155 GB, outvar_size=1.675 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.876, peak_memory=7.790 GB, invar_size=3.057 GB, outvar_size=0.858 GB, temp_buffer_size=4.063 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.594, peak_memory=6.004 GB, invar_size=2.788 GB, outvar_size=0.640 GB, temp_buffer_size=2.882 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.273 GB, invar_size=1.843 GB, outvar_size=0.670 GB, temp_buffer_size=0.760 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.014, peak_memory=5.403 GB, invar_size=3.075 GB, outvar_size=1.035 GB, temp_buffer_size=1.993 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.347, peak_memory=3.802 GB, invar_size=1.290 GB, outvar_size=1.173 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=5.630 GB, invar_size=3.278 GB, outvar_size=1.304 GB, temp_buffer_size=2.017 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.800 GB, invar_size=2.202 GB, outvar_size=0.838 GB, temp_buffer_size=0.760 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.740 GB, invar_size=1.604 GB, outvar_size=0.670 GB, temp_buffer_size=0.467 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.800 GB, invar_size=2.202 GB, outvar_size=0.838 GB, temp_buffer_size=0.760 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.691, peak_memory=6.531 GB, invar_size=3.314 GB, outvar_size=0.820 GB, temp_buffer_size=2.882 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.024, peak_memory=6.013 GB, invar_size=3.649 GB, outvar_size=1.574 GB, temp_buffer_size=2.028 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=5.798 GB, invar_size=3.446 GB, outvar_size=1.304 GB, temp_buffer_size=2.017 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.740 GB, invar_size=1.604 GB, outvar_size=0.670 GB, temp_buffer_size=0.467 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.168, peak_memory=2.944 GB, invar_size=1.424 GB, outvar_size=0.670 GB, temp_buffer_size=0.850 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.573, peak_memory=5.628 GB, invar_size=3.081 GB, outvar_size=0.954 GB, temp_buffer_size=2.212 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.024, peak_memory=6.013 GB, invar_size=3.649 GB, outvar_size=1.574 GB, temp_buffer_size=2.028 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.267 GB, invar_size=1.963 GB, outvar_size=0.838 GB, temp_buffer_size=0.467 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.168, peak_memory=2.393 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.030, peak_memory=6.485 GB, invar_size=4.020 GB, outvar_size=1.843 GB, temp_buffer_size=2.130 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.699 GB, invar_size=1.604 GB, outvar_size=0.670 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.267 GB, invar_size=1.963 GB, outvar_size=0.838 GB, temp_buffer_size=0.467 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.030, peak_memory=6.485 GB, invar_size=4.020 GB, outvar_size=1.843 GB, temp_buffer_size=2.130 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.455, peak_memory=5.364 GB, invar_size=2.848 GB, outvar_size=1.089 GB, temp_buffer_size=2.181 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.699 GB, invar_size=1.604 GB, outvar_size=0.670 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.672 GB, invar_size=3.709 GB, outvar_size=1.603 GB, temp_buffer_size=1.795 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.402, peak_memory=4.167 GB, invar_size=2.441 GB, outvar_size=0.885 GB, temp_buffer_size=1.559 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.168, peak_memory=2.058 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=0.335 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.553, peak_memory=5.890 GB, invar_size=3.374 GB, outvar_size=1.268 GB, temp_buffer_size=2.181 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.672 GB, invar_size=3.709 GB, outvar_size=1.603 GB, temp_buffer_size=1.795 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=5.206 GB, invar_size=3.327 GB, outvar_size=0.754 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.208, peak_memory=2.741 GB, invar_size=1.233 GB, outvar_size=0.838 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=4.679 GB, invar_size=2.968 GB, outvar_size=0.586 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.208, peak_memory=3.291 GB, invar_size=1.604 GB, outvar_size=0.838 GB, temp_buffer_size=0.850 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.037, peak_memory=7.730 GB, invar_size=4.906 GB, outvar_size=2.202 GB, temp_buffer_size=2.489 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.037, peak_memory=7.730 GB, invar_size=4.906 GB, outvar_size=2.202 GB, temp_buffer_size=2.489 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=4.153 GB, invar_size=2.705 GB, outvar_size=0.754 GB, temp_buffer_size=0.694 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.402, peak_memory=4.167 GB, invar_size=2.441 GB, outvar_size=0.885 GB, temp_buffer_size=1.559 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.500, peak_memory=4.694 GB, invar_size=2.967 GB, outvar_size=1.065 GB, temp_buffer_size=1.559 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.035, peak_memory=6.916 GB, invar_size=4.595 GB, outvar_size=1.962 GB, temp_buffer_size=2.154 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=3.626 GB, invar_size=2.346 GB, outvar_size=0.586 GB, temp_buffer_size=0.694 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.672 GB, invar_size=3.709 GB, outvar_size=1.603 GB, temp_buffer_size=1.795 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=6.199 GB, invar_size=4.404 GB, outvar_size=0.670 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.273, peak_memory=3.255 GB, invar_size=1.736 GB, outvar_size=0.754 GB, temp_buffer_size=0.766 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.672 GB, invar_size=3.709 GB, outvar_size=1.603 GB, temp_buffer_size=1.795 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.035, peak_memory=6.916 GB, invar_size=4.595 GB, outvar_size=1.962 GB, temp_buffer_size=2.154 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=5.672 GB, invar_size=4.045 GB, outvar_size=0.503 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.199, peak_memory=4.524 GB, invar_size=2.825 GB, outvar_size=0.838 GB, temp_buffer_size=0.862 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.313, peak_memory=3.602 GB, invar_size=1.915 GB, outvar_size=0.921 GB, temp_buffer_size=0.766 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.054, peak_memory=10.924 GB, invar_size=6.353 GB, outvar_size=2.967 GB, temp_buffer_size=4.403 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=12.168 GB, invar_size=7.239 GB, outvar_size=3.326 GB, temp_buffer_size=4.762 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.199, peak_memory=3.997 GB, invar_size=2.466 GB, outvar_size=0.670 GB, temp_buffer_size=0.862 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.504, peak_memory=6.294 GB, invar_size=4.415 GB, outvar_size=1.747 GB, temp_buffer_size=1.711 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.406, peak_memory=5.769 GB, invar_size=3.889 GB, outvar_size=1.567 GB, temp_buffer_size=1.713 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=6.636 GB, invar_size=5.176 GB, outvar_size=0.335 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=8.986 GB, invar_size=6.078 GB, outvar_size=2.704 GB, temp_buffer_size=2.740 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=7.163 GB, invar_size=5.535 GB, outvar_size=0.503 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.742 GB, invar_size=5.193 GB, outvar_size=2.345 GB, temp_buffer_size=2.381 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.311, peak_memory=4.225 GB, invar_size=2.454 GB, outvar_size=0.838 GB, temp_buffer_size=0.934 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.307, peak_memory=4.045 GB, invar_size=2.274 GB, outvar_size=0.838 GB, temp_buffer_size=0.933 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.237, peak_memory=4.815 GB, invar_size=3.211 GB, outvar_size=0.670 GB, temp_buffer_size=0.934 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.223, peak_memory=4.204 GB, invar_size=2.840 GB, outvar_size=0.503 GB, temp_buffer_size=0.862 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.209, peak_memory=4.384 GB, invar_size=3.019 GB, outvar_size=0.503 GB, temp_buffer_size=0.862 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.074, peak_memory=14.262 GB, invar_size=8.423 GB, outvar_size=4.044 GB, temp_buffer_size=5.671 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.388, peak_memory=7.004 GB, invar_size=5.049 GB, outvar_size=2.106 GB, temp_buffer_size=1.787 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.473, peak_memory=7.363 GB, invar_size=5.408 GB, outvar_size=2.285 GB, temp_buffer_size=1.787 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.081, peak_memory=15.506 GB, invar_size=9.309 GB, outvar_size=4.403 GB, temp_buffer_size=6.030 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.195, peak_memory=8.032 GB, invar_size=6.318 GB, outvar_size=2.824 GB, temp_buffer_size=1.547 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.095, peak_memory=17.656 GB, invar_size=10.519 GB, outvar_size=5.176 GB, temp_buffer_size=6.970 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.187, peak_memory=7.147 GB, invar_size=5.432 GB, outvar_size=2.465 GB, temp_buffer_size=1.547 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.345, peak_memory=7.803 GB, invar_size=5.846 GB, outvar_size=2.672 GB, temp_buffer_size=1.790 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.345, peak_memory=4.719 GB, invar_size=3.020 GB, outvar_size=0.838 GB, temp_buffer_size=0.862 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.102, peak_memory=18.901 GB, invar_size=11.404 GB, outvar_size=5.535 GB, temp_buffer_size=7.329 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.455, peak_memory=8.446 GB, invar_size=6.540 GB, outvar_size=2.851 GB, temp_buffer_size=1.738 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.247, peak_memory=7.922 GB, invar_size=6.205 GB, outvar_size=2.851 GB, temp_buffer_size=1.550 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.255, peak_memory=8.640 GB, invar_size=6.923 GB, outvar_size=3.210 GB, temp_buffer_size=1.550 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 40.98 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.231 GB, invar_size=1.460 GB, outvar_size=0.670 GB, temp_buffer_size=1.101 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.529 GB, invar_size=0.850 GB, outvar_size=1.340 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.606 GB, invar_size=2.919 GB, outvar_size=1.460 GB, temp_buffer_size=3.017 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.536 GB, invar_size=1.191 GB, outvar_size=1.005 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.536 GB, invar_size=1.191 GB, outvar_size=1.005 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.456 GB, invar_size=2.716 GB, outvar_size=1.191 GB, temp_buffer_size=3.070 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.776 GB, invar_size=0.082 GB, outvar_size=2.345 GB, temp_buffer_size=3.350 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.776 GB, invar_size=0.082 GB, outvar_size=2.345 GB, temp_buffer_size=3.350 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.779 GB, invar_size=2.369 GB, outvar_size=0.849 GB, temp_buffer_size=3.740 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.456 GB, invar_size=2.716 GB, outvar_size=1.191 GB, temp_buffer_size=3.070 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.094 GB, invar_size=1.059 GB, outvar_size=2.680 GB, temp_buffer_size=3.356 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=13.540 GB, invar_size=2.472 GB, outvar_size=0.046 GB, temp_buffer_size=11.067 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=13.540 GB, invar_size=2.472 GB, outvar_size=0.046 GB, temp_buffer_size=11.067 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=14.861 GB, invar_size=3.792 GB, outvar_size=1.059 GB, temp_buffer_size=10.064 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.231 GB, invar_size=1.460 GB, outvar_size=0.670 GB, temp_buffer_size=1.101 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.824 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=1.101 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.824 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=1.101 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.888 GB, invar_size=0.850 GB, outvar_size=1.340 GB, temp_buffer_size=1.699 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.888 GB, invar_size=0.850 GB, outvar_size=1.340 GB, temp_buffer_size=1.699 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.529 GB, invar_size=0.850 GB, outvar_size=1.340 GB, temp_buffer_size=1.340 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.393 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.606 GB, invar_size=2.919 GB, outvar_size=1.460 GB, temp_buffer_size=3.017 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.393 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.837 GB, invar_size=2.441 GB, outvar_size=1.053 GB, temp_buffer_size=2.061 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.007 GB, invar_size=1.538 GB, outvar_size=1.340 GB, temp_buffer_size=2.129 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.393 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.393 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.837 GB, invar_size=2.441 GB, outvar_size=1.053 GB, temp_buffer_size=2.061 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.393 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.837 GB, invar_size=2.441 GB, outvar_size=1.053 GB, temp_buffer_size=2.061 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.779 GB, invar_size=2.369 GB, outvar_size=0.849 GB, temp_buffer_size=3.740 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.779 GB, invar_size=2.369 GB, outvar_size=0.849 GB, temp_buffer_size=3.740 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.779 GB, invar_size=2.369 GB, outvar_size=0.849 GB, temp_buffer_size=3.740 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.837 GB, invar_size=2.441 GB, outvar_size=1.053 GB, temp_buffer_size=2.061 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.393 GB, invar_size=1.053 GB, outvar_size=0.670 GB, temp_buffer_size=0.670 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.877 GB, invar_size=3.075 GB, outvar_size=1.537 GB, temp_buffer_size=5.462 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.502 GB, invar_size=2.441 GB, outvar_size=1.053 GB, temp_buffer_size=1.726 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.502 GB, invar_size=2.441 GB, outvar_size=1.053 GB, temp_buffer_size=1.726 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.502 GB, invar_size=2.441 GB, outvar_size=1.053 GB, temp_buffer_size=1.726 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.007 GB, invar_size=1.538 GB, outvar_size=1.340 GB, temp_buffer_size=2.129 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.160 GB, invar_size=1.470 GB, outvar_size=2.010 GB, temp_buffer_size=2.680 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.309 GB, invar_size=2.418 GB, outvar_size=0.503 GB, temp_buffer_size=1.389 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.502 GB, invar_size=2.441 GB, outvar_size=1.053 GB, temp_buffer_size=1.726 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.309 GB, invar_size=2.418 GB, outvar_size=0.503 GB, temp_buffer_size=1.389 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.877 GB, invar_size=3.075 GB, outvar_size=1.537 GB, temp_buffer_size=5.462 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.160 GB, invar_size=5.001 GB, outvar_size=2.417 GB, temp_buffer_size=2.824 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.160 GB, invar_size=5.001 GB, outvar_size=2.417 GB, temp_buffer_size=2.824 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=12.333 GB, invar_size=3.610 GB, outvar_size=1.470 GB, temp_buffer_size=7.383 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.160 GB, invar_size=1.470 GB, outvar_size=2.010 GB, temp_buffer_size=2.680 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.361 GB, invar_size=3.495 GB, outvar_size=0.335 GB, temp_buffer_size=1.531 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.417 GB, invar_size=3.094 GB, outvar_size=0.168 GB, temp_buffer_size=1.155 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=10.317 GB, invar_size=6.987 GB, outvar_size=3.494 GB, temp_buffer_size=2.995 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.094 GB, invar_size=1.059 GB, outvar_size=2.680 GB, temp_buffer_size=3.356 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.361 GB, invar_size=3.495 GB, outvar_size=0.335 GB, temp_buffer_size=1.531 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.417 GB, invar_size=3.094 GB, outvar_size=0.168 GB, temp_buffer_size=1.155 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=10.317 GB, invar_size=6.987 GB, outvar_size=3.494 GB, temp_buffer_size=2.995 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=12.333 GB, invar_size=3.610 GB, outvar_size=1.470 GB, temp_buffer_size=7.383 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.359 GB, invar_size=6.187 GB, outvar_size=3.094 GB, temp_buffer_size=2.004 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.359 GB, invar_size=6.187 GB, outvar_size=3.094 GB, temp_buffer_size=2.004 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=14.861 GB, invar_size=3.792 GB, outvar_size=1.059 GB, temp_buffer_size=10.064 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 17.41 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 2, 0) has been pruned...
[TMP] Stage (0, 6, 2, 1) has been pruned...
[TMP] Stage (0, 6, 2, 2) has been pruned...
[TMP] Stage (0, 7, 2, 0) has been pruned...
[TMP] Stage (0, 7, 2, 1) has been pruned...
[TMP] Stage (0, 7, 2, 2) has been pruned...
[TMP] Stage (0, 8, 2, 0) has been pruned...
[TMP] Stage (0, 8, 2, 1) has been pruned...
[TMP] Stage (0, 8, 2, 2) has been pruned...
[TMP] Stage (1, 6, 2, 0) has been pruned...
[TMP] Stage (1, 6, 2, 1) has been pruned...
[TMP] Stage (1, 6, 2, 2) has been pruned...
[TMP] Stage (1, 7, 2, 0) has been pruned...
[TMP] Stage (1, 7, 2, 1) has been pruned...
[TMP] Stage (1, 7, 2, 2) has been pruned...
[TMP] Stage (1, 8, 2, 0) has been pruned...
[TMP] Stage (1, 8, 2, 1) has been pruned...
[TMP] Stage (1, 8, 2, 2) has been pruned...
[TMP] Stage (1, 9, 2, 0) has been pruned...
[TMP] Stage (1, 9, 2, 1) has been pruned...
[TMP] Stage (1, 9, 2, 2) has been pruned...
[TMP] Stage (2, 7, 2, 0) has been pruned...
[TMP] Stage (2, 7, 2, 1) has been pruned...
[TMP] Stage (2, 7, 2, 2) has been pruned...
[TMP] Stage (2, 8, 2, 0) has been pruned...
[TMP] Stage (2, 8, 2, 1) has been pruned...
[TMP] Stage (2, 8, 2, 2) has been pruned...
[TMP] Stage (2, 9, 2, 0) has been pruned...
[TMP] Stage (2, 9, 2, 1) has been pruned...
[TMP] Stage (2, 9, 2, 2) has been pruned...
[TMP] Stage (2, 10, 2, 0) has been pruned...
[TMP] Stage (2, 10, 2, 1) has been pruned...
[TMP] Stage (2, 10, 2, 2) has been pruned...
[TMP] Stage (3, 9, 2, 0) has been pruned...
[TMP] Stage (3, 9, 2, 1) has been pruned...
[TMP] Stage (3, 9, 2, 2) has been pruned...
[TMP] Stage (3, 10, 2, 0) has been pruned...
[TMP] Stage (3, 10, 2, 1) has been pruned...
[TMP] Stage (3, 10, 2, 2) has been pruned...
[TMP] Stage (3, 11, 2, 0) has been pruned...
[TMP] Stage (3, 11, 2, 1) has been pruned...
[TMP] Stage (3, 11, 2, 2) has been pruned...
[TMP] Stage (4, 10, 2, 0) has been pruned...
[TMP] Stage (4, 10, 2, 1) has been pruned...
[TMP] Stage (4, 10, 2, 2) has been pruned...
[TMP] Stage (4, 11, 2, 0) has been pruned...
[TMP] Stage (4, 11, 2, 1) has been pruned...
[TMP] Stage (4, 11, 2, 2) has been pruned...
[TMP] Stage (4, 12, 2, 0) has been pruned...
[TMP] Stage (4, 12, 2, 1) has been pruned...
[TMP] Stage (4, 12, 2, 2) has been pruned...
[TMP] Stage (5, 11, 2, 0) has been pruned...
[TMP] Stage (5, 11, 2, 1) has been pruned...
[TMP] Stage (5, 11, 2, 2) has been pruned...
[TMP] Stage (5, 12, 2, 0) has been pruned...
[TMP] Stage (5, 12, 2, 1) has been pruned...
[TMP] Stage (5, 12, 2, 2) has been pruned...
[TMP] Stage (5, 13, 2, 0) has been pruned...
[TMP] Stage (5, 13, 2, 1) has been pruned...
[TMP] Stage (5, 13, 2, 2) has been pruned...
[TMP] Stage (5, 14, 2, 0) has been pruned...
[TMP] Stage (5, 14, 2, 1) has been pruned...
[TMP] Stage (5, 14, 2, 2) has been pruned...
[TMP] Stage (6, 12, 2, 0) has been pruned...
[TMP] Stage (6, 12, 2, 1) has been pruned...
[TMP] Stage (6, 12, 2, 2) has been pruned...
[TMP] Stage (6, 13, 2, 0) has been pruned...
[TMP] Stage (6, 13, 2, 1) has been pruned...
[TMP] Stage (6, 13, 2, 2) has been pruned...
[TMP] Stage (6, 14, 2, 0) has been pruned...
[TMP] Stage (6, 14, 2, 1) has been pruned...
[TMP] Stage (6, 14, 2, 2) has been pruned...
[TMP] Stage (6, 15, 2, 0) has been pruned...
[TMP] Stage (6, 15, 2, 1) has been pruned...
[TMP] Stage (6, 15, 2, 2) has been pruned...
[TMP] Stage (7, 13, 2, 0) has been pruned...
[TMP] Stage (7, 13, 2, 1) has been pruned...
[TMP] Stage (7, 13, 2, 2) has been pruned...
[TMP] Stage (7, 14, 2, 0) has been pruned...
[TMP] Stage (7, 14, 2, 1) has been pruned...
[TMP] Stage (7, 14, 2, 2) has been pruned...
[TMP] Stage (7, 15, 2, 0) has been pruned...
[TMP] Stage (7, 15, 2, 1) has been pruned...
[TMP] Stage (7, 15, 2, 2) has been pruned...
[TMP] Stage (8, 14, 2, 0) has been pruned...
[TMP] Stage (8, 14, 2, 1) has been pruned...
[TMP] Stage (8, 14, 2, 2) has been pruned...
[TMP] Stage (8, 15, 2, 0) has been pruned...
[TMP] Stage (8, 15, 2, 1) has been pruned...
[TMP] Stage (8, 15, 2, 2) has been pruned...
[TMP] Stage (9, 15, 2, 0) has been pruned...
[TMP] Stage (9, 15, 2, 1) has been pruned...
[TMP] Stage (9, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO comm 0x39620c0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1806008, ip=192.168.0.34)[0m gpu19:1806008:1806008 [0] NCCL INFO comm 0x3f93410 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO comm 0x4209270 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1806009, ip=192.168.0.34)[0m gpu19:1806009:1806009 [0] NCCL INFO comm 0x6327120 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1876315, ip=192.168.0.35)[0m gpu20:1876315:1876315 [0] NCCL INFO comm 0x975dce0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO comm 0x478d740 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2591263)[0m 
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2591263)[0m 
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2591263)[0m 
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO comm 0x4232d90 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1876316, ip=192.168.0.35)[0m gpu20:1876316:1876316 [0] NCCL INFO comm 0x9cf8bd0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2591263)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2591262)[0m 
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2591263)[0m 
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2591263)[0m gpu16:2591263:2591263 [0] NCCL INFO comm 0x9a62ac0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2591262)[0m 
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2591262)[0m 
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO comm 0x3869390 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2591262)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2591262)[0m 
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO comm 0x5036550 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2591262)[0m gpu16:2591262:2591262 [0] NCCL INFO comm 0x6cc9a20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3088150, ip=192.168.0.39)[0m gpu24:3088150:3088150 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3088150, ip=192.168.0.39)[0m gpu24:3088150:3088150 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3088150, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3088150, ip=192.168.0.39)[0m gpu24:3088150:3088150 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3088150, ip=192.168.0.39)[0m gpu24:3088150:3088150 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3088150, ip=192.168.0.39)[0m gpu24:3088150:3088150 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3088151, ip=192.168.0.39)[0m gpu24:3088151:3088151 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
