
------------------------------------------------------------------
- (1/3) Profiling wide_resnet_4B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_4B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 640
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.342 GB, invar_size=0.505 GB, outvar_size=0.359 GB, temp_buffer_size=0.479 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.985 GB, invar_size=1.182 GB, outvar_size=0.180 GB, temp_buffer_size=0.623 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.378 GB, invar_size=0.290 GB, outvar_size=0.479 GB, temp_buffer_size=0.610 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.115 GB, invar_size=0.098 GB, outvar_size=0.419 GB, temp_buffer_size=0.598 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.378 GB, invar_size=0.290 GB, outvar_size=0.479 GB, temp_buffer_size=0.610 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.289 GB, invar_size=0.642 GB, outvar_size=0.239 GB, temp_buffer_size=0.408 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.021 GB, invar_size=0.423 GB, outvar_size=0.239 GB, temp_buffer_size=0.359 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.522 GB, invar_size=0.680 GB, outvar_size=0.239 GB, temp_buffer_size=0.602 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.191, peak_memory=2.167 GB, invar_size=0.611 GB, outvar_size=0.598 GB, temp_buffer_size=0.957 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.342 GB, invar_size=0.505 GB, outvar_size=0.359 GB, temp_buffer_size=0.479 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.688 GB, invar_size=1.045 GB, outvar_size=0.120 GB, temp_buffer_size=0.523 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.475 GB, invar_size=1.732 GB, outvar_size=0.120 GB, temp_buffer_size=0.623 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.095, peak_memory=1.640 GB, invar_size=0.862 GB, outvar_size=0.299 GB, temp_buffer_size=0.479 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.219 GB, invar_size=1.525 GB, outvar_size=0.120 GB, temp_buffer_size=0.574 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.115 GB, invar_size=0.098 GB, outvar_size=0.419 GB, temp_buffer_size=0.598 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.140, peak_memory=1.312 GB, invar_size=0.056 GB, outvar_size=0.538 GB, temp_buffer_size=0.718 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.112, peak_memory=1.616 GB, invar_size=0.414 GB, outvar_size=0.479 GB, temp_buffer_size=0.724 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.103, peak_memory=1.548 GB, invar_size=0.771 GB, outvar_size=0.299 GB, temp_buffer_size=0.479 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=2.898 GB, invar_size=0.878 GB, outvar_size=0.289 GB, temp_buffer_size=1.840 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.688 GB, invar_size=1.045 GB, outvar_size=0.120 GB, temp_buffer_size=0.523 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.386 GB, invar_size=0.853 GB, outvar_size=0.120 GB, temp_buffer_size=0.414 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.386 GB, invar_size=0.853 GB, outvar_size=0.120 GB, temp_buffer_size=0.414 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=2.053 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.841 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=2.052 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.841 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=2.053 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.841 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.144 GB, invar_size=1.525 GB, outvar_size=0.120 GB, temp_buffer_size=0.499 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=0.374, peak_memory=3.599 GB, invar_size=0.947 GB, outvar_size=0.414 GB, temp_buffer_size=2.293 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=2.796 GB, invar_size=1.129 GB, outvar_size=0.505 GB, temp_buffer_size=1.427 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.147, peak_memory=2.049 GB, invar_size=0.845 GB, outvar_size=0.303 GB, temp_buffer_size=1.085 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.371 GB, invar_size=0.853 GB, outvar_size=0.120 GB, temp_buffer_size=0.399 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=2.916 GB, invar_size=1.249 GB, outvar_size=0.505 GB, temp_buffer_size=1.427 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=2.717 GB, invar_size=1.284 GB, outvar_size=0.642 GB, temp_buffer_size=1.194 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.133 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.408 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.367 GB, invar_size=3.110 GB, outvar_size=1.525 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.021, peak_memory=4.289 GB, invar_size=2.424 GB, outvar_size=1.182 GB, temp_buffer_size=1.746 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=2.898 GB, invar_size=0.878 GB, outvar_size=0.289 GB, temp_buffer_size=1.840 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.112 GB, invar_size=0.423 GB, outvar_size=0.239 GB, temp_buffer_size=0.450 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.144 GB, invar_size=1.525 GB, outvar_size=0.120 GB, temp_buffer_size=0.499 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.116, peak_memory=3.215 GB, invar_size=1.971 GB, outvar_size=0.926 GB, temp_buffer_size=1.125 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.270, peak_memory=3.632 GB, invar_size=1.342 GB, outvar_size=0.372 GB, temp_buffer_size=2.051 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.371 GB, invar_size=0.853 GB, outvar_size=0.120 GB, temp_buffer_size=0.399 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.031, peak_memory=5.975 GB, invar_size=3.462 GB, outvar_size=1.731 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.371 GB, invar_size=0.853 GB, outvar_size=0.120 GB, temp_buffer_size=0.399 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.133 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.408 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.072, peak_memory=2.845 GB, invar_size=1.784 GB, outvar_size=0.742 GB, temp_buffer_size=0.942 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.144 GB, invar_size=1.525 GB, outvar_size=0.120 GB, temp_buffer_size=0.499 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.371 GB, invar_size=0.853 GB, outvar_size=0.120 GB, temp_buffer_size=0.399 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.289 GB, invar_size=0.642 GB, outvar_size=0.239 GB, temp_buffer_size=0.408 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.116, peak_memory=3.215 GB, invar_size=1.971 GB, outvar_size=0.926 GB, temp_buffer_size=1.125 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.367 GB, invar_size=3.110 GB, outvar_size=1.525 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.633 GB, invar_size=1.704 GB, outvar_size=0.792 GB, temp_buffer_size=0.868 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.371 GB, invar_size=0.853 GB, outvar_size=0.120 GB, temp_buffer_size=0.399 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=1.371 GB, invar_size=0.853 GB, outvar_size=0.120 GB, temp_buffer_size=0.399 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.633 GB, invar_size=1.704 GB, outvar_size=0.792 GB, temp_buffer_size=0.868 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.146, peak_memory=2.785 GB, invar_size=1.601 GB, outvar_size=0.651 GB, temp_buffer_size=1.064 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.612 GB, invar_size=0.610 GB, outvar_size=0.094 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.251 GB, invar_size=4.309 GB, outvar_size=0.090 GB, temp_buffer_size=1.852 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=3.299 GB, invar_size=0.641 GB, outvar_size=0.047 GB, temp_buffer_size=2.658 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.458 GB, invar_size=0.849 GB, outvar_size=0.094 GB, temp_buffer_size=2.608 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.640 GB, invar_size=1.704 GB, outvar_size=0.792 GB, temp_buffer_size=0.876 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=8.418 GB, invar_size=6.507 GB, outvar_size=0.060 GB, temp_buffer_size=1.852 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.308 GB, invar_size=2.245 GB, outvar_size=0.090 GB, temp_buffer_size=0.973 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.308 GB, invar_size=2.245 GB, outvar_size=0.090 GB, temp_buffer_size=0.973 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.367 GB, invar_size=3.110 GB, outvar_size=1.525 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=2.052 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.841 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=4.436 GB, invar_size=3.343 GB, outvar_size=0.060 GB, temp_buffer_size=1.033 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=4.436 GB, invar_size=3.343 GB, outvar_size=0.060 GB, temp_buffer_size=1.033 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.520 GB, invar_size=1.704 GB, outvar_size=0.792 GB, temp_buffer_size=0.756 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.809 GB, invar_size=5.968 GB, outvar_size=0.030 GB, temp_buffer_size=1.810 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.640 GB, invar_size=1.704 GB, outvar_size=0.792 GB, temp_buffer_size=0.876 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.027, peak_memory=3.976 GB, invar_size=3.000 GB, outvar_size=0.030 GB, temp_buffer_size=0.946 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.520 GB, invar_size=1.704 GB, outvar_size=0.792 GB, temp_buffer_size=0.756 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.367 GB, invar_size=3.110 GB, outvar_size=1.525 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.027, peak_memory=3.976 GB, invar_size=3.000 GB, outvar_size=0.030 GB, temp_buffer_size=0.946 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.520 GB, invar_size=1.704 GB, outvar_size=0.792 GB, temp_buffer_size=0.756 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.520 GB, invar_size=1.704 GB, outvar_size=0.792 GB, temp_buffer_size=0.756 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.080, peak_memory=16.226 GB, invar_size=8.647 GB, outvar_size=4.308 GB, temp_buffer_size=7.520 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.147, peak_memory=2.049 GB, invar_size=0.845 GB, outvar_size=0.303 GB, temp_buffer_size=1.085 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.184, peak_memory=2.881 GB, invar_size=1.121 GB, outvar_size=0.441 GB, temp_buffer_size=1.521 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=2.837 GB, invar_size=1.404 GB, outvar_size=0.642 GB, temp_buffer_size=1.194 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=22.447 GB, invar_size=13.011 GB, outvar_size=6.506 GB, temp_buffer_size=9.376 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.112, peak_memory=20.834 GB, invar_size=11.936 GB, outvar_size=5.968 GB, temp_buffer_size=8.868 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.103, peak_memory=6.512 GB, invar_size=4.488 GB, outvar_size=2.184 GB, temp_buffer_size=1.964 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.103, peak_memory=6.512 GB, invar_size=4.488 GB, outvar_size=2.184 GB, temp_buffer_size=1.964 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.048, peak_memory=7.569 GB, invar_size=5.998 GB, outvar_size=2.999 GB, temp_buffer_size=1.541 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.048, peak_memory=7.569 GB, invar_size=5.998 GB, outvar_size=2.999 GB, temp_buffer_size=1.541 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.077, peak_memory=8.853 GB, invar_size=6.626 GB, outvar_size=3.283 GB, temp_buffer_size=2.167 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.077, peak_memory=8.853 GB, invar_size=6.626 GB, outvar_size=3.283 GB, temp_buffer_size=2.167 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 16.81 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.199 GB, invar_size=0.524 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.260 GB, invar_size=0.423 GB, outvar_size=0.239 GB, temp_buffer_size=0.598 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.260 GB, invar_size=0.423 GB, outvar_size=0.239 GB, temp_buffer_size=0.598 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.199 GB, invar_size=0.524 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.260 GB, invar_size=0.423 GB, outvar_size=0.239 GB, temp_buffer_size=0.598 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.260 GB, invar_size=0.423 GB, outvar_size=0.239 GB, temp_buffer_size=0.598 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.187 GB, invar_size=1.049 GB, outvar_size=0.524 GB, temp_buffer_size=2.660 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.997 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.173 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.187 GB, invar_size=1.049 GB, outvar_size=0.524 GB, temp_buffer_size=2.660 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.510 GB, invar_size=0.845 GB, outvar_size=0.422 GB, temp_buffer_size=1.426 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.271 GB, invar_size=0.845 GB, outvar_size=0.422 GB, temp_buffer_size=1.187 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.510 GB, invar_size=0.845 GB, outvar_size=0.422 GB, temp_buffer_size=1.426 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.510 GB, invar_size=0.845 GB, outvar_size=0.422 GB, temp_buffer_size=1.426 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.997 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.173 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.997 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.173 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.878 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.054 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.997 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.173 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.878 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.054 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.878 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.054 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.561 GB, invar_size=0.852 GB, outvar_size=0.120 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.883 GB, invar_size=2.990 GB, outvar_size=0.060 GB, temp_buffer_size=1.833 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.883 GB, invar_size=2.990 GB, outvar_size=0.060 GB, temp_buffer_size=1.833 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.878 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.054 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.878 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.054 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.260 GB, invar_size=0.423 GB, outvar_size=0.239 GB, temp_buffer_size=0.598 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.878 GB, invar_size=1.704 GB, outvar_size=0.852 GB, temp_buffer_size=1.054 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.931 GB, invar_size=3.068 GB, outvar_size=0.000 GB, temp_buffer_size=1.862 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.931 GB, invar_size=3.068 GB, outvar_size=0.000 GB, temp_buffer_size=1.862 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.848 GB, invar_size=5.980 GB, outvar_size=2.990 GB, temp_buffer_size=2.808 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.260 GB, invar_size=0.423 GB, outvar_size=0.239 GB, temp_buffer_size=0.598 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.848 GB, invar_size=5.980 GB, outvar_size=2.990 GB, temp_buffer_size=2.808 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.825 GB, invar_size=6.076 GB, outvar_size=3.068 GB, temp_buffer_size=2.689 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.271 GB, invar_size=0.845 GB, outvar_size=0.422 GB, temp_buffer_size=1.187 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.825 GB, invar_size=6.076 GB, outvar_size=3.068 GB, temp_buffer_size=2.689 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.510 GB, invar_size=0.845 GB, outvar_size=0.422 GB, temp_buffer_size=1.426 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.70 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 7, 3, 0) has been pruned...
[TMP] Stage (2, 7, 3, 1) has been pruned...
[TMP] Stage (2, 7, 3, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (3, 5, 2, 0) has been pruned...
[TMP] Stage (3, 5, 2, 1) has been pruned...
[TMP] Stage (3, 5, 2, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 8, 3, 0) has been pruned...
[TMP] Stage (3, 8, 3, 1) has been pruned...
[TMP] Stage (3, 8, 3, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (5, 7, 2, 0) has been pruned...
[TMP] Stage (5, 7, 2, 1) has been pruned...
[TMP] Stage (5, 7, 2, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4], [5], [6, 7], [8], [9], [10], [11], [12, 13], [14], [15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO comm 0x4a34340 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO comm 0x4a5f490 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO comm 0x6ab7da0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO comm 0x6afa2b0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=685688)[0m 
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=685688)[0m 
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=685688)[0m 
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO comm 0xab5b3f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO comm 0x356ebf0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=685688)[0m 
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO comm 0xafd2820 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=685687)[0m 
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO comm 0x6616850 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=685688)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=685687)[0m 
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=685687)[0m 
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO comm 0x393bd50 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=685687)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=685688)[0m 
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO comm 0x63c7930 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m 
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO comm 0x921f1c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO comm 0x4457800 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO comm 0x649eb00 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=685687)[0m 
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO comm 0x5729ca0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO comm 0xa22beb0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO comm 0x4cf11d0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO comm 0xa489260 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO comm 0xa245ad0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO comm 0x8045e20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO comm 0x50646e0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO comm 0x7e60ad0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO comm 0x4bc3810 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO comm 0x4e915a0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO comm 0x79b8f80 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO comm 0x53a5910 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO comm 0x79ad560 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO comm 0x75aaec0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO comm 0x7166bd0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO comm 0xb3f8320 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO comm 0x429c090 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO comm 0xb38f1e0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO comm 0x97f0e10 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1488843, ip=192.168.0.32)[0m gpu17:1488843:1488843 [0] NCCL INFO comm 0x40db960 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO comm 0x96439f0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 53.23 s
compilation time breakdown: {'stage-construction': '29.85', 'stage-construction-dp': '1.50', 'stage-construction-compilation': '6.22', 'stage-construction-profiling': '11.39'}
 - Compile (worker): 3.81 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO comm 0x7f49012ea250 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303250 [1] NCCL INFO comm 0x7f48e8e98cb0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303248 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3303186, ip=192.168.0.31)[0m gpu16:3303186:3303186 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363512 [1] NCCL INFO comm 0x7f66aa3edf70 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO comm 0x7f66b8030210 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363510 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2363364, ip=192.168.0.27)[0m gpu12:2363364:2363364 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=685688)[0m gpu2:685688:685688 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=685687)[0m gpu2:685687:685687 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO comm 0x7ef6d589fe40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168352 [1] NCCL INFO comm 0x7ef6b05ee280 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168350 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2168162, ip=192.168.0.18)[0m gpu3:2168162:2168162 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3658815, ip=192.168.0.39)[0m gpu24:3658815:3658815 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3658816, ip=192.168.0.39)[0m gpu24:3658816:3658816 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=985862, ip=192.168.0.26)[0m gpu11:985862:985862 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=985861, ip=192.168.0.26)[0m gpu11:985861:985861 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539036 [1] NCCL INFO comm 0x7fc435b03a70 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO comm 0x7fc451526a40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2539034 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2538972, ip=192.168.0.38)[0m gpu23:2538972:2538972 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1488844, ip=192.168.0.32)[0m gpu17:1488844:1488844 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 269.19 s

[54.85645031929016, 35.699695110321045, 35.11375069618225, 34.78358554840088, 34.71726703643799, 35.421128034591675, 34.80295515060425]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 177.164 s.
 - Average e2e iteration time: 35.43300247192383 s.
 - Total local training time: 174.83900451660156 s.
 - Average local iteration time: 34.96800231933594 s.
 - Max allocated memory among devices: 20.267 GB.
 - Compilation times:  {'stage-construction': 29.853845596313477, 'stage-construction-dp': 1.501868486404419, 'stage-construction-compilation': 6.2153003215789795, 'stage-construction-profiling': 11.38848328590393}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 34.96773910522461
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_4B_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling wide_resnet_4B with batch size: 512...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal/wide_resnet_4B_512.pkl`, creating it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 640
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.898 GB, invar_size=1.851 GB, outvar_size=0.239 GB, temp_buffer_size=0.807 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.756 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.671 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=3.073 GB, invar_size=1.159 GB, outvar_size=0.479 GB, temp_buffer_size=1.436 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=2.351 GB, invar_size=1.285 GB, outvar_size=0.239 GB, temp_buffer_size=0.827 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.468 GB, invar_size=1.302 GB, outvar_size=0.359 GB, temp_buffer_size=0.807 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.468 GB, invar_size=1.302 GB, outvar_size=0.359 GB, temp_buffer_size=0.807 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=1.858 GB, invar_size=0.662 GB, outvar_size=0.479 GB, temp_buffer_size=0.718 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=2.565 GB, invar_size=1.010 GB, outvar_size=0.598 GB, temp_buffer_size=0.957 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.225, peak_memory=3.172 GB, invar_size=0.773 GB, outvar_size=0.957 GB, temp_buffer_size=1.442 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.635 GB, invar_size=0.469 GB, outvar_size=0.957 GB, temp_buffer_size=1.209 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.635 GB, invar_size=0.469 GB, outvar_size=0.957 GB, temp_buffer_size=1.209 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.198 GB, invar_size=0.882 GB, outvar_size=0.479 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=1.950 GB, invar_size=0.662 GB, outvar_size=0.479 GB, temp_buffer_size=0.809 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.419 GB, invar_size=0.744 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.136 GB, invar_size=0.103 GB, outvar_size=0.837 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.136 GB, invar_size=0.103 GB, outvar_size=0.837 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.277, peak_memory=2.577 GB, invar_size=0.065 GB, outvar_size=1.077 GB, temp_buffer_size=1.436 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.363, peak_memory=5.274 GB, invar_size=1.839 GB, outvar_size=0.680 GB, temp_buffer_size=2.957 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=4.397 GB, invar_size=1.763 GB, outvar_size=0.882 GB, temp_buffer_size=2.155 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=2.351 GB, invar_size=1.285 GB, outvar_size=0.239 GB, temp_buffer_size=0.827 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.290, peak_memory=3.664 GB, invar_size=1.323 GB, outvar_size=0.422 GB, temp_buffer_size=2.102 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.756 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.671 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.850 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.639 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=4.636 GB, invar_size=2.002 GB, outvar_size=0.882 GB, temp_buffer_size=2.155 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=3.190 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.533 GB, invar_size=1.585 GB, outvar_size=0.239 GB, temp_buffer_size=0.709 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=3.190 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=3.190 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.850 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.639 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=3.190 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.413 GB, invar_size=1.585 GB, outvar_size=0.239 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.700 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.489 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.279, peak_memory=4.025 GB, invar_size=2.139 GB, outvar_size=0.770 GB, temp_buffer_size=1.647 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.031, peak_memory=6.285 GB, invar_size=3.702 GB, outvar_size=1.851 GB, temp_buffer_size=2.344 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=0.746, peak_memory=7.069 GB, invar_size=1.785 GB, outvar_size=0.773 GB, temp_buffer_size=4.566 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.700 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.489 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.021, peak_memory=4.887 GB, invar_size=2.723 GB, outvar_size=1.302 GB, temp_buffer_size=1.926 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.021, peak_memory=4.916 GB, invar_size=2.842 GB, outvar_size=1.302 GB, temp_buffer_size=1.835 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.700 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.489 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.700 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.489 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.413 GB, invar_size=1.585 GB, outvar_size=0.239 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.198 GB, invar_size=0.882 GB, outvar_size=0.479 GB, temp_buffer_size=0.837 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.413 GB, invar_size=1.585 GB, outvar_size=0.239 GB, temp_buffer_size=0.589 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.200, peak_memory=4.256 GB, invar_size=2.329 GB, outvar_size=1.045 GB, temp_buffer_size=1.687 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.191, peak_memory=4.134 GB, invar_size=2.329 GB, outvar_size=1.045 GB, temp_buffer_size=1.565 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=6.478 GB, invar_size=1.188 GB, outvar_size=0.047 GB, temp_buffer_size=5.290 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.157, peak_memory=3.350 GB, invar_size=1.944 GB, outvar_size=0.852 GB, temp_buffer_size=1.287 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.290, peak_memory=3.664 GB, invar_size=1.323 GB, outvar_size=0.422 GB, temp_buffer_size=2.102 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=5.530 GB, invar_size=1.536 GB, outvar_size=0.469 GB, temp_buffer_size=3.635 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.419 GB, invar_size=0.744 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.157, peak_memory=3.350 GB, invar_size=1.944 GB, outvar_size=0.852 GB, temp_buffer_size=1.287 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.606 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.380, peak_memory=4.200 GB, invar_size=1.090 GB, outvar_size=1.196 GB, temp_buffer_size=1.914 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.700 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.489 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=1.700 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.489 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=5.530 GB, invar_size=1.536 GB, outvar_size=0.469 GB, temp_buffer_size=3.635 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=6.701 GB, invar_size=1.512 GB, outvar_size=0.094 GB, temp_buffer_size=5.189 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.494 GB, invar_size=4.369 GB, outvar_size=0.180 GB, temp_buffer_size=1.946 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.606 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.157, peak_memory=3.350 GB, invar_size=1.944 GB, outvar_size=0.852 GB, temp_buffer_size=1.287 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.126, peak_memory=3.611 GB, invar_size=2.364 GB, outvar_size=0.180 GB, temp_buffer_size=1.067 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.157, peak_memory=3.350 GB, invar_size=1.944 GB, outvar_size=0.852 GB, temp_buffer_size=1.287 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.126, peak_memory=3.611 GB, invar_size=2.364 GB, outvar_size=0.180 GB, temp_buffer_size=1.067 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=5.009 GB, invar_size=1.033 GB, outvar_size=0.094 GB, temp_buffer_size=3.975 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.157, peak_memory=3.197 GB, invar_size=1.944 GB, outvar_size=0.852 GB, temp_buffer_size=1.134 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.606 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.157, peak_memory=3.197 GB, invar_size=1.944 GB, outvar_size=0.852 GB, temp_buffer_size=1.134 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=8.632 GB, invar_size=6.566 GB, outvar_size=0.120 GB, temp_buffer_size=1.946 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=4.821 GB, invar_size=3.463 GB, outvar_size=0.120 GB, temp_buffer_size=1.238 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=4.821 GB, invar_size=3.463 GB, outvar_size=0.120 GB, temp_buffer_size=1.238 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.606 GB, invar_size=3.289 GB, outvar_size=1.585 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.054, peak_memory=4.103 GB, invar_size=3.029 GB, outvar_size=0.060 GB, temp_buffer_size=1.014 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.157, peak_memory=3.197 GB, invar_size=1.944 GB, outvar_size=0.852 GB, temp_buffer_size=1.134 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.157, peak_memory=3.197 GB, invar_size=1.944 GB, outvar_size=0.852 GB, temp_buffer_size=1.134 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.921 GB, invar_size=5.998 GB, outvar_size=0.060 GB, temp_buffer_size=1.863 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.054, peak_memory=4.103 GB, invar_size=3.029 GB, outvar_size=0.060 GB, temp_buffer_size=1.014 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.200, peak_memory=7.115 GB, invar_size=4.727 GB, outvar_size=2.244 GB, temp_buffer_size=2.268 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.200, peak_memory=7.115 GB, invar_size=4.727 GB, outvar_size=2.244 GB, temp_buffer_size=2.268 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.080, peak_memory=16.436 GB, invar_size=8.796 GB, outvar_size=4.368 GB, temp_buffer_size=7.520 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.144, peak_memory=9.599 GB, invar_size=6.805 GB, outvar_size=3.343 GB, temp_buffer_size=2.674 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=22.626 GB, invar_size=13.131 GB, outvar_size=6.566 GB, temp_buffer_size=9.376 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.144, peak_memory=9.599 GB, invar_size=6.805 GB, outvar_size=3.343 GB, temp_buffer_size=2.674 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.535, peak_memory=6.982 GB, invar_size=2.419 GB, outvar_size=0.611 GB, temp_buffer_size=4.084 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.096, peak_memory=8.020 GB, invar_size=6.058 GB, outvar_size=3.029 GB, temp_buffer_size=1.902 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.112, peak_memory=20.924 GB, invar_size=11.996 GB, outvar_size=5.998 GB, temp_buffer_size=8.868 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=5.009 GB, invar_size=1.728 GB, outvar_size=0.744 GB, temp_buffer_size=2.803 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.096, peak_memory=8.020 GB, invar_size=6.058 GB, outvar_size=3.029 GB, temp_buffer_size=1.902 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=5.249 GB, invar_size=1.967 GB, outvar_size=0.744 GB, temp_buffer_size=2.803 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 17.46 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.353 GB, invar_size=1.003 GB, outvar_size=0.957 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.254 GB, invar_size=2.006 GB, outvar_size=1.003 GB, temp_buffer_size=5.292 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.337 GB, invar_size=0.662 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.337 GB, invar_size=0.662 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.337 GB, invar_size=0.662 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.353 GB, invar_size=1.003 GB, outvar_size=0.957 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.337 GB, invar_size=0.662 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.544 GB, invar_size=1.323 GB, outvar_size=0.662 GB, temp_buffer_size=2.742 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.939 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.756 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.544 GB, invar_size=1.323 GB, outvar_size=0.662 GB, temp_buffer_size=2.742 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.544 GB, invar_size=1.323 GB, outvar_size=0.662 GB, temp_buffer_size=2.742 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.254 GB, invar_size=2.006 GB, outvar_size=1.003 GB, temp_buffer_size=5.292 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.066 GB, invar_size=1.323 GB, outvar_size=0.662 GB, temp_buffer_size=2.264 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.940 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.757 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.940 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.757 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.939 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.756 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.700 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.518 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.700 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.518 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.950 GB, invar_size=0.972 GB, outvar_size=0.239 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.700 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.518 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.077 GB, invar_size=3.050 GB, outvar_size=0.120 GB, temp_buffer_size=1.907 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.077 GB, invar_size=3.050 GB, outvar_size=0.120 GB, temp_buffer_size=1.907 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.700 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.518 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.700 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.518 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.095 GB, invar_size=3.128 GB, outvar_size=0.000 GB, temp_buffer_size=1.967 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.700 GB, invar_size=1.944 GB, outvar_size=0.972 GB, temp_buffer_size=1.518 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.296 GB, invar_size=6.099 GB, outvar_size=3.050 GB, temp_buffer_size=3.078 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.296 GB, invar_size=6.099 GB, outvar_size=3.050 GB, temp_buffer_size=3.078 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.095 GB, invar_size=3.128 GB, outvar_size=0.000 GB, temp_buffer_size=1.967 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.337 GB, invar_size=0.662 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.094 GB, invar_size=6.136 GB, outvar_size=3.128 GB, temp_buffer_size=2.838 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.337 GB, invar_size=0.662 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.094 GB, invar_size=6.136 GB, outvar_size=3.128 GB, temp_buffer_size=2.838 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.066 GB, invar_size=1.323 GB, outvar_size=0.662 GB, temp_buffer_size=2.264 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.544 GB, invar_size=1.323 GB, outvar_size=0.662 GB, temp_buffer_size=2.742 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.90 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 7, 3, 0) has been pruned...
[TMP] Stage (2, 7, 3, 1) has been pruned...
[TMP] Stage (2, 7, 3, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (3, 5, 2, 0) has been pruned...
[TMP] Stage (3, 5, 2, 1) has been pruned...
[TMP] Stage (3, 5, 2, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 8, 3, 0) has been pruned...
[TMP] Stage (3, 8, 3, 1) has been pruned...
[TMP] Stage (3, 8, 3, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (5, 7, 2, 0) has been pruned...
[TMP] Stage (5, 7, 2, 1) has been pruned...
[TMP] Stage (5, 7, 2, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4], [5], [6, 7], [8], [9], [10], [11], [12, 13], [14], [15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=695812)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO comm 0x3e97a50 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO comm 0x4cc5ac0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO comm 0x7e06f60 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO comm 0x61eee70 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO comm 0xb7cdc10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO comm 0x3ee68d0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO comm 0x95f7830 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO comm 0xb5cc3e0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO comm 0x4227570 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO comm 0x6996300 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO comm 0x6bed480 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO comm 0x4f76ff0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO comm 0x6cc0390 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO comm 0x72ae5e0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO comm 0xc88d3b0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO comm 0x37da5b0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO comm 0x90094d0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO comm 0xc68ba20 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO comm 0x59e0d70 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO comm 0x47e0060 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO comm 0x49fd0f0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO comm 0x72f4dd0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO comm 0x75126c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO comm 0x427ac10 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO comm 0x70722c0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO comm 0x4e05d30 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO comm 0x6e54730 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO comm 0x6c5bc60 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO comm 0xabea150 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO comm 0x4b28090 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO comm 0xae40f20 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO comm 0x763cbe0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2173814, ip=192.168.0.18)[0m gpu3:2173814:2173814 [0] NCCL INFO comm 0x454b1c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO comm 0x6e1e320 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 54.36 s
compilation time breakdown: {'stage-construction': '30.68', 'stage-construction-dp': '1.47', 'stage-construction-compilation': '6.41', 'stage-construction-profiling': '11.57'}
 - Compile (worker): 4.43 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=695812)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=695812)[0m 
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695962 [1] NCCL INFO comm 0x7f110e81ea60 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO comm 0x7f1118af2730 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695960 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=695812)[0m gpu2:695812:695812 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494808 [1] NCCL INFO comm 0x7fbe46eb4770 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO comm 0x7fbe3d4e8ce0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494806 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1494744, ip=192.168.0.32)[0m gpu17:1494744:1494744 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2540431, ip=192.168.0.38)[0m gpu23:2540431:2540431 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2540430, ip=192.168.0.38)[0m gpu23:2540430:2540430 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO comm 0x7f91af45f550 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665046 [1] NCCL INFO comm 0x7f91883fd1e0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3665044 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3664982, ip=192.168.0.39)[0m gpu24:3664982:3664982 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3304141, ip=192.168.0.31)[0m gpu16:3304141:3304141 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3304142, ip=192.168.0.31)[0m gpu16:3304142:3304142 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2369780, ip=192.168.0.27)[0m gpu12:2369780:2369780 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2369781, ip=192.168.0.27)[0m gpu12:2369781:2369781 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990520 [1] NCCL INFO comm 0x7eee32c5f240 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO comm 0x7eee36ae35b0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990518 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=990393, ip=192.168.0.26)[0m gpu11:990393:990393 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2173815, ip=192.168.0.18)[0m gpu3:2173815:2173815 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 403.87 s

[71.85061383247375, 54.99192762374878, 54.28717374801636, 54.191866397857666, 54.265905141830444, 53.95611906051636, 53.87856602668762]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 275.112 s.
 - Average e2e iteration time: 55.022003173828125 s.
 - Total local training time: 270.58001708984375 s.
 - Average local iteration time: 54.11600112915039 s.
 - Max allocated memory among devices: 22.27 GB.
 - Compilation times:  {'stage-construction': 30.675049304962158, 'stage-construction-dp': 1.473907709121704, 'stage-construction-compilation': 6.409663915634155, 'stage-construction-profiling': 11.568066596984863}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 54.11592483520508
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_4B_512.pkl`...

------------------------------------------------------------------
- (3/3) Profiling wide_resnet_4B with batch size: 1024...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal/wide_resnet_4B_1024.pkl`, creating it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 640
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.759 GB, invar_size=0.845 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.179 GB, invar_size=0.112 GB, outvar_size=1.675 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.179 GB, invar_size=0.112 GB, outvar_size=1.675 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.178, peak_memory=3.533 GB, invar_size=1.140 GB, outvar_size=0.957 GB, temp_buffer_size=1.436 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.553 GB, invar_size=1.541 GB, outvar_size=0.718 GB, temp_buffer_size=1.294 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.759 GB, invar_size=0.845 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.552, peak_memory=5.107 GB, invar_size=0.083 GB, outvar_size=2.153 GB, temp_buffer_size=2.871 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.047 GB, invar_size=0.845 GB, outvar_size=0.957 GB, temp_buffer_size=1.245 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.147 GB, invar_size=0.828 GB, outvar_size=1.914 GB, temp_buffer_size=2.405 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.147 GB, invar_size=0.828 GB, outvar_size=1.914 GB, temp_buffer_size=2.405 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.572 GB, invar_size=1.223 GB, outvar_size=1.436 GB, temp_buffer_size=1.914 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=5.463 GB, invar_size=2.168 GB, outvar_size=0.845 GB, temp_buffer_size=2.816 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.997 GB, invar_size=1.360 GB, outvar_size=0.957 GB, temp_buffer_size=1.680 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.572 GB, invar_size=1.223 GB, outvar_size=1.436 GB, temp_buffer_size=1.914 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.758, peak_memory=8.268 GB, invar_size=2.047 GB, outvar_size=2.393 GB, temp_buffer_size=3.828 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.448, peak_memory=6.282 GB, invar_size=1.491 GB, outvar_size=1.914 GB, temp_buffer_size=2.877 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=3.677 GB, invar_size=1.763 GB, outvar_size=0.479 GB, temp_buffer_size=1.436 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=5.463 GB, invar_size=2.168 GB, outvar_size=0.845 GB, temp_buffer_size=2.816 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=5.463 GB, invar_size=2.168 GB, outvar_size=0.845 GB, temp_buffer_size=2.816 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=2.886 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.196 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.161 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.978 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.575, peak_memory=6.895 GB, invar_size=2.280 GB, outvar_size=0.662 GB, temp_buffer_size=4.136 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.744 GB, invar_size=2.091 GB, outvar_size=0.479 GB, temp_buffer_size=1.175 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=1.161, peak_memory=12.836 GB, invar_size=2.283 GB, outvar_size=0.047 GB, temp_buffer_size=10.553 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=1.488, peak_memory=14.008 GB, invar_size=3.460 GB, outvar_size=1.491 GB, temp_buffer_size=9.113 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.744 GB, invar_size=2.091 GB, outvar_size=0.479 GB, temp_buffer_size=1.175 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.553 GB, invar_size=1.541 GB, outvar_size=0.718 GB, temp_buffer_size=1.294 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=9.803 GB, invar_size=1.880 GB, outvar_size=0.094 GB, temp_buffer_size=7.923 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.386, peak_memory=4.599 GB, invar_size=1.488 GB, outvar_size=1.196 GB, temp_buffer_size=1.914 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.161 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.978 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.575, peak_memory=6.895 GB, invar_size=2.280 GB, outvar_size=0.662 GB, temp_buffer_size=4.136 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=1.064, peak_memory=13.681 GB, invar_size=4.572 GB, outvar_size=1.090 GB, temp_buffer_size=8.152 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=2.408 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=0.718 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=9.316 GB, invar_size=2.924 GB, outvar_size=1.223 GB, temp_buffer_size=5.435 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.031, peak_memory=7.520 GB, invar_size=4.180 GB, outvar_size=2.090 GB, temp_buffer_size=2.861 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.047 GB, invar_size=0.845 GB, outvar_size=0.957 GB, temp_buffer_size=1.245 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.006, peak_memory=9.794 GB, invar_size=3.402 GB, outvar_size=1.223 GB, temp_buffer_size=5.435 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=10.794 GB, invar_size=2.852 GB, outvar_size=0.828 GB, temp_buffer_size=7.224 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.300, peak_memory=4.966 GB, invar_size=2.422 GB, outvar_size=0.972 GB, temp_buffer_size=2.305 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=10.794 GB, invar_size=2.852 GB, outvar_size=0.828 GB, temp_buffer_size=7.224 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.085 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=13.160 GB, invar_size=2.837 GB, outvar_size=0.094 GB, temp_buffer_size=10.323 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=2.408 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=0.718 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.378, peak_memory=6.740 GB, invar_size=3.047 GB, outvar_size=1.284 GB, temp_buffer_size=3.214 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.178, peak_memory=3.625 GB, invar_size=1.140 GB, outvar_size=0.957 GB, temp_buffer_size=1.527 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.031, peak_memory=7.759 GB, invar_size=4.420 GB, outvar_size=2.090 GB, temp_buffer_size=2.861 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.128, peak_memory=2.408 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=0.718 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.997 GB, invar_size=1.360 GB, outvar_size=0.957 GB, temp_buffer_size=1.680 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.255, peak_memory=5.944 GB, invar_size=2.116 GB, outvar_size=0.957 GB, temp_buffer_size=2.871 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.021, peak_memory=6.883 GB, invar_size=3.560 GB, outvar_size=1.541 GB, temp_buffer_size=2.845 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.085 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.922 GB, invar_size=1.705 GB, outvar_size=0.479 GB, temp_buffer_size=0.739 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.982 GB, invar_size=4.488 GB, outvar_size=0.359 GB, temp_buffer_size=2.134 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=5.463 GB, invar_size=2.168 GB, outvar_size=0.845 GB, temp_buffer_size=2.816 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.085 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.021, peak_memory=6.388 GB, invar_size=3.321 GB, outvar_size=1.541 GB, temp_buffer_size=2.589 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.544, peak_memory=6.873 GB, invar_size=3.216 GB, outvar_size=1.010 GB, temp_buffer_size=3.178 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.300, peak_memory=4.966 GB, invar_size=2.422 GB, outvar_size=0.972 GB, temp_buffer_size=2.305 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.085 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.219, peak_memory=4.696 GB, invar_size=2.603 GB, outvar_size=0.598 GB, temp_buffer_size=1.495 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.085 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.300, peak_memory=4.535 GB, invar_size=2.422 GB, outvar_size=0.972 GB, temp_buffer_size=1.874 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=9.060 GB, invar_size=6.686 GB, outvar_size=0.240 GB, temp_buffer_size=2.134 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.219, peak_memory=4.696 GB, invar_size=2.603 GB, outvar_size=0.598 GB, temp_buffer_size=1.495 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=5.538 GB, invar_size=3.702 GB, outvar_size=0.240 GB, temp_buffer_size=1.597 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.085 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.085 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=8.145 GB, invar_size=6.058 GB, outvar_size=0.120 GB, temp_buffer_size=1.967 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=5.538 GB, invar_size=3.702 GB, outvar_size=0.240 GB, temp_buffer_size=1.597 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=4.567 GB, invar_size=3.209 GB, outvar_size=0.120 GB, temp_buffer_size=1.238 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.300, peak_memory=4.535 GB, invar_size=2.422 GB, outvar_size=0.972 GB, temp_buffer_size=1.874 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=4.567 GB, invar_size=3.209 GB, outvar_size=0.120 GB, temp_buffer_size=1.238 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=6.085 GB, invar_size=3.648 GB, outvar_size=1.704 GB, temp_buffer_size=2.198 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=8.234 GB, invar_size=3.199 GB, outvar_size=1.360 GB, temp_buffer_size=4.078 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.718, peak_memory=10.060 GB, invar_size=3.274 GB, outvar_size=1.159 GB, temp_buffer_size=5.828 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.080, peak_memory=16.855 GB, invar_size=9.095 GB, outvar_size=4.488 GB, temp_buffer_size=7.520 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=7.755 GB, invar_size=2.720 GB, outvar_size=1.360 GB, temp_buffer_size=4.078 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.317, peak_memory=8.537 GB, invar_size=5.325 GB, outvar_size=2.364 GB, temp_buffer_size=2.972 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.317, peak_memory=8.537 GB, invar_size=5.325 GB, outvar_size=2.364 GB, temp_buffer_size=2.972 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=22.985 GB, invar_size=13.370 GB, outvar_size=6.685 GB, temp_buffer_size=9.376 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.253, peak_memory=10.580 GB, invar_size=7.164 GB, outvar_size=3.462 GB, temp_buffer_size=3.177 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.112, peak_memory=21.103 GB, invar_size=12.115 GB, outvar_size=6.058 GB, temp_buffer_size=8.868 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.191, peak_memory=9.190 GB, invar_size=6.297 GB, outvar_size=3.089 GB, temp_buffer_size=2.773 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.253, peak_memory=10.580 GB, invar_size=7.164 GB, outvar_size=3.462 GB, temp_buffer_size=3.177 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.191, peak_memory=9.190 GB, invar_size=6.297 GB, outvar_size=3.089 GB, temp_buffer_size=2.773 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 17.68 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.490 GB, invar_size=1.140 GB, outvar_size=0.957 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.659 GB, invar_size=1.960 GB, outvar_size=1.914 GB, temp_buffer_size=4.785 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.972 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=3.072 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.490 GB, invar_size=1.140 GB, outvar_size=0.957 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.490 GB, invar_size=1.140 GB, outvar_size=0.957 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.975 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=3.074 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.490 GB, invar_size=1.140 GB, outvar_size=0.957 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.655 GB, invar_size=2.280 GB, outvar_size=1.140 GB, temp_buffer_size=4.418 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.659 GB, invar_size=1.960 GB, outvar_size=1.914 GB, temp_buffer_size=4.785 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=16.389 GB, invar_size=3.920 GB, outvar_size=1.960 GB, temp_buffer_size=10.555 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.612 GB, invar_size=2.280 GB, outvar_size=1.140 GB, temp_buffer_size=5.375 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.611 GB, invar_size=2.280 GB, outvar_size=1.140 GB, temp_buffer_size=5.374 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.611 GB, invar_size=2.280 GB, outvar_size=1.140 GB, temp_buffer_size=5.374 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=16.389 GB, invar_size=3.920 GB, outvar_size=1.960 GB, temp_buffer_size=10.555 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.975 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=3.074 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.842 GB, invar_size=1.211 GB, outvar_size=0.479 GB, temp_buffer_size=1.152 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.972 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=3.072 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.496 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=2.595 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.496 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=2.595 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.466 GB, invar_size=3.170 GB, outvar_size=0.240 GB, temp_buffer_size=2.057 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.496 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=2.595 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.466 GB, invar_size=3.170 GB, outvar_size=0.240 GB, temp_buffer_size=2.057 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.496 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=2.595 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.496 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=2.595 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.424 GB, invar_size=3.248 GB, outvar_size=0.000 GB, temp_buffer_size=2.177 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.496 GB, invar_size=2.422 GB, outvar_size=1.211 GB, temp_buffer_size=2.595 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.424 GB, invar_size=3.248 GB, outvar_size=0.000 GB, temp_buffer_size=2.177 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=10.194 GB, invar_size=6.338 GB, outvar_size=3.169 GB, temp_buffer_size=3.616 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=10.194 GB, invar_size=6.338 GB, outvar_size=3.169 GB, temp_buffer_size=3.616 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.752 GB, invar_size=6.255 GB, outvar_size=3.247 GB, temp_buffer_size=3.257 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.490 GB, invar_size=1.140 GB, outvar_size=0.957 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.752 GB, invar_size=6.255 GB, outvar_size=3.247 GB, temp_buffer_size=3.257 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.490 GB, invar_size=1.140 GB, outvar_size=0.957 GB, temp_buffer_size=2.393 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=7.655 GB, invar_size=2.280 GB, outvar_size=1.140 GB, temp_buffer_size=4.418 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.612 GB, invar_size=2.280 GB, outvar_size=1.140 GB, temp_buffer_size=5.375 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.80 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 7, 3, 0) has been pruned...
[TMP] Stage (2, 7, 3, 1) has been pruned...
[TMP] Stage (2, 7, 3, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (3, 5, 2, 0) has been pruned...
[TMP] Stage (3, 5, 2, 1) has been pruned...
[TMP] Stage (3, 5, 2, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 8, 3, 0) has been pruned...
[TMP] Stage (3, 8, 3, 1) has been pruned...
[TMP] Stage (3, 8, 3, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (5, 7, 2, 0) has been pruned...
[TMP] Stage (5, 7, 2, 1) has been pruned...
[TMP] Stage (5, 7, 2, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4], [5], [6, 7], [8], [9], [10], [11], [12, 13], [14], [15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1), (1, 1), (1, 1), (1, 2), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1), (1, 1), (1, 1), (2, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO comm 0x36ef4c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO comm 0x3ea7370 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO comm 0x5a5b9d0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO comm 0x6061880 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=705832)[0m 
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=705832)[0m 
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=705832)[0m 
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO comm 0x4781fa0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO comm 0xae28160 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m 
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO comm 0xa3ab460 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=705832)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=705831)[0m 
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO comm 0xae331e0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=705832)[0m 
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO comm 0x78e9950 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=705831)[0m 
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=705831)[0m 
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO comm 0x35a2690 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=705831)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=705831)[0m 
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO comm 0x664bb50 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO comm 0x51d6fb0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=705831)[0m 
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO comm 0x74aa080 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO comm 0xa62c490 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO comm 0xb4f5610 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO comm 0x4187490 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO comm 0xb55e750 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO comm 0x6b590d0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO comm 0x4ec4f60 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO comm 0x6ba6910 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO comm 0x3b62290 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO comm 0x79d9ca0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO comm 0x4043b10 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO comm 0x6957c80 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO comm 0x6b5dae0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO comm 0x3627340 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO comm 0x6314fd0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO comm 0x69c9970 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [0] NCCL INFO comm 0x9a1af40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO comm 0x3c32dc0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO comm 0x6747ed0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542164 [1] NCCL INFO comm 0x99b1e00 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1500358, ip=192.168.0.32)[0m gpu17:1500358:1500358 [0] NCCL INFO comm 0x7a4ab80 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1500359, ip=192.168.0.32)[0m gpu17:1500359:1500359 [0] NCCL INFO comm 0x3915df0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 54.67 s
compilation time breakdown: {'stage-construction': '30.91', 'stage-construction-dp': '1.45', 'stage-construction-compilation': '6.37', 'stage-construction-profiling': '11.63'}
 - Compile (worker): 3.93 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993985 [1] NCCL INFO comm 0x7f4d0f179780 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO comm 0x7f4d05283270 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993983 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=993837, ip=192.168.0.26)[0m gpu11:993837:993837 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO comm 0x7fb901007790 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377862 [1] NCCL INFO comm 0x7fb8e9db3730 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377860 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2377711, ip=192.168.0.27)[0m gpu12:2377711:2377711 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=705832)[0m gpu2:705832:705832 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=705831)[0m gpu2:705831:705831 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO comm 0x7f7273ebaf80 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180111 [1] NCCL INFO comm 0x7f725d6f3960 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180109 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2180026, ip=192.168.0.18)[0m gpu3:2180026:2180026 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3305939, ip=192.168.0.31)[0m gpu16:3305939:3305939 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3305940, ip=192.168.0.31)[0m gpu16:3305940:3305940 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3671347, ip=192.168.0.39)[0m gpu24:3671347:3671347 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3671346, ip=192.168.0.39)[0m gpu24:3671346:3671346 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542268 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2542164, ip=192.168.0.38)[0m gpu23:2542164:2542270 [1] NCCL INFO Connected all trees
