
------------------------------------------------------------------
- (1/3) Profiling bert_2.6B with batch size: 128...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_2.6B_128.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fc59d30d700>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=4.692 GB, invar_size=4.203 GB, outvar_size=0.098 GB, temp_buffer_size=0.391 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.155, peak_memory=4.607 GB, invar_size=4.080 GB, outvar_size=0.098 GB, temp_buffer_size=0.430 GB, available_memory=35.242 GB)
result[(0, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=15.676 GB, invar_size=10.483 GB, outvar_size=5.193 GB, temp_buffer_size=5.193 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.600, peak_memory=15.108 GB, invar_size=9.992 GB, outvar_size=4.947 GB, temp_buffer_size=5.116 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 69.26 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.166, peak_memory=3.167 GB, invar_size=2.433 GB, outvar_size=0.117 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.166, peak_memory=3.167 GB, invar_size=2.433 GB, outvar_size=0.117 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.178, peak_memory=3.490 GB, invar_size=2.591 GB, outvar_size=0.117 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.178, peak_memory=3.490 GB, invar_size=2.591 GB, outvar_size=0.117 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.168, peak_memory=3.216 GB, invar_size=2.482 GB, outvar_size=0.117 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.168, peak_memory=3.216 GB, invar_size=2.482 GB, outvar_size=0.117 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.111, peak_memory=2.346 GB, invar_size=1.651 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.111, peak_memory=2.346 GB, invar_size=1.651 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.496, peak_memory=17.375 GB, invar_size=4.943 GB, outvar_size=2.433 GB, temp_buffer_size=12.393 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.496, peak_memory=17.375 GB, invar_size=4.943 GB, outvar_size=2.433 GB, temp_buffer_size=12.393 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.516, peak_memory=18.341 GB, invar_size=5.300 GB, outvar_size=2.591 GB, temp_buffer_size=13.041 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.516, peak_memory=18.341 GB, invar_size=5.300 GB, outvar_size=2.591 GB, temp_buffer_size=13.041 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.504, peak_memory=17.339 GB, invar_size=5.041 GB, outvar_size=2.482 GB, temp_buffer_size=12.259 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.504, peak_memory=17.339 GB, invar_size=5.041 GB, outvar_size=2.482 GB, temp_buffer_size=12.259 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.534, peak_memory=18.469 GB, invar_size=5.320 GB, outvar_size=2.640 GB, temp_buffer_size=13.110 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.534, peak_memory=18.469 GB, invar_size=5.320 GB, outvar_size=2.640 GB, temp_buffer_size=13.110 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 40.16 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2], [3, 4, 5]]
Result mesh_shapes: [(1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=2404938, ip=192.168.0.18)[0m gpu3:2404938:2404938 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2404938, ip=192.168.0.18)[0m gpu3:2404938:2404938 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2404938, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2404938, ip=192.168.0.18)[0m gpu3:2404938:2404938 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2404938, ip=192.168.0.18)[0m gpu3:2404938:2404938 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2404938, ip=192.168.0.18)[0m gpu3:2404938:2404938 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2404938, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1107754)[0m 
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1107754)[0m 
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1107754)[0m 
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1107754)[0m gpu2:1107754:1107754 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
