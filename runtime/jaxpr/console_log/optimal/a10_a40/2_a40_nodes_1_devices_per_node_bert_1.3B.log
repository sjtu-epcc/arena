
------------------------------------------------------------------
- (1/3) Profiling bert_1.3B with batch size: 128...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_1.3B_128.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f40bab106a0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.080, peak_memory=2.573 GB, invar_size=2.105 GB, outvar_size=0.078 GB, temp_buffer_size=0.391 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.084, peak_memory=2.506 GB, invar_size=2.006 GB, outvar_size=0.078 GB, temp_buffer_size=0.422 GB, available_memory=35.242 GB)
result[(0, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.314, peak_memory=9.011 GB, invar_size=5.382 GB, outvar_size=2.652 GB, temp_buffer_size=3.629 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.323, peak_memory=8.617 GB, invar_size=4.989 GB, outvar_size=2.455 GB, temp_buffer_size=3.628 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 50.68 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=1.876 GB, invar_size=1.188 GB, outvar_size=0.094 GB, temp_buffer_size=0.594 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=1.876 GB, invar_size=1.188 GB, outvar_size=0.094 GB, temp_buffer_size=0.594 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=2.229 GB, invar_size=1.354 GB, outvar_size=0.094 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=2.229 GB, invar_size=1.354 GB, outvar_size=0.094 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=1.876 GB, invar_size=1.188 GB, outvar_size=0.094 GB, temp_buffer_size=0.594 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=1.876 GB, invar_size=1.188 GB, outvar_size=0.094 GB, temp_buffer_size=0.594 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.054, peak_memory=1.438 GB, invar_size=0.782 GB, outvar_size=0.062 GB, temp_buffer_size=0.594 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.249, peak_memory=10.922 GB, invar_size=2.439 GB, outvar_size=1.188 GB, temp_buffer_size=8.452 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.249, peak_memory=10.922 GB, invar_size=2.439 GB, outvar_size=1.188 GB, temp_buffer_size=8.452 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.272, peak_memory=11.014 GB, invar_size=2.802 GB, outvar_size=1.354 GB, temp_buffer_size=8.212 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.272, peak_memory=11.014 GB, invar_size=2.802 GB, outvar_size=1.354 GB, temp_buffer_size=8.212 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.054, peak_memory=1.438 GB, invar_size=0.782 GB, outvar_size=0.062 GB, temp_buffer_size=0.594 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.249, peak_memory=11.611 GB, invar_size=2.439 GB, outvar_size=1.188 GB, temp_buffer_size=9.141 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.249, peak_memory=11.611 GB, invar_size=2.439 GB, outvar_size=1.188 GB, temp_buffer_size=9.141 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.272, peak_memory=9.830 GB, invar_size=2.689 GB, outvar_size=1.329 GB, temp_buffer_size=7.110 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.272, peak_memory=9.830 GB, invar_size=2.689 GB, outvar_size=1.329 GB, temp_buffer_size=7.110 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 30.14 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2], [3, 4, 5]]
Result mesh_shapes: [(1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1009190)[0m 
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1009190)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1009190)[0m 
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1009190)[0m 
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2355954, ip=192.168.0.18)[0m gpu3:2355954:2355954 [0] NCCL INFO comm 0x415dd40 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO comm 0x47a2950 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
 - Compile (driver): 116.89 s
compilation time breakdown: {'stage-construction': '84.33', 'stage-construction-dp': '1.37', 'stage-construction-compilation': '30.47', 'stage-construction-profiling': '28.65'}
 - Compile (worker): 9.99 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1009190)[0m gpu2:1009190:1009190 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 101.54 s

[16.576217889785767, 13.139936685562134, 13.22271990776062, 13.121177434921265, 13.145243883132935, 13.132943868637085, 13.136413335800171]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 68.897 s.
 - Average e2e iteration time: 13.779000282287598 s.
 - Total local training time: 65.75800323486328 s.
 - Average local iteration time: 13.152000427246094 s.
 - Max allocated memory among devices: 20.109 GB.
 - Compilation times:  {'stage-construction': 84.33084225654602, 'stage-construction-dp': 1.3652291297912598, 'stage-construction-compilation': 30.474074363708496, 'stage-construction-profiling': 28.64970064163208}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 13.151700019836426
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_1.3B_128.pkl`...

------------------------------------------------------------------
- (2/3) Profiling bert_1.3B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_1.3B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7ff97780b6d0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=3.042 GB, invar_size=2.105 GB, outvar_size=0.156 GB, temp_buffer_size=0.781 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.159, peak_memory=3.006 GB, invar_size=2.006 GB, outvar_size=0.156 GB, temp_buffer_size=0.844 GB, available_memory=35.242 GB)
result[(0, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.568, peak_memory=12.717 GB, invar_size=5.460 GB, outvar_size=2.652 GB, temp_buffer_size=7.257 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.589, peak_memory=12.324 GB, invar_size=5.067 GB, outvar_size=2.455 GB, temp_buffer_size=7.256 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 50.52 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.160, peak_memory=2.594 GB, invar_size=1.219 GB, outvar_size=0.188 GB, temp_buffer_size=1.188 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.160, peak_memory=2.594 GB, invar_size=1.219 GB, outvar_size=0.188 GB, temp_buffer_size=1.188 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.185, peak_memory=3.104 GB, invar_size=1.354 GB, outvar_size=0.188 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.185, peak_memory=3.104 GB, invar_size=1.354 GB, outvar_size=0.188 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.160, peak_memory=2.594 GB, invar_size=1.219 GB, outvar_size=0.188 GB, temp_buffer_size=1.188 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.160, peak_memory=2.594 GB, invar_size=1.219 GB, outvar_size=0.188 GB, temp_buffer_size=1.188 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.103, peak_memory=2.125 GB, invar_size=0.813 GB, outvar_size=0.125 GB, temp_buffer_size=1.188 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.480, peak_memory=19.525 GB, invar_size=2.564 GB, outvar_size=1.219 GB, temp_buffer_size=16.899 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.480, peak_memory=19.525 GB, invar_size=2.564 GB, outvar_size=1.219 GB, temp_buffer_size=16.899 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.525, peak_memory=19.313 GB, invar_size=2.896 GB, outvar_size=1.354 GB, temp_buffer_size=16.417 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.525, peak_memory=19.313 GB, invar_size=2.896 GB, outvar_size=1.354 GB, temp_buffer_size=16.417 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.103, peak_memory=2.125 GB, invar_size=0.813 GB, outvar_size=0.125 GB, temp_buffer_size=1.188 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.480, peak_memory=20.903 GB, invar_size=2.564 GB, outvar_size=1.219 GB, temp_buffer_size=18.277 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.480, peak_memory=20.903 GB, invar_size=2.564 GB, outvar_size=1.219 GB, temp_buffer_size=18.277 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.527, peak_memory=17.058 GB, invar_size=2.783 GB, outvar_size=1.360 GB, temp_buffer_size=14.213 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.527, peak_memory=17.058 GB, invar_size=2.783 GB, outvar_size=1.360 GB, temp_buffer_size=14.213 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 30.77 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5]]
Result mesh_shapes: [(2, 1)]
Result logical_mesh_shapes: [(2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 126.69 s
compilation time breakdown: {'stage-construction': '84.97', 'stage-construction-dp': '1.30', 'stage-construction-compilation': '30.01', 'stage-construction-profiling': '29.40'}
 - Compile (worker): 20.43 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1015358)[0m 
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1015358)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1015358)[0m 
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1015358)[0m 
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO comm 0x3a48da30 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1015358)[0m gpu2:1015358:1015358 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 179.16 s

[25.86653184890747, 23.549243927001953, 23.55869746208191, 23.534655809402466, 23.519009828567505, 23.52772092819214, 23.543397903442383]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 123.872 s.
 - Average e2e iteration time: 24.774002075195312 s.
 - Total local training time: 117.6830062866211 s.
 - Average local iteration time: 23.53700065612793 s.
 - Max allocated memory among devices: 23.018 GB.
 - Compilation times:  {'stage-construction': 84.97315907478333, 'stage-construction-dp': 1.297593593597412, 'stage-construction-compilation': 30.013625860214233, 'stage-construction-profiling': 29.403587818145752}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 23.536697387695312
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_1.3B_256.pkl`...

------------------------------------------------------------------
- (3/3) Profiling bert_1.3B with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_1.3B_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f262e18c7c0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.289, peak_memory=3.980 GB, invar_size=2.105 GB, outvar_size=0.312 GB, temp_buffer_size=1.563 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.289, peak_memory=3.980 GB, invar_size=2.105 GB, outvar_size=0.312 GB, temp_buffer_size=1.563 GB, available_memory=35.242 GB)
result[(0, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=1.077, peak_memory=20.130 GB, invar_size=5.616 GB, outvar_size=2.652 GB, temp_buffer_size=14.514 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=1.077, peak_memory=20.130 GB, invar_size=5.616 GB, outvar_size=2.652 GB, temp_buffer_size=14.514 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 47.23 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.319, peak_memory=4.032 GB, invar_size=1.282 GB, outvar_size=0.375 GB, temp_buffer_size=2.375 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.319, peak_memory=4.032 GB, invar_size=1.282 GB, outvar_size=0.375 GB, temp_buffer_size=2.375 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.368, peak_memory=4.855 GB, invar_size=1.354 GB, outvar_size=0.375 GB, temp_buffer_size=3.125 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.368, peak_memory=4.855 GB, invar_size=1.354 GB, outvar_size=0.375 GB, temp_buffer_size=3.125 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.319, peak_memory=4.032 GB, invar_size=1.282 GB, outvar_size=0.375 GB, temp_buffer_size=2.375 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.319, peak_memory=4.032 GB, invar_size=1.282 GB, outvar_size=0.375 GB, temp_buffer_size=2.375 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.206, peak_memory=3.501 GB, invar_size=0.876 GB, outvar_size=0.250 GB, temp_buffer_size=2.375 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.954, peak_memory=36.730 GB, invar_size=2.814 GB, outvar_size=1.282 GB, temp_buffer_size=33.791 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.954, peak_memory=36.730 GB, invar_size=2.814 GB, outvar_size=1.282 GB, temp_buffer_size=33.791 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=1.046, peak_memory=35.910 GB, invar_size=3.084 GB, outvar_size=1.354 GB, temp_buffer_size=32.827 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=1.046, peak_memory=35.910 GB, invar_size=3.084 GB, outvar_size=1.354 GB, temp_buffer_size=32.827 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.206, peak_memory=3.501 GB, invar_size=0.876 GB, outvar_size=0.250 GB, temp_buffer_size=2.375 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.954, peak_memory=39.484 GB, invar_size=2.814 GB, outvar_size=1.282 GB, temp_buffer_size=36.545 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.954, peak_memory=39.484 GB, invar_size=2.814 GB, outvar_size=1.282 GB, temp_buffer_size=36.545 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=1.050, peak_memory=31.512 GB, invar_size=2.970 GB, outvar_size=1.423 GB, temp_buffer_size=28.417 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=1.050, peak_memory=31.512 GB, invar_size=2.970 GB, outvar_size=1.423 GB, temp_buffer_size=28.417 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 29.87 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5]]
Result mesh_shapes: [(2, 1)]
Result logical_mesh_shapes: [(2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 124.40 s
compilation time breakdown: {'stage-construction': '80.75', 'stage-construction-dp': '1.37', 'stage-construction-compilation': '28.30', 'stage-construction-profiling': '27.06'}
 - Compile (worker): 19.00 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1023912)[0m 
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1023912)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1023912)[0m 
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1023912)[0m 
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO comm 0x377c96f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1023912)[0m gpu2:1023912:1023912 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 337.78 s

[48.54803109169006, 45.89798974990845, 45.92099142074585, 45.91636347770691, 45.899620056152344, 45.922425985336304, 45.90077567100525]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 237.757 s.
 - Average e2e iteration time: 47.551002502441406 s.
 - Total local training time: 229.5600128173828 s.
 - Average local iteration time: 45.91200256347656 s.
 - Max allocated memory among devices: 32.776 GB.
 - Compilation times:  {'stage-construction': 80.75290155410767, 'stage-construction-dp': 1.3708033561706543, 'stage-construction-compilation': 28.297285556793213, 'stage-construction-profiling': 27.060612201690674}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 45.91203308105469
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_1.3B_512.pkl`...
