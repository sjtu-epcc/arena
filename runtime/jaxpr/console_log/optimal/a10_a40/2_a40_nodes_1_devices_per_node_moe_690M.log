
------------------------------------------------------------------
- (1/3) Profiling moe_690M with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_690M_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f1f20c5ba90>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.024, peak_memory=1.842 GB, invar_size=1.256 GB, outvar_size=0.082 GB, temp_buffer_size=0.504 GB, available_memory=35.242 GB)
result[(0, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.094, peak_memory=4.488 GB, invar_size=2.730 GB, outvar_size=1.324 GB, temp_buffer_size=1.759 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=1.280 GB, invar_size=0.694 GB, outvar_size=0.082 GB, temp_buffer_size=0.504 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.102, peak_memory=3.363 GB, invar_size=1.605 GB, outvar_size=0.761 GB, temp_buffer_size=1.759 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 30.83 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.460 GB, invar_size=0.639 GB, outvar_size=0.070 GB, temp_buffer_size=0.751 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.460 GB, invar_size=0.639 GB, outvar_size=0.070 GB, temp_buffer_size=0.751 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.027, peak_memory=1.788 GB, invar_size=0.920 GB, outvar_size=0.094 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.024, peak_memory=1.719 GB, invar_size=0.641 GB, outvar_size=0.070 GB, temp_buffer_size=1.008 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.024, peak_memory=1.719 GB, invar_size=0.641 GB, outvar_size=0.070 GB, temp_buffer_size=1.008 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.027, peak_memory=1.788 GB, invar_size=0.920 GB, outvar_size=0.094 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.028, peak_memory=1.769 GB, invar_size=0.667 GB, outvar_size=0.094 GB, temp_buffer_size=1.008 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.028, peak_memory=1.769 GB, invar_size=0.667 GB, outvar_size=0.094 GB, temp_buffer_size=1.008 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=4.261 GB, invar_size=1.352 GB, outvar_size=0.641 GB, temp_buffer_size=2.909 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=4.261 GB, invar_size=1.352 GB, outvar_size=0.641 GB, temp_buffer_size=2.909 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=4.633 GB, invar_size=1.325 GB, outvar_size=0.639 GB, temp_buffer_size=3.285 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=4.633 GB, invar_size=1.325 GB, outvar_size=0.639 GB, temp_buffer_size=3.285 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=4.738 GB, invar_size=1.428 GB, outvar_size=0.667 GB, temp_buffer_size=3.310 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=4.738 GB, invar_size=1.428 GB, outvar_size=0.667 GB, temp_buffer_size=3.310 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.079, peak_memory=5.243 GB, invar_size=1.910 GB, outvar_size=0.920 GB, temp_buffer_size=3.309 GB, available_memory=35.446 GB)
result[(2, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.025, peak_memory=1.505 GB, invar_size=0.661 GB, outvar_size=0.094 GB, temp_buffer_size=0.751 GB, available_memory=35.446 GB)
result[(2, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.025, peak_memory=1.505 GB, invar_size=0.661 GB, outvar_size=0.094 GB, temp_buffer_size=0.751 GB, available_memory=35.446 GB)
result[(1, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=1.814 GB, invar_size=0.947 GB, outvar_size=0.117 GB, temp_buffer_size=0.751 GB, available_memory=35.446 GB)
result[(1, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=1.814 GB, invar_size=0.947 GB, outvar_size=0.117 GB, temp_buffer_size=0.751 GB, available_memory=35.446 GB)
result[(3, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.507 GB, invar_size=0.639 GB, outvar_size=0.094 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(2, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.031, peak_memory=1.833 GB, invar_size=0.942 GB, outvar_size=0.117 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(2, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.031, peak_memory=1.833 GB, invar_size=0.942 GB, outvar_size=0.117 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.079, peak_memory=5.243 GB, invar_size=1.910 GB, outvar_size=0.920 GB, temp_buffer_size=3.309 GB, available_memory=35.446 GB)
result[(2, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.099 GB, invar_size=1.392 GB, outvar_size=0.661 GB, temp_buffer_size=3.684 GB, available_memory=35.446 GB)
result[(3, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.507 GB, invar_size=0.639 GB, outvar_size=0.094 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(3, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.507 GB, invar_size=0.639 GB, outvar_size=0.094 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(1, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.092, peak_memory=5.740 GB, invar_size=1.987 GB, outvar_size=0.946 GB, temp_buffer_size=3.730 GB, available_memory=35.446 GB)
result[(1, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.092, peak_memory=5.740 GB, invar_size=1.987 GB, outvar_size=0.946 GB, temp_buffer_size=3.730 GB, available_memory=35.446 GB)
result[(2, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.099 GB, invar_size=1.392 GB, outvar_size=0.661 GB, temp_buffer_size=3.684 GB, available_memory=35.446 GB)
result[(3, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.507 GB, invar_size=0.639 GB, outvar_size=0.094 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(4, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.457 GB, invar_size=0.612 GB, outvar_size=0.070 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(4, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.457 GB, invar_size=0.612 GB, outvar_size=0.070 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(3, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.062, peak_memory=4.571 GB, invar_size=1.348 GB, outvar_size=0.639 GB, temp_buffer_size=3.199 GB, available_memory=35.446 GB)
result[(3, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.062, peak_memory=4.571 GB, invar_size=1.348 GB, outvar_size=0.639 GB, temp_buffer_size=3.199 GB, available_memory=35.446 GB)
result[(2, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.089, peak_memory=5.756 GB, invar_size=1.978 GB, outvar_size=0.942 GB, temp_buffer_size=3.755 GB, available_memory=35.446 GB)
result[(3, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.089, peak_memory=5.025 GB, invar_size=1.484 GB, outvar_size=0.707 GB, temp_buffer_size=3.518 GB, available_memory=35.446 GB)
result[(2, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.089, peak_memory=5.756 GB, invar_size=1.978 GB, outvar_size=0.942 GB, temp_buffer_size=3.755 GB, available_memory=35.446 GB)
result[(4, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=4.926 GB, invar_size=1.407 GB, outvar_size=0.680 GB, temp_buffer_size=3.495 GB, available_memory=35.446 GB)
result[(4, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=4.926 GB, invar_size=1.407 GB, outvar_size=0.680 GB, temp_buffer_size=3.495 GB, available_memory=35.446 GB)
result[(3, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.089, peak_memory=5.025 GB, invar_size=1.484 GB, outvar_size=0.707 GB, temp_buffer_size=3.518 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 37.12 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3], [4, 5, 6, 7]]
Result mesh_shapes: [(1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {}]
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=971968)[0m 
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=971968)[0m 
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=971968)[0m 
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=971968)[0m gpu2:971968:971968 [0] NCCL INFO comm 0x37b3c70 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO comm 0x41ccaa0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
 - Compile (driver): 89.97 s
compilation time breakdown: {'stage-construction': '70.50', 'stage-construction-dp': '1.35', 'stage-construction-compilation': '20.11', 'stage-construction-profiling': '23.16'}
 - Compile (worker): 8.58 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=2335528, ip=192.168.0.18)[0m gpu3:2335528:2335528 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 38.87 s

[6.492170095443726, 4.928289175033569, 4.910902738571167, 4.92359471321106, 4.924182415008545, 4.92002534866333, 4.921185255050659]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 25.867 s.
 - Average e2e iteration time: 5.173000335693359 s.
 - Total local training time: 24.600000381469727 s.
 - Average local iteration time: 4.920000076293945 s.
 - Max allocated memory among devices: 7.65 GB.
 - Compilation times:  {'stage-construction': 70.50325632095337, 'stage-construction-dp': 1.3451988697052002, 'stage-construction-compilation': 20.114402770996094, 'stage-construction-profiling': 23.162325859069824}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 4.919978618621826
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_690M_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling moe_690M with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_690M_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f783d085ca0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=2.428 GB, invar_size=1.256 GB, outvar_size=0.164 GB, temp_buffer_size=1.008 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.063, peak_memory=1.866 GB, invar_size=0.694 GB, outvar_size=0.164 GB, temp_buffer_size=1.008 GB, available_memory=35.242 GB)
result[(0, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.165, peak_memory=6.329 GB, invar_size=2.812 GB, outvar_size=1.324 GB, temp_buffer_size=3.517 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.188, peak_memory=5.204 GB, invar_size=1.687 GB, outvar_size=0.761 GB, temp_buffer_size=3.517 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 29.71 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=2.797 GB, invar_size=0.641 GB, outvar_size=0.141 GB, temp_buffer_size=2.016 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=2.797 GB, invar_size=0.641 GB, outvar_size=0.141 GB, temp_buffer_size=2.016 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=2.304 GB, invar_size=0.662 GB, outvar_size=0.141 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=2.304 GB, invar_size=0.662 GB, outvar_size=0.141 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.055, peak_memory=2.870 GB, invar_size=0.667 GB, outvar_size=0.188 GB, temp_buffer_size=2.016 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.055, peak_memory=2.870 GB, invar_size=0.667 GB, outvar_size=0.188 GB, temp_buffer_size=2.016 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.053, peak_memory=2.679 GB, invar_size=0.944 GB, outvar_size=0.188 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.053, peak_memory=2.679 GB, invar_size=0.944 GB, outvar_size=0.188 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=7.240 GB, invar_size=1.422 GB, outvar_size=0.641 GB, temp_buffer_size=5.818 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=7.240 GB, invar_size=1.422 GB, outvar_size=0.641 GB, temp_buffer_size=5.818 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=8.034 GB, invar_size=1.418 GB, outvar_size=0.662 GB, temp_buffer_size=6.569 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=8.034 GB, invar_size=1.418 GB, outvar_size=0.662 GB, temp_buffer_size=6.569 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.149, peak_memory=8.140 GB, invar_size=1.522 GB, outvar_size=0.667 GB, temp_buffer_size=6.618 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.149, peak_memory=8.140 GB, invar_size=1.522 GB, outvar_size=0.667 GB, temp_buffer_size=6.618 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.153, peak_memory=8.692 GB, invar_size=2.028 GB, outvar_size=0.944 GB, temp_buffer_size=6.617 GB, available_memory=35.446 GB)
result[(2, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.048, peak_memory=2.373 GB, invar_size=0.684 GB, outvar_size=0.188 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(2, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.048, peak_memory=2.373 GB, invar_size=0.684 GB, outvar_size=0.188 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(1, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.062, peak_memory=2.705 GB, invar_size=0.970 GB, outvar_size=0.234 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(1, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.062, peak_memory=2.705 GB, invar_size=0.970 GB, outvar_size=0.234 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(2, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.060, peak_memory=2.748 GB, invar_size=0.966 GB, outvar_size=0.234 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(3, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=2.398 GB, invar_size=0.662 GB, outvar_size=0.188 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.153, peak_memory=8.692 GB, invar_size=2.028 GB, outvar_size=0.944 GB, temp_buffer_size=6.617 GB, available_memory=35.446 GB)
result[(2, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.060, peak_memory=2.748 GB, invar_size=0.966 GB, outvar_size=0.234 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(2, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=8.923 GB, invar_size=1.509 GB, outvar_size=0.684 GB, temp_buffer_size=7.367 GB, available_memory=35.446 GB)
result[(3, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=2.398 GB, invar_size=0.662 GB, outvar_size=0.188 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(3, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=2.398 GB, invar_size=0.662 GB, outvar_size=0.188 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(1, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.178, peak_memory=9.634 GB, invar_size=2.127 GB, outvar_size=0.970 GB, temp_buffer_size=7.460 GB, available_memory=35.446 GB)
result[(2, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=8.923 GB, invar_size=1.509 GB, outvar_size=0.684 GB, temp_buffer_size=7.367 GB, available_memory=35.446 GB)
result[(1, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.178, peak_memory=9.634 GB, invar_size=2.127 GB, outvar_size=0.970 GB, temp_buffer_size=7.460 GB, available_memory=35.446 GB)
result[(3, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=2.398 GB, invar_size=0.662 GB, outvar_size=0.188 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(4, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.324 GB, invar_size=0.636 GB, outvar_size=0.141 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(3, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.120, peak_memory=7.909 GB, invar_size=1.465 GB, outvar_size=0.662 GB, temp_buffer_size=6.397 GB, available_memory=35.446 GB)
result[(4, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.324 GB, invar_size=0.636 GB, outvar_size=0.141 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(3, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.120, peak_memory=7.909 GB, invar_size=1.465 GB, outvar_size=0.662 GB, temp_buffer_size=6.397 GB, available_memory=35.446 GB)
result[(2, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.172, peak_memory=9.674 GB, invar_size=2.119 GB, outvar_size=0.965 GB, temp_buffer_size=7.509 GB, available_memory=35.446 GB)
result[(2, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.172, peak_memory=9.674 GB, invar_size=2.119 GB, outvar_size=0.965 GB, temp_buffer_size=7.509 GB, available_memory=35.446 GB)
result[(3, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.174, peak_memory=8.682 GB, invar_size=1.601 GB, outvar_size=0.730 GB, temp_buffer_size=7.034 GB, available_memory=35.446 GB)
result[(4, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.148, peak_memory=8.537 GB, invar_size=1.501 GB, outvar_size=0.704 GB, temp_buffer_size=6.989 GB, available_memory=35.446 GB)
result[(4, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.148, peak_memory=8.537 GB, invar_size=1.501 GB, outvar_size=0.704 GB, temp_buffer_size=6.989 GB, available_memory=35.446 GB)
result[(3, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.174, peak_memory=8.682 GB, invar_size=1.601 GB, outvar_size=0.730 GB, temp_buffer_size=7.034 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 36.49 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7]]
Result mesh_shapes: [(2, 1)]
Result logical_mesh_shapes: [(2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 91.72 s
compilation time breakdown: {'stage-construction': '68.81', 'stage-construction-dp': '1.39', 'stage-construction-compilation': '18.72', 'stage-construction-profiling': '23.60'}
 - Compile (worker): 16.79 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=977542)[0m 
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=977542)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=977542)[0m 
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=977542)[0m 
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO comm 0x35359400 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=977542)[0m gpu2:977542:977542 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 64.99 s

[9.813905477523804, 8.689606666564941, 8.696090698242188, 8.686631441116333, 8.685079336166382, 8.684384107589722, 8.68170166015625]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 44.392 s.
 - Average e2e iteration time: 8.878000259399414 s.
 - Total local training time: 43.43400192260742 s.
 - Average local iteration time: 8.687000274658203 s.
 - Max allocated memory among devices: 10.122 GB.
 - Compilation times:  {'stage-construction': 68.80777955055237, 'stage-construction-dp': 1.3944854736328125, 'stage-construction-compilation': 18.717883825302124, 'stage-construction-profiling': 23.60183620452881}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 8.68677806854248
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_690M_512.pkl`...

------------------------------------------------------------------
- (3/3) Profiling moe_690M with batch size: 1024...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_690M_1024.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f4d25c0a2b0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.088, peak_memory=3.600 GB, invar_size=1.256 GB, outvar_size=0.328 GB, temp_buffer_size=2.016 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.123, peak_memory=3.038 GB, invar_size=0.694 GB, outvar_size=0.328 GB, temp_buffer_size=2.016 GB, available_memory=35.242 GB)
result[(0, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=10.011 GB, invar_size=2.976 GB, outvar_size=1.324 GB, temp_buffer_size=7.035 GB, available_memory=35.242 GB)
result[(0, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.366, peak_memory=8.886 GB, invar_size=1.851 GB, outvar_size=0.761 GB, temp_buffer_size=7.035 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 31.08 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=4.954 GB, invar_size=0.641 GB, outvar_size=0.281 GB, temp_buffer_size=4.031 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=4.954 GB, invar_size=0.641 GB, outvar_size=0.281 GB, temp_buffer_size=4.031 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.108, peak_memory=5.074 GB, invar_size=0.668 GB, outvar_size=0.375 GB, temp_buffer_size=4.031 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=3.993 GB, invar_size=0.709 GB, outvar_size=0.281 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.108, peak_memory=5.074 GB, invar_size=0.668 GB, outvar_size=0.375 GB, temp_buffer_size=4.031 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=3.993 GB, invar_size=0.709 GB, outvar_size=0.281 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=4.461 GB, invar_size=0.991 GB, outvar_size=0.375 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=4.461 GB, invar_size=0.991 GB, outvar_size=0.375 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=13.199 GB, invar_size=1.563 GB, outvar_size=0.641 GB, temp_buffer_size=11.636 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=13.199 GB, invar_size=1.563 GB, outvar_size=0.641 GB, temp_buffer_size=11.636 GB, available_memory=35.446 GB)
result[(0, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.291, peak_memory=14.944 GB, invar_size=1.710 GB, outvar_size=0.667 GB, temp_buffer_size=13.234 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=14.837 GB, invar_size=1.606 GB, outvar_size=0.709 GB, temp_buffer_size=13.137 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=14.837 GB, invar_size=1.606 GB, outvar_size=0.709 GB, temp_buffer_size=13.137 GB, available_memory=35.446 GB)
result[(0, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.291, peak_memory=14.944 GB, invar_size=1.710 GB, outvar_size=0.667 GB, temp_buffer_size=13.234 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.300, peak_memory=15.589 GB, invar_size=2.262 GB, outvar_size=0.990 GB, temp_buffer_size=13.233 GB, available_memory=35.446 GB)
result[(2, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.093, peak_memory=4.108 GB, invar_size=0.731 GB, outvar_size=0.375 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(1, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.121, peak_memory=4.488 GB, invar_size=1.017 GB, outvar_size=0.469 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(1, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.121, peak_memory=4.488 GB, invar_size=1.017 GB, outvar_size=0.469 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(2, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.093, peak_memory=4.108 GB, invar_size=0.731 GB, outvar_size=0.375 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(2, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.117, peak_memory=4.577 GB, invar_size=1.013 GB, outvar_size=0.469 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.300, peak_memory=15.589 GB, invar_size=2.262 GB, outvar_size=0.990 GB, temp_buffer_size=13.233 GB, available_memory=35.446 GB)
result[(3, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.180 GB, invar_size=0.709 GB, outvar_size=0.375 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(2, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.117, peak_memory=4.577 GB, invar_size=1.013 GB, outvar_size=0.469 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(2, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.270, peak_memory=16.570 GB, invar_size=1.744 GB, outvar_size=0.731 GB, temp_buffer_size=14.733 GB, available_memory=35.446 GB)
result[(3, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.180 GB, invar_size=0.709 GB, outvar_size=0.375 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(2, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.270, peak_memory=16.570 GB, invar_size=1.744 GB, outvar_size=0.731 GB, temp_buffer_size=14.733 GB, available_memory=35.446 GB)
result[(3, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.180 GB, invar_size=0.709 GB, outvar_size=0.375 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(1, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.349, peak_memory=17.422 GB, invar_size=2.409 GB, outvar_size=1.017 GB, temp_buffer_size=14.919 GB, available_memory=35.446 GB)
result[(1, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.349, peak_memory=17.422 GB, invar_size=2.409 GB, outvar_size=1.017 GB, temp_buffer_size=14.919 GB, available_memory=35.446 GB)
result[(3, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.180 GB, invar_size=0.709 GB, outvar_size=0.375 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(4, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=4.060 GB, invar_size=0.683 GB, outvar_size=0.281 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(3, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.234, peak_memory=14.586 GB, invar_size=1.700 GB, outvar_size=0.709 GB, temp_buffer_size=12.793 GB, available_memory=35.446 GB)
result[(4, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=4.060 GB, invar_size=0.683 GB, outvar_size=0.281 GB, temp_buffer_size=3.096 GB, available_memory=35.446 GB)
result[(3, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.234, peak_memory=14.586 GB, invar_size=1.700 GB, outvar_size=0.709 GB, temp_buffer_size=12.793 GB, available_memory=35.446 GB)
result[(2, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.338, peak_memory=17.510 GB, invar_size=2.400 GB, outvar_size=1.012 GB, temp_buffer_size=15.016 GB, available_memory=35.446 GB)
result[(2, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.338, peak_memory=17.510 GB, invar_size=2.400 GB, outvar_size=1.012 GB, temp_buffer_size=15.016 GB, available_memory=35.446 GB)
result[(3, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.342, peak_memory=15.996 GB, invar_size=1.836 GB, outvar_size=0.777 GB, temp_buffer_size=14.066 GB, available_memory=35.446 GB)
result[(4, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=15.760 GB, invar_size=1.689 GB, outvar_size=0.751 GB, temp_buffer_size=13.977 GB, available_memory=35.446 GB)
result[(4, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=15.760 GB, invar_size=1.689 GB, outvar_size=0.751 GB, temp_buffer_size=13.977 GB, available_memory=35.446 GB)
result[(3, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.342, peak_memory=15.996 GB, invar_size=1.836 GB, outvar_size=0.777 GB, temp_buffer_size=14.066 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 37.84 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7]]
Result mesh_shapes: [(2, 1)]
Result logical_mesh_shapes: [(2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 94.32 s
compilation time breakdown: {'stage-construction': '71.49', 'stage-construction-dp': '1.36', 'stage-construction-compilation': '19.37', 'stage-construction-profiling': '24.28'}
 - Compile (worker): 15.87 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=983614)[0m 
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=983614)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=983614)[0m 
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=983614)[0m 
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO comm 0x353d87b0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=983614)[0m gpu2:983614:983614 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 122.36 s

[17.916043519973755, 16.858962535858154, 16.861380338668823, 16.853670358657837, 16.848833799362183, 16.852750301361084, 16.859763622283936]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 85.258 s.
 - Average e2e iteration time: 17.052000045776367 s.
 - Total local training time: 84.2760009765625 s.
 - Average local iteration time: 16.85500144958496 s.
 - Max allocated memory among devices: 16.268 GB.
 - Compilation times:  {'stage-construction': 71.49116611480713, 'stage-construction-dp': 1.3587181568145752, 'stage-construction-compilation': 19.370176792144775, 'stage-construction-profiling': 24.28093433380127}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 16.85527992248535
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_690M_1024.pkl`...
