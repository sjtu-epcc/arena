
------------------------------------------------------------------
- (1/3) Profiling bert_2.6B with batch size: 128...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_2.6B_128.pkl`, updating/rewriting it...
[WARN] Local batch size 8 is not divisible by num devices 16, skipping...
[TMP] Current profiling results of key `8_a40_8_n_2_d`: none
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_2.6B_128.pkl`...

------------------------------------------------------------------
- (2/3) Profiling bert_2.6B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_2.6B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f64d7f1b730>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.135, peak_memory=1.184 GB, invar_size=0.371 GB, outvar_size=0.156 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.153, peak_memory=1.233 GB, invar_size=0.420 GB, outvar_size=0.156 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.135, peak_memory=1.184 GB, invar_size=0.371 GB, outvar_size=0.156 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.135, peak_memory=1.184 GB, invar_size=0.371 GB, outvar_size=0.156 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.050, peak_memory=1.595 GB, invar_size=0.735 GB, outvar_size=0.078 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.135, peak_memory=1.184 GB, invar_size=0.371 GB, outvar_size=0.156 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.062, peak_memory=1.549 GB, invar_size=0.612 GB, outvar_size=0.078 GB, temp_buffer_size=0.859 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.136, peak_memory=1.266 GB, invar_size=0.368 GB, outvar_size=0.117 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.047, peak_memory=1.418 GB, invar_size=0.723 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.047, peak_memory=1.418 GB, invar_size=0.723 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.127, peak_memory=1.145 GB, invar_size=0.371 GB, outvar_size=0.117 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.047, peak_memory=1.418 GB, invar_size=0.723 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.047, peak_memory=1.418 GB, invar_size=0.723 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.153, peak_memory=1.233 GB, invar_size=0.420 GB, outvar_size=0.156 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.699 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.370 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.044, peak_memory=1.369 GB, invar_size=0.674 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.311, peak_memory=5.567 GB, invar_size=0.892 GB, outvar_size=0.368 GB, temp_buffer_size=4.675 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.154, peak_memory=5.110 GB, invar_size=1.302 GB, outvar_size=0.612 GB, temp_buffer_size=3.808 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.044, peak_memory=1.369 GB, invar_size=0.674 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.699 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.370 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.699 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.370 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.143, peak_memory=1.169 GB, invar_size=0.396 GB, outvar_size=0.117 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.311, peak_memory=5.103 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.204 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.127, peak_memory=1.145 GB, invar_size=0.371 GB, outvar_size=0.117 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.311, peak_memory=5.183 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.284 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.044, peak_memory=1.369 GB, invar_size=0.674 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.143, peak_memory=1.169 GB, invar_size=0.396 GB, outvar_size=0.117 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.044, peak_memory=1.369 GB, invar_size=0.674 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=0.311, peak_memory=5.103 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.204 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.584 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.255 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.338, peak_memory=5.357 GB, invar_size=0.918 GB, outvar_size=0.420 GB, temp_buffer_size=4.361 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.141, peak_memory=5.319 GB, invar_size=1.548 GB, outvar_size=0.735 GB, temp_buffer_size=3.771 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.127, peak_memory=1.145 GB, invar_size=0.371 GB, outvar_size=0.117 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=4.738 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.410 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.738 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.410 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.738 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.410 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.584 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.255 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.143, peak_memory=5.246 GB, invar_size=1.485 GB, outvar_size=0.723 GB, temp_buffer_size=3.722 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.140, peak_memory=5.285 GB, invar_size=1.485 GB, outvar_size=0.723 GB, temp_buffer_size=3.761 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.311, peak_memory=5.103 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.204 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.143, peak_memory=5.324 GB, invar_size=1.524 GB, outvar_size=0.723 GB, temp_buffer_size=3.761 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.140, peak_memory=5.324 GB, invar_size=1.524 GB, outvar_size=0.723 GB, temp_buffer_size=3.761 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.296, peak_memory=5.099 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.200 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.127, peak_memory=1.145 GB, invar_size=0.371 GB, outvar_size=0.117 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=0.988 GB, invar_size=0.332 GB, outvar_size=0.039 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.323, peak_memory=5.588 GB, invar_size=0.918 GB, outvar_size=0.420 GB, temp_buffer_size=4.592 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.068, peak_memory=0.959 GB, invar_size=0.225 GB, outvar_size=0.078 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.699 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.370 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=0.988 GB, invar_size=0.332 GB, outvar_size=0.039 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=4.738 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.410 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.127, peak_memory=1.145 GB, invar_size=0.371 GB, outvar_size=0.117 GB, temp_buffer_size=0.656 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.321 GB, invar_size=0.625 GB, outvar_size=0.078 GB, temp_buffer_size=0.617 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=5.914 GB, invar_size=1.387 GB, outvar_size=0.674 GB, temp_buffer_size=4.488 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.333, peak_memory=5.670 GB, invar_size=0.870 GB, outvar_size=0.396 GB, temp_buffer_size=4.722 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.285, peak_memory=5.015 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.116 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=4.659 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.330 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=5.992 GB, invar_size=1.426 GB, outvar_size=0.674 GB, temp_buffer_size=4.527 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=5.953 GB, invar_size=1.387 GB, outvar_size=0.674 GB, temp_buffer_size=4.527 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.321, peak_memory=6.305 GB, invar_size=0.870 GB, outvar_size=0.396 GB, temp_buffer_size=5.358 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=5.992 GB, invar_size=1.426 GB, outvar_size=0.674 GB, temp_buffer_size=4.527 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=4.659 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.330 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.285, peak_memory=5.015 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.116 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=4.698 GB, invar_size=1.329 GB, outvar_size=0.625 GB, temp_buffer_size=3.330 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=4.698 GB, invar_size=1.329 GB, outvar_size=0.625 GB, temp_buffer_size=3.330 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.285, peak_memory=5.015 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.116 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=4.659 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.330 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.298, peak_memory=5.024 GB, invar_size=0.821 GB, outvar_size=0.371 GB, temp_buffer_size=4.125 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=4.698 GB, invar_size=1.329 GB, outvar_size=0.625 GB, temp_buffer_size=3.330 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.126, peak_memory=4.659 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.330 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.351, peak_memory=5.139 GB, invar_size=0.950 GB, outvar_size=0.475 GB, temp_buffer_size=4.111 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.126, peak_memory=4.659 GB, invar_size=1.290 GB, outvar_size=0.625 GB, temp_buffer_size=3.330 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.166, peak_memory=5.324 GB, invar_size=1.666 GB, outvar_size=0.833 GB, temp_buffer_size=3.619 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.179, peak_memory=5.119 GB, invar_size=1.422 GB, outvar_size=0.711 GB, temp_buffer_size=3.658 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 35.02 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(7, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(3, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.052, peak_memory=1.781 GB, invar_size=0.469 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(7, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.564 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.743 GB, available_memory=35.446 GB)
result[(7, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.564 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.743 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=2.098 GB, invar_size=0.442 GB, outvar_size=0.078 GB, temp_buffer_size=1.578 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=2.098 GB, invar_size=0.442 GB, outvar_size=0.078 GB, temp_buffer_size=1.578 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.047, peak_memory=1.733 GB, invar_size=0.420 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(1, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.047, peak_memory=1.733 GB, invar_size=0.420 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(1, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(1, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.329 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.509 GB, available_memory=35.446 GB)
result[(3, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.149, peak_memory=8.462 GB, invar_size=0.938 GB, outvar_size=0.469 GB, temp_buffer_size=7.446 GB, available_memory=35.446 GB)
result[(3, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.052, peak_memory=1.781 GB, invar_size=0.469 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(1, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.329 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.509 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.564 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.743 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.329 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.509 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.113, peak_memory=7.561 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.741 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.329 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.509 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.564 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.743 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.113, peak_memory=7.561 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.741 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.564 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.743 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.144, peak_memory=8.502 GB, invar_size=0.962 GB, outvar_size=0.442 GB, temp_buffer_size=7.540 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.564 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.743 GB, available_memory=35.446 GB)
result[(13, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=0.144, peak_memory=8.502 GB, invar_size=0.962 GB, outvar_size=0.442 GB, temp_buffer_size=7.540 GB, available_memory=35.446 GB)
result[(13, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(3, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.149, peak_memory=8.462 GB, invar_size=0.938 GB, outvar_size=0.469 GB, temp_buffer_size=7.446 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.140, peak_memory=9.893 GB, invar_size=0.840 GB, outvar_size=0.420 GB, temp_buffer_size=8.975 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.402 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.582 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.140, peak_memory=9.893 GB, invar_size=0.840 GB, outvar_size=0.420 GB, temp_buffer_size=8.975 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.718 GB, invar_size=0.743 GB, outvar_size=0.371 GB, temp_buffer_size=6.897 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.718 GB, invar_size=0.743 GB, outvar_size=0.371 GB, temp_buffer_size=6.897 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.402 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.582 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.402 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.582 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.402 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.582 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.402 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.582 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.402 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.582 GB, available_memory=35.446 GB)
result[(13, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.402 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.582 GB, available_memory=35.446 GB)
result[(13, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=7.402 GB, invar_size=0.742 GB, outvar_size=0.371 GB, temp_buffer_size=6.582 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.483 GB, invar_size=0.743 GB, outvar_size=0.371 GB, temp_buffer_size=6.663 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.119, peak_memory=7.483 GB, invar_size=0.743 GB, outvar_size=0.371 GB, temp_buffer_size=6.663 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.193, peak_memory=8.396 GB, invar_size=1.080 GB, outvar_size=0.579 GB, temp_buffer_size=7.238 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.193, peak_memory=8.396 GB, invar_size=1.080 GB, outvar_size=0.579 GB, temp_buffer_size=7.238 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 19.77 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 5, 2, 0) has been pruned...
[TMP] Stage (1, 5, 2, 1) has been pruned...
[TMP] Stage (1, 5, 2, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (1, 10, 3, 0) has been pruned...
[TMP] Stage (1, 10, 3, 1) has been pruned...
[TMP] Stage (1, 10, 3, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 6, 2, 0) has been pruned...
[TMP] Stage (2, 6, 2, 1) has been pruned...
[TMP] Stage (2, 6, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (2, 11, 3, 0) has been pruned...
[TMP] Stage (2, 11, 3, 1) has been pruned...
[TMP] Stage (2, 11, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 7, 2, 0) has been pruned...
[TMP] Stage (3, 7, 2, 1) has been pruned...
[TMP] Stage (3, 7, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (3, 12, 3, 0) has been pruned...
[TMP] Stage (3, 12, 3, 1) has been pruned...
[TMP] Stage (3, 12, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 9, 2, 0) has been pruned...
[TMP] Stage (5, 9, 2, 1) has been pruned...
[TMP] Stage (5, 9, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 10, 2, 0) has been pruned...
[TMP] Stage (6, 10, 2, 1) has been pruned...
[TMP] Stage (6, 10, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {}, {}, {}, {}, {}, {}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO comm 0x3a483e0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO comm 0x3d090c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO comm 0x70a72a0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO comm 0x8e2fc50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=624545)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO comm 0x37875f0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO comm 0x95a0610 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO comm 0x5986e40 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO comm 0x9d0e640 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO comm 0x4bad460 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO comm 0x97d07f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO comm 0x6dace20 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO comm 0x97676b0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO comm 0xabf6950 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO comm 0x3603ec0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO comm 0x57be650 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO comm 0xab8d810 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO comm 0x960e430 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO comm 0x521bc10 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO comm 0x95a52f0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO comm 0xa5ce7e0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO comm 0xb493280 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO comm 0x4dc0a10 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO comm 0x6e4ac10 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO comm 0xb4fc3c0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [0] NCCL INFO comm 0x4189740 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO comm 0xabe3680 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463681 [1] NCCL INFO comm 0x752c4b0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO comm 0xae3a450 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 112.83 s
compilation time breakdown: {'stage-construction': '59.57', 'stage-construction-dp': '1.51', 'stage-construction-compilation': '11.13', 'stage-construction-profiling': '12.70'}
 - Compile (worker): 4.80 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968659 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624545 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632737 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530092 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142513 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335035 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=624545)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295150 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463871 [1] NCCL INFO comm 0x7f463a837cf0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO comm 0x7f4633a2d930 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1463681, ip=192.168.0.32)[0m gpu17:1463681:1463869 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO comm 0x7f03179a6010 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295257 [1] NCCL INFO comm 0x7f032114b670 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3295150, ip=192.168.0.31)[0m gpu16:3295150:3295255 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO comm 0x7f1096ee0fb0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335099 [1] NCCL INFO comm 0x7f10a18d4440 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2335035, ip=192.168.0.27)[0m gpu12:2335035:2335097 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142619 [1] NCCL INFO comm 0x7f65d760d060 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO comm 0x7f65dc0cc290 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2142513, ip=192.168.0.18)[0m gpu3:2142513:2142617 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530240 [1] NCCL INFO comm 0x7ef966afe900 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO comm 0x7ef971e4f390 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632855 [1] NCCL INFO comm 0x7ef116b8a310 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO comm 0x7ef110e36b10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=624545)[0m 
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO comm 0x7f55d6ed4250 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624623 [1] NCCL INFO comm 0x7f55cfaa0450 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=624545)[0m gpu2:624545:624621 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968735 [1] NCCL INFO comm 0x7fd8498efce0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO comm 0x7fd84325b8f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2530092, ip=192.168.0.38)[0m gpu23:2530092:2530238 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3632737, ip=192.168.0.39)[0m gpu24:3632737:3632853 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=968659, ip=192.168.0.26)[0m gpu11:968659:968733 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 250.46 s

[185.17684268951416, 10.051440000534058, 10.158924102783203, 10.143704175949097, 10.004234075546265, 10.010032415390015, 10.028357982635498]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 52.396 s.
 - Average e2e iteration time: 10.479000091552734 s.
 - Total local training time: 50.345001220703125 s.
 - Average local iteration time: 10.069000244140625 s.
 - Max allocated memory among devices: 9.109 GB.
 - Compilation times:  {'stage-construction': 59.566575050354004, 'stage-construction-dp': 1.514284372329712, 'stage-construction-compilation': 11.133747816085815, 'stage-construction-profiling': 12.69926118850708}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 10.069050788879395
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_2.6B_256.pkl`...

------------------------------------------------------------------
- (3/3) Profiling bert_2.6B with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_2.6B_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fc249a4c760>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.268, peak_memory=2.075 GB, invar_size=0.450 GB, outvar_size=0.312 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.268, peak_memory=2.075 GB, invar_size=0.450 GB, outvar_size=0.312 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.454 GB, invar_size=0.735 GB, outvar_size=0.156 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.454 GB, invar_size=0.735 GB, outvar_size=0.156 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.270, peak_memory=2.165 GB, invar_size=0.368 GB, outvar_size=0.234 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.153 GB, invar_size=0.762 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.251, peak_memory=1.996 GB, invar_size=0.450 GB, outvar_size=0.234 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.153 GB, invar_size=0.762 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.268, peak_memory=2.075 GB, invar_size=0.450 GB, outvar_size=0.312 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.304, peak_memory=2.123 GB, invar_size=0.498 GB, outvar_size=0.312 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.268, peak_memory=2.075 GB, invar_size=0.450 GB, outvar_size=0.312 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.153 GB, invar_size=0.762 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.304, peak_memory=2.123 GB, invar_size=0.498 GB, outvar_size=0.312 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.091, peak_memory=2.153 GB, invar_size=0.762 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.087, peak_memory=2.104 GB, invar_size=0.713 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.243, peak_memory=8.226 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.741 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.087, peak_memory=2.104 GB, invar_size=0.713 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.616, peak_memory=10.394 GB, invar_size=1.048 GB, outvar_size=0.368 GB, temp_buffer_size=9.346 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.282, peak_memory=2.021 GB, invar_size=0.474 GB, outvar_size=0.234 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.243, peak_memory=8.226 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.741 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.615, peak_memory=9.612 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.401 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.243, peak_memory=8.226 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.741 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.087, peak_memory=2.104 GB, invar_size=0.713 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.087, peak_memory=2.104 GB, invar_size=0.713 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.282, peak_memory=2.021 GB, invar_size=0.474 GB, outvar_size=0.234 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.251, peak_memory=1.996 GB, invar_size=0.450 GB, outvar_size=0.234 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.270, peak_memory=9.167 GB, invar_size=1.627 GB, outvar_size=0.735 GB, temp_buffer_size=7.540 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.251, peak_memory=1.996 GB, invar_size=0.450 GB, outvar_size=0.234 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=8.304 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.819 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=0.615, peak_memory=9.612 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.400 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.615, peak_memory=9.771 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.560 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.669, peak_memory=10.023 GB, invar_size=1.153 GB, outvar_size=0.498 GB, temp_buffer_size=8.714 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.585, peak_memory=9.606 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.395 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.270, peak_memory=9.167 GB, invar_size=1.627 GB, outvar_size=0.735 GB, temp_buffer_size=7.540 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.639, peak_memory=10.487 GB, invar_size=1.153 GB, outvar_size=0.498 GB, temp_buffer_size=9.178 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.615, peak_memory=9.612 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.401 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=8.304 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.819 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.243, peak_memory=8.304 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.819 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.243, peak_memory=8.304 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.819 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.243, peak_memory=7.994 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.509 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.243, peak_memory=7.994 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.509 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.274, peak_memory=9.124 GB, invar_size=1.602 GB, outvar_size=0.762 GB, temp_buffer_size=7.444 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.268, peak_memory=9.281 GB, invar_size=1.680 GB, outvar_size=0.762 GB, temp_buffer_size=7.522 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.274, peak_memory=9.280 GB, invar_size=1.680 GB, outvar_size=0.762 GB, temp_buffer_size=7.522 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.268, peak_memory=9.202 GB, invar_size=1.602 GB, outvar_size=0.762 GB, temp_buffer_size=7.522 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.134, peak_memory=1.772 GB, invar_size=0.303 GB, outvar_size=0.156 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=1.684 GB, invar_size=0.371 GB, outvar_size=0.078 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.251, peak_memory=1.996 GB, invar_size=0.450 GB, outvar_size=0.234 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.243, peak_memory=8.226 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.741 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=2.055 GB, invar_size=0.664 GB, outvar_size=0.156 GB, temp_buffer_size=1.234 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.251, peak_memory=1.996 GB, invar_size=0.450 GB, outvar_size=0.234 GB, temp_buffer_size=1.313 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.264, peak_memory=10.714 GB, invar_size=1.583 GB, outvar_size=0.713 GB, temp_buffer_size=9.053 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.240, peak_memory=8.145 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.660 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.659, peak_memory=10.705 GB, invar_size=1.104 GB, outvar_size=0.474 GB, temp_buffer_size=9.444 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.264, peak_memory=10.558 GB, invar_size=1.504 GB, outvar_size=0.713 GB, temp_buffer_size=8.975 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.564, peak_memory=9.440 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.229 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.240, peak_memory=8.145 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.660 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.240, peak_memory=8.223 GB, invar_size=1.485 GB, outvar_size=0.664 GB, temp_buffer_size=6.660 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.263, peak_memory=10.714 GB, invar_size=1.583 GB, outvar_size=0.713 GB, temp_buffer_size=9.053 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.634, peak_memory=11.969 GB, invar_size=1.104 GB, outvar_size=0.474 GB, temp_buffer_size=10.709 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.564, peak_memory=9.440 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.229 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.263, peak_memory=10.636 GB, invar_size=1.504 GB, outvar_size=0.713 GB, temp_buffer_size=9.053 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.240, peak_memory=8.223 GB, invar_size=1.485 GB, outvar_size=0.664 GB, temp_buffer_size=6.660 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.564, peak_memory=9.440 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.229 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.240, peak_memory=8.223 GB, invar_size=1.485 GB, outvar_size=0.664 GB, temp_buffer_size=6.660 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.589, peak_memory=9.453 GB, invar_size=1.055 GB, outvar_size=0.449 GB, temp_buffer_size=8.242 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.240, peak_memory=8.145 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.660 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.241, peak_memory=8.145 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.660 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.696, peak_memory=9.479 GB, invar_size=1.107 GB, outvar_size=0.553 GB, temp_buffer_size=8.216 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.241, peak_memory=8.145 GB, invar_size=1.407 GB, outvar_size=0.664 GB, temp_buffer_size=6.660 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.319, peak_memory=9.060 GB, invar_size=1.744 GB, outvar_size=0.872 GB, temp_buffer_size=7.238 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.347, peak_memory=8.894 GB, invar_size=1.500 GB, outvar_size=0.750 GB, temp_buffer_size=7.316 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 35.63 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(7, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(7, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(3, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.103, peak_memory=3.172 GB, invar_size=0.547 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(3, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.103, peak_memory=3.172 GB, invar_size=0.547 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=0.116, peak_memory=3.755 GB, invar_size=0.442 GB, outvar_size=0.156 GB, temp_buffer_size=3.156 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=0.116, peak_memory=3.755 GB, invar_size=0.442 GB, outvar_size=0.156 GB, temp_buffer_size=3.156 GB, available_memory=35.446 GB)
result[(1, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(1, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.540 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.485 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.071 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.016 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.540 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.485 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.094, peak_memory=3.123 GB, invar_size=0.498 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(7, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.540 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.485 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.224, peak_memory=14.537 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.481 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.094, peak_memory=3.123 GB, invar_size=0.498 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(3, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.298, peak_memory=16.142 GB, invar_size=1.094 GB, outvar_size=0.547 GB, temp_buffer_size=14.891 GB, available_memory=35.446 GB)
result[(7, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.540 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.485 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.287, peak_memory=16.119 GB, invar_size=1.041 GB, outvar_size=0.442 GB, temp_buffer_size=15.078 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.224, peak_memory=14.537 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.481 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=0.287, peak_memory=16.119 GB, invar_size=1.041 GB, outvar_size=0.442 GB, temp_buffer_size=15.078 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.071 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.016 GB, available_memory=35.446 GB)
result[(3, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.298, peak_memory=16.142 GB, invar_size=1.094 GB, outvar_size=0.547 GB, temp_buffer_size=14.891 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(1, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.071 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.016 GB, available_memory=35.446 GB)
result[(1, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.071 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.016 GB, available_memory=35.446 GB)
result[(13, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.540 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.485 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.540 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.485 GB, available_memory=35.446 GB)
result[(13, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.079, peak_memory=3.074 GB, invar_size=0.449 GB, outvar_size=0.156 GB, temp_buffer_size=2.469 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.849 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.794 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.233, peak_memory=14.218 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.163 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.849 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.794 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.278, peak_memory=19.103 GB, invar_size=0.996 GB, outvar_size=0.498 GB, temp_buffer_size=17.950 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.233, peak_memory=14.218 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.163 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.278, peak_memory=19.103 GB, invar_size=0.996 GB, outvar_size=0.498 GB, temp_buffer_size=17.950 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.233, peak_memory=14.218 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.163 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.233, peak_memory=14.218 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.163 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.233, peak_memory=14.218 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.163 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.233, peak_memory=14.218 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.163 GB, available_memory=35.446 GB)
result[(13, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.233, peak_memory=14.218 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.163 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.381 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.325 GB, available_memory=35.446 GB)
result[(13, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.233, peak_memory=14.218 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.163 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.386, peak_memory=15.790 GB, invar_size=1.158 GB, outvar_size=0.657 GB, temp_buffer_size=14.476 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=14.381 GB, invar_size=0.899 GB, outvar_size=0.449 GB, temp_buffer_size=13.325 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.386, peak_memory=15.790 GB, invar_size=1.158 GB, outvar_size=0.657 GB, temp_buffer_size=14.476 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 19.64 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 5, 2, 0) has been pruned...
[TMP] Stage (1, 5, 2, 1) has been pruned...
[TMP] Stage (1, 5, 2, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (1, 10, 3, 0) has been pruned...
[TMP] Stage (1, 10, 3, 1) has been pruned...
[TMP] Stage (1, 10, 3, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 6, 2, 0) has been pruned...
[TMP] Stage (2, 6, 2, 1) has been pruned...
[TMP] Stage (2, 6, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (2, 11, 3, 0) has been pruned...
[TMP] Stage (2, 11, 3, 1) has been pruned...
[TMP] Stage (2, 11, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 7, 2, 0) has been pruned...
[TMP] Stage (3, 7, 2, 1) has been pruned...
[TMP] Stage (3, 7, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (3, 12, 3, 0) has been pruned...
[TMP] Stage (3, 12, 3, 1) has been pruned...
[TMP] Stage (3, 12, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 9, 2, 0) has been pruned...
[TMP] Stage (5, 9, 2, 1) has been pruned...
[TMP] Stage (5, 9, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 10, 2, 0) has been pruned...
[TMP] Stage (6, 10, 2, 1) has been pruned...
[TMP] Stage (6, 10, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {}, {}, {'force_batch_dim_to_mesh_dim': 0}, {}, {}, {}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO comm 0x3690b60 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO comm 0x430ae80 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO comm 0x56beba0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO comm 0x977d890 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO comm 0x3afb440 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO comm 0x9eecb40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO comm 0x8ef8cc0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO comm 0xf37d8b0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO comm 0x9dc0440 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO comm 0x507e340 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO comm 0x9e29580 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO comm 0x727dd30 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO comm 0x433ce90 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO comm 0xb0ce0d0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO comm 0x660a3a0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO comm 0xb064f90 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO comm 0xb9f1b30 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO comm 0x4ed9160 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO comm 0x721bfa0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO comm 0xb9fca60 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO comm 0x4ab42a0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO comm 0xc80db50 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO comm 0x7e19270 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO comm 0xc547550 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [0] NCCL INFO comm 0x3be6a70 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO comm 0xae5e410 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973183 [1] NCCL INFO comm 0x6f84c20 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO comm 0xadf52d0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 112.98 s
compilation time breakdown: {'stage-construction': '59.81', 'stage-construction-dp': '1.51', 'stage-construction-compilation': '11.53', 'stage-construction-profiling': '12.58'}
 - Compile (worker): 4.81 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469419 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640524 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2532854 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297508 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148420 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341051 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638603 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=640524)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO comm 0x7f360cf836a0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973289 [1] NCCL INFO comm 0x7f3607d5dd10 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=973183, ip=192.168.0.26)[0m gpu11:973183:973287 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638709 [1] NCCL INFO comm 0x7f3f97db6220 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO comm 0x7f3f8aca8560 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3638603, ip=192.168.0.39)[0m gpu24:3638603:3638707 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO comm 0x7f04ef26eb50 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341199 [1] NCCL INFO comm 0x7f04d820e690 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2341051, ip=192.168.0.27)[0m gpu12:2341051:2341197 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148496 [1] NCCL INFO comm 0x7f59126267b0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO comm 0x7f590b04f770 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2148420, ip=192.168.0.18)[0m gpu3:2148420:2148494 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO comm 0x7f758d5b7770 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297572 [1] NCCL INFO comm 0x7f75979e1cf0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO comm 0x7f8b48263340 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533002 [1] NCCL INFO comm 0x7f8b53622250 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=640524)[0m 
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640661 [1] NCCL INFO comm 0x7f118ba67ae0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO comm 0x7f1197ed9140 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=640524)[0m gpu2:640524:640659 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO comm 0x7f5aa2e1caa0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469525 [1] NCCL INFO comm 0x7f5a9abe1050 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3297508, ip=192.168.0.31)[0m gpu16:3297508:3297570 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2532854, ip=192.168.0.38)[0m gpu23:2532854:2533000 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1469419, ip=192.168.0.32)[0m gpu17:1469419:1469523 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 171.05 s

[40.41317534446716, 20.679534673690796, 20.840521574020386, 20.595980167388916, 20.766303539276123, 20.662423610687256, 20.68443727493286]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 107.106 s.
 - Average e2e iteration time: 21.421001434326172 s.
 - Total local training time: 103.55000305175781 s.
 - Average local iteration time: 20.71000099182129 s.
 - Max allocated memory among devices: 15.042 GB.
 - Compilation times:  {'stage-construction': 59.812971115112305, 'stage-construction-dp': 1.507746934890747, 'stage-construction-compilation': 11.533031702041626, 'stage-construction-profiling': 12.576003789901733}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 20.70993423461914
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_2.6B_512.pkl`...
