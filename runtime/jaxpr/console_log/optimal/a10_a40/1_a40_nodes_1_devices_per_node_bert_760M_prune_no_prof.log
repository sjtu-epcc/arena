
------------------------------------------------------------------
- (1/3) Profiling bert_760M with batch size: 128...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal_prune_no_prof/bert_760M_128.pkl`, updating/rewriting it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f82937ced30>
    dtype = float16
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
[TMP] The range of stage num is (min #stage -> max #stage): 1 -> 1
[I] Coarsening pipeline layers from 16 to 4...
[I] Coarsened scheme: [[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1),)
[TMP] The range of device num per stage is (min #gpu -> max #gpu): 1 -> 1
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.240, peak_memory=2.694 GB, invar_size=1.397 GB, outvar_size=0.516 GB, temp_buffer_size=0.781 GB, available_memory=35.242 GB)
result[(0, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.702, peak_memory=7.000 GB, invar_size=3.647 GB, outvar_size=1.565 GB, temp_buffer_size=3.354 GB, available_memory=35.242 GB)
result[(0, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.229, peak_memory=2.694 GB, invar_size=1.397 GB, outvar_size=0.516 GB, temp_buffer_size=0.781 GB, available_memory=35.242 GB)
result[(0, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.690, peak_memory=7.000 GB, invar_size=3.647 GB, outvar_size=1.565 GB, temp_buffer_size=3.354 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 58.70 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 1)]
Result logical_mesh_shapes: [(1, 1)]
Result autosharding_option_dicts: [{}]
 - Compile (driver): 96.89 s
compilation time breakdown: {'stage-construction': '64.57', 'stage-construction-dp': '1.25', 'stage-construction-compilation': '3.81', 'stage-construction-profiling': '47.36'}
 - Compile (worker): 18.07 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 104.55 s

[15.242953538894653, 13.868424654006958, 13.850015878677368, 13.879758596420288, 13.830211877822876, 13.832205057144165, 13.847820043563843]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 71.819 s.
 - Average e2e iteration time: 14.36400032043457 s.
 - Total local training time: 69.24000549316406 s.
 - Average local iteration time: 13.848000526428223 s.
 - Max allocated memory among devices: 24.129 GB.
 - Compilation times:  {'stage-construction': 64.57330203056335, 'stage-construction-dp': 1.2529730796813965, 'stage-construction-compilation': 3.809187650680542, 'stage-construction-profiling': 47.355419397354126}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `1_a40_1_n_1_d`: 13.848001480102539
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal_prune_no_prof/bert_760M_128.pkl`...

------------------------------------------------------------------
- (2/3) Profiling bert_760M with batch size: 256...
------------------------------------------------------------------
