
------------------------------------------------------------------
- (1/3) Profiling moe_2.4B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_2.4B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f0cceed83a0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.094 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.012, peak_memory=0.860 GB, invar_size=0.313 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.110 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.011, peak_memory=0.891 GB, invar_size=0.328 GB, outvar_size=0.031 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.142 GB, invar_size=0.641 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.157 GB, invar_size=0.641 GB, outvar_size=0.313 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.094 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.110 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.012, peak_memory=0.860 GB, invar_size=0.313 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=2.206 GB, invar_size=0.641 GB, outvar_size=0.305 GB, temp_buffer_size=1.534 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.110 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.012, peak_memory=1.251 GB, invar_size=0.594 GB, outvar_size=0.156 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.011, peak_memory=0.891 GB, invar_size=0.328 GB, outvar_size=0.031 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.094 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.012, peak_memory=0.860 GB, invar_size=0.313 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.028, peak_memory=0.985 GB, invar_size=0.297 GB, outvar_size=0.172 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.015, peak_memory=1.001 GB, invar_size=0.344 GB, outvar_size=0.156 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.011, peak_memory=1.243 GB, invar_size=0.711 GB, outvar_size=0.031 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.015, peak_memory=0.993 GB, invar_size=0.461 GB, outvar_size=0.031 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.037, peak_memory=0.989 GB, invar_size=0.442 GB, outvar_size=0.047 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.642 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.011, peak_memory=0.891 GB, invar_size=0.328 GB, outvar_size=0.031 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.033, peak_memory=2.173 GB, invar_size=0.672 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.626 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.470 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.039, peak_memory=2.424 GB, invar_size=0.845 GB, outvar_size=0.344 GB, temp_buffer_size=1.580 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=2.173 GB, invar_size=0.641 GB, outvar_size=0.289 GB, temp_buffer_size=1.517 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=2.173 GB, invar_size=0.641 GB, outvar_size=0.289 GB, temp_buffer_size=1.517 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.066, peak_memory=2.472 GB, invar_size=0.782 GB, outvar_size=0.297 GB, temp_buffer_size=1.690 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.142 GB, invar_size=0.641 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=2.206 GB, invar_size=0.641 GB, outvar_size=0.305 GB, temp_buffer_size=1.534 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.038, peak_memory=2.893 GB, invar_size=1.345 GB, outvar_size=0.594 GB, temp_buffer_size=1.548 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.626 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.470 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.110 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.642 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.626 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.470 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=2.238 GB, invar_size=0.641 GB, outvar_size=0.305 GB, temp_buffer_size=1.566 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.657 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.094 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.077, peak_memory=2.605 GB, invar_size=0.789 GB, outvar_size=0.441 GB, temp_buffer_size=1.660 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.033, peak_memory=2.173 GB, invar_size=0.672 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.012, peak_memory=0.860 GB, invar_size=0.313 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.110 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.011, peak_memory=0.891 GB, invar_size=0.328 GB, outvar_size=0.031 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.038, peak_memory=2.955 GB, invar_size=1.313 GB, outvar_size=0.711 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.012, peak_memory=0.860 GB, invar_size=0.313 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.094 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.500 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=2.173 GB, invar_size=0.641 GB, outvar_size=0.289 GB, temp_buffer_size=1.517 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.038, peak_memory=2.502 GB, invar_size=0.828 GB, outvar_size=0.461 GB, temp_buffer_size=1.533 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.642 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=2.206 GB, invar_size=0.641 GB, outvar_size=0.305 GB, temp_buffer_size=1.534 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.005, peak_memory=1.047 GB, invar_size=0.516 GB, outvar_size=0.016 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.033, peak_memory=2.173 GB, invar_size=0.672 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.009, peak_memory=1.110 GB, invar_size=0.563 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.007, peak_memory=0.829 GB, invar_size=0.281 GB, outvar_size=0.016 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=2.173 GB, invar_size=0.641 GB, outvar_size=0.289 GB, temp_buffer_size=1.517 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.011, peak_memory=0.891 GB, invar_size=0.328 GB, outvar_size=0.031 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=2.206 GB, invar_size=0.641 GB, outvar_size=0.305 GB, temp_buffer_size=1.534 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.007, peak_memory=0.829 GB, invar_size=0.281 GB, outvar_size=0.016 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.012, peak_memory=0.860 GB, invar_size=0.313 GB, outvar_size=0.031 GB, temp_buffer_size=0.516 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.142 GB, invar_size=0.641 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=0.883 GB, invar_size=0.305 GB, outvar_size=0.047 GB, temp_buffer_size=0.532 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.642 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.033, peak_memory=2.173 GB, invar_size=0.672 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.626 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.470 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.626 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.470 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.142 GB, invar_size=0.641 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=2.173 GB, invar_size=0.641 GB, outvar_size=0.289 GB, temp_buffer_size=1.517 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=2.206 GB, invar_size=0.641 GB, outvar_size=0.305 GB, temp_buffer_size=1.534 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.042, peak_memory=3.084 GB, invar_size=1.232 GB, outvar_size=0.616 GB, temp_buffer_size=1.837 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.142 GB, invar_size=0.641 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=2.642 GB, invar_size=1.141 GB, outvar_size=0.563 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.033, peak_memory=2.173 GB, invar_size=0.672 GB, outvar_size=0.313 GB, temp_buffer_size=1.485 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.047, peak_memory=2.554 GB, invar_size=0.686 GB, outvar_size=0.335 GB, temp_buffer_size=1.853 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.066, peak_memory=2.610 GB, invar_size=0.647 GB, outvar_size=0.316 GB, temp_buffer_size=1.947 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 27.12 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.165 GB, invar_size=0.071 GB, outvar_size=0.031 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.165 GB, invar_size=0.071 GB, outvar_size=0.031 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.021, peak_memory=2.495 GB, invar_size=0.173 GB, outvar_size=0.071 GB, temp_buffer_size=2.322 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=0.021, peak_memory=2.495 GB, invar_size=0.173 GB, outvar_size=0.071 GB, temp_buffer_size=2.322 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.040, peak_memory=3.937 GB, invar_size=0.232 GB, outvar_size=0.131 GB, temp_buffer_size=3.674 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=3.033 GB, invar_size=1.063 GB, outvar_size=0.531 GB, temp_buffer_size=1.939 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.040, peak_memory=3.937 GB, invar_size=0.232 GB, outvar_size=0.131 GB, temp_buffer_size=3.674 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 11.09 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 2, 2, 0) has been pruned...
[TMP] Stage (0, 2, 2, 1) has been pruned...
[TMP] Stage (0, 2, 2, 2) has been pruned...
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (2, 11, 3, 0) has been pruned...
[TMP] Stage (2, 11, 3, 1) has been pruned...
[TMP] Stage (2, 11, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 7, 2, 0) has been pruned...
[TMP] Stage (3, 7, 2, 1) has been pruned...
[TMP] Stage (3, 7, 2, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (3, 12, 3, 0) has been pruned...
[TMP] Stage (3, 12, 3, 1) has been pruned...
[TMP] Stage (3, 12, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 9, 2, 0) has been pruned...
[TMP] Stage (5, 9, 2, 1) has been pruned...
[TMP] Stage (5, 9, 2, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 10, 2, 0) has been pruned...
[TMP] Stage (6, 10, 2, 1) has been pruned...
[TMP] Stage (6, 10, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO comm 0x35faa00 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO comm 0x42c92b0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO comm 0x5804c60 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO comm 0x6310ed0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO comm 0x50ebf70 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO comm 0xa3c30e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO comm 0x713a190 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO comm 0xa7203b0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO comm 0x3d5e0b0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO comm 0xaec9dd0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO comm 0x3d50b80 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO comm 0xb127180 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=655725)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO comm 0x98e65e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO comm 0x42bd5a0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO comm 0xa060810 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO comm 0x6304af0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO comm 0xa3b62f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO comm 0x41b6910 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO comm 0xa712b70 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO comm 0x64be530 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO comm 0x5015c90 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO comm 0xbab6f20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO comm 0xb7f12b0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO comm 0x7328000 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO comm 0xc714600 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [0] NCCL INFO comm 0x3c39e00 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO comm 0xc5bf790 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153427 [1] NCCL INFO comm 0x5e3f9d0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 82.50 s
compilation time breakdown: {'stage-construction': '41.92', 'stage-construction-dp': '1.44', 'stage-construction-compilation': '9.25', 'stage-construction-profiling': '11.13'}
 - Compile (worker): 6.46 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346556 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299220 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534657 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655725 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643656 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976323 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474239 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO comm 0x7f7226cac780 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153534 [1] NCCL INFO comm 0x7f721fa78930 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2153427, ip=192.168.0.18)[0m gpu3:2153427:2153532 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO comm 0x7f56bb4807e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474303 [1] NCCL INFO comm 0x7f56ce41df80 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976471 [1] NCCL INFO comm 0x7f2c144aedd0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO comm 0x7f2c0dd326c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643763 [1] NCCL INFO comm 0x7f8c834b6910 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO comm 0x7f780f396080 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=655725)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=655725)[0m 
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO comm 0x7f1b2b575710 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655804 [1] NCCL INFO comm 0x7f06719b1550 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO comm 0x7fb207d9eca0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534775 [1] NCCL INFO comm 0x7fb1e8d443b0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO comm 0x7fa1be7e9500 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299296 [1] NCCL INFO comm 0x7fa1c5a460c0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346761 [1] NCCL INFO comm 0x7fb7c1d10d10 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO comm 0x7fcbef3a4210 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1474239, ip=192.168.0.32)[0m gpu17:1474239:1474301 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=976323, ip=192.168.0.26)[0m gpu11:976323:976469 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3643656, ip=192.168.0.39)[0m gpu24:3643656:3643761 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=655725)[0m gpu2:655725:655802 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2534657, ip=192.168.0.38)[0m gpu23:2534657:2534773 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3299220, ip=192.168.0.31)[0m gpu16:3299220:3299294 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2346556, ip=192.168.0.27)[0m gpu12:2346556:2346759 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 47.89 s

[16.254827976226807, 4.60939884185791, 4.664917230606079, 4.538685083389282, 4.62719464302063, 4.633107423782349, 4.807153224945068]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 24.474 s.
 - Average e2e iteration time: 4.895000457763672 s.
 - Total local training time: 23.2710018157959 s.
 - Average local iteration time: 4.654000282287598 s.
 - Max allocated memory among devices: 5.886 GB.
 - Compilation times:  {'stage-construction': 41.91861701011658, 'stage-construction-dp': 1.4393548965454102, 'stage-construction-compilation': 9.24605131149292, 'stage-construction-profiling': 11.12867259979248}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 4.654211521148682
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_2.4B_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling moe_2.4B with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_2.4B_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f4b82d15760>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=1.423 GB, invar_size=0.328 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.485 GB, invar_size=0.359 GB, outvar_size=0.062 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=1.693 GB, invar_size=0.598 GB, outvar_size=0.094 GB, temp_buffer_size=1.001 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.673 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.673 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=1.907 GB, invar_size=0.594 GB, outvar_size=0.312 GB, temp_buffer_size=1.000 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.485 GB, invar_size=0.359 GB, outvar_size=0.062 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.641 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.000 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=1.423 GB, invar_size=0.328 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=1.423 GB, invar_size=0.328 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.915 GB, invar_size=0.852 GB, outvar_size=0.062 GB, temp_buffer_size=1.000 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.641 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.000 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.054, peak_memory=1.674 GB, invar_size=0.298 GB, outvar_size=0.344 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.673 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.689 GB, invar_size=0.688 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.027, peak_memory=1.665 GB, invar_size=0.602 GB, outvar_size=0.062 GB, temp_buffer_size=1.001 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.029, peak_memory=1.657 GB, invar_size=0.344 GB, outvar_size=0.312 GB, temp_buffer_size=1.000 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.149, peak_memory=4.639 GB, invar_size=1.008 GB, outvar_size=0.598 GB, temp_buffer_size=3.318 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=3.864 GB, invar_size=0.735 GB, outvar_size=0.336 GB, temp_buffer_size=3.067 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.189 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.189 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.641 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.000 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.689 GB, invar_size=0.688 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=3.752 GB, invar_size=0.750 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=3.799 GB, invar_size=0.735 GB, outvar_size=0.305 GB, temp_buffer_size=3.033 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=3.864 GB, invar_size=0.735 GB, outvar_size=0.336 GB, temp_buffer_size=3.067 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.158 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.939 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.126, peak_memory=4.349 GB, invar_size=0.970 GB, outvar_size=0.297 GB, temp_buffer_size=3.379 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.673 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.073, peak_memory=4.160 GB, invar_size=1.001 GB, outvar_size=0.344 GB, temp_buffer_size=3.159 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.221 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.158 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.939 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.066, peak_memory=4.598 GB, invar_size=1.501 GB, outvar_size=0.594 GB, temp_buffer_size=3.097 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=3.927 GB, invar_size=0.735 GB, outvar_size=0.336 GB, temp_buffer_size=3.130 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=3.799 GB, invar_size=0.735 GB, outvar_size=0.305 GB, temp_buffer_size=3.033 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.721 GB, invar_size=0.688 GB, outvar_size=0.328 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.485 GB, invar_size=0.359 GB, outvar_size=0.062 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.066, peak_memory=4.769 GB, invar_size=1.485 GB, outvar_size=0.852 GB, temp_buffer_size=3.003 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=3.752 GB, invar_size=0.750 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.641 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.000 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=1.423 GB, invar_size=0.328 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.485 GB, invar_size=0.359 GB, outvar_size=0.062 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.673 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=1.423 GB, invar_size=0.328 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.158 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.939 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.070, peak_memory=4.363 GB, invar_size=1.016 GB, outvar_size=0.602 GB, temp_buffer_size=3.066 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.641 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.000 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=3.864 GB, invar_size=0.735 GB, outvar_size=0.336 GB, temp_buffer_size=3.067 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=3.799 GB, invar_size=0.735 GB, outvar_size=0.305 GB, temp_buffer_size=3.033 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.017, peak_memory=1.673 GB, invar_size=0.578 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=1.594 GB, invar_size=0.531 GB, outvar_size=0.031 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.158 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.939 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.189 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.689 GB, invar_size=0.688 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=3.799 GB, invar_size=0.735 GB, outvar_size=0.305 GB, temp_buffer_size=3.033 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=3.752 GB, invar_size=0.750 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.014, peak_memory=1.407 GB, invar_size=0.313 GB, outvar_size=0.031 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=1.485 GB, invar_size=0.359 GB, outvar_size=0.062 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.189 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=3.864 GB, invar_size=0.735 GB, outvar_size=0.336 GB, temp_buffer_size=3.067 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=1.423 GB, invar_size=0.328 GB, outvar_size=0.062 GB, temp_buffer_size=1.032 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.013, peak_memory=1.407 GB, invar_size=0.313 GB, outvar_size=0.031 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.493 GB, invar_size=0.336 GB, outvar_size=0.094 GB, temp_buffer_size=1.063 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=3.752 GB, invar_size=0.750 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.689 GB, invar_size=0.688 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.075, peak_memory=4.968 GB, invar_size=1.263 GB, outvar_size=0.631 GB, temp_buffer_size=3.674 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=3.799 GB, invar_size=0.735 GB, outvar_size=0.305 GB, temp_buffer_size=3.033 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.158 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.939 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=4.189 GB, invar_size=1.188 GB, outvar_size=0.578 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.117, peak_memory=3.864 GB, invar_size=0.735 GB, outvar_size=0.336 GB, temp_buffer_size=3.067 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=3.752 GB, invar_size=0.750 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.063, peak_memory=3.689 GB, invar_size=0.688 GB, outvar_size=0.328 GB, temp_buffer_size=2.970 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.129, peak_memory=4.619 GB, invar_size=0.694 GB, outvar_size=0.331 GB, temp_buffer_size=3.894 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.079, peak_memory=4.499 GB, invar_size=0.794 GB, outvar_size=0.381 GB, temp_buffer_size=3.674 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 27.19 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=0.019, peak_memory=2.260 GB, invar_size=0.071 GB, outvar_size=0.062 GB, temp_buffer_size=2.126 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=0.019, peak_memory=2.260 GB, invar_size=0.071 GB, outvar_size=0.062 GB, temp_buffer_size=2.126 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=0.041, peak_memory=4.849 GB, invar_size=0.204 GB, outvar_size=0.071 GB, temp_buffer_size=4.644 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.041, peak_memory=4.849 GB, invar_size=0.204 GB, outvar_size=0.071 GB, temp_buffer_size=4.644 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.079, peak_memory=7.673 GB, invar_size=0.263 GB, outvar_size=0.163 GB, temp_buffer_size=7.347 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.079, peak_memory=7.673 GB, invar_size=0.263 GB, outvar_size=0.163 GB, temp_buffer_size=7.347 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.057, peak_memory=5.066 GB, invar_size=1.125 GB, outvar_size=0.563 GB, temp_buffer_size=3.878 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 11.27 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 2, 2, 0) has been pruned...
[TMP] Stage (0, 2, 2, 1) has been pruned...
[TMP] Stage (0, 2, 2, 2) has been pruned...
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (2, 11, 3, 0) has been pruned...
[TMP] Stage (2, 11, 3, 1) has been pruned...
[TMP] Stage (2, 11, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 7, 2, 0) has been pruned...
[TMP] Stage (3, 7, 2, 1) has been pruned...
[TMP] Stage (3, 7, 2, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (3, 12, 3, 0) has been pruned...
[TMP] Stage (3, 12, 3, 1) has been pruned...
[TMP] Stage (3, 12, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 9, 2, 0) has been pruned...
[TMP] Stage (5, 9, 2, 1) has been pruned...
[TMP] Stage (5, 9, 2, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 10, 2, 0) has been pruned...
[TMP] Stage (6, 10, 2, 1) has been pruned...
[TMP] Stage (6, 10, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO comm 0x3a189f0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO comm 0x3ad43e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO comm 0x5b21350 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO comm 0x5a60c50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO comm 0x9b12260 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO comm 0x3950260 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO comm 0x9e6f590 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO comm 0x5c619c0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO comm 0x5185360 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO comm 0xb04e280 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO comm 0xaef9360 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO comm 0x74ae6e0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO comm 0xc87c790 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO comm 0x45afc40 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO comm 0xc8876c0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO comm 0x65f7710 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO comm 0xa3848e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO comm 0x4a02c50 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO comm 0xa5e1c90 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO comm 0x6c07c10 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO comm 0x3978900 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO comm 0xaa55fa0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO comm 0x6af1450 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO comm 0xa9ece60 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=665135)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO comm 0x98358f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [0] NCCL INFO comm 0x4f6c0b0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO comm 0x9844d80 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2157948 [1] NCCL INFO comm 0x716b910 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 81.58 s
compilation time breakdown: {'stage-construction': '42.19', 'stage-construction-dp': '1.46', 'stage-construction-compilation': '9.34', 'stage-construction-profiling': '11.33'}
 - Compile (worker): 7.15 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478815 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648349 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979282 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536044 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300571 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351574 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665135 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO comm 0x7f72c896e880 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158096 [1] NCCL INFO comm 0x7f72c02bc1f0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2157948, ip=192.168.0.18)[0m gpu3:2157948:2158094 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=665135)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=665135)[0m 
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665300 [1] NCCL INFO comm 0x7ef6e19b1550 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO comm 0x7ef6f75f4c10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO comm 0x7f9a2bf43330 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351638 [1] NCCL INFO comm 0x7f9a2f7420c0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO comm 0x7ef6f10bf540 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300677 [1] NCCL INFO comm 0x7ef6ed9ff060 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO comm 0x7f64d77ac650 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536108 [1] NCCL INFO comm 0x7f64db2d6700 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO comm 0x7fc6f2fc9430 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979347 [1] NCCL INFO comm 0x7fc6dd3973c0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO comm 0x7f98bcd051b0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648539 [1] NCCL INFO comm 0x7f98ce8c9280 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO comm 0x7f6f6c275720 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478879 [1] NCCL INFO comm 0x7f6f5cf49b50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=665135)[0m gpu2:665135:665298 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2351574, ip=192.168.0.27)[0m gpu12:2351574:2351636 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3300571, ip=192.168.0.31)[0m gpu16:3300571:3300675 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2536044, ip=192.168.0.38)[0m gpu23:2536044:2536106 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=979282, ip=192.168.0.26)[0m gpu11:979282:979345 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3648349, ip=192.168.0.39)[0m gpu24:3648349:3648537 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1478815, ip=192.168.0.32)[0m gpu17:1478815:1478877 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 83.83 s

[20.72373104095459, 9.677846908569336, 9.910858154296875, 9.96170949935913, 10.117939472198486, 9.800460577011108, 10.117374420166016]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 51.737 s.
 - Average e2e iteration time: 10.347000122070312 s.
 - Total local training time: 49.90800094604492 s.
 - Average local iteration time: 9.982000350952148 s.
 - Max allocated memory among devices: 10.06 GB.
 - Compilation times:  {'stage-construction': 42.186485052108765, 'stage-construction-dp': 1.4592750072479248, 'stage-construction-compilation': 9.344588041305542, 'stage-construction-profiling': 11.328946113586426}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 9.981668472290039
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_2.4B_512.pkl`...

------------------------------------------------------------------
- (3/3) Profiling moe_2.4B with batch size: 1024...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_2.4B_1024.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f106ef268b0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=2.674 GB, invar_size=0.422 GB, outvar_size=0.125 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.141, peak_memory=3.100 GB, invar_size=0.910 GB, outvar_size=0.188 GB, temp_buffer_size=2.002 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=2.548 GB, invar_size=0.360 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.736 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.097, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.107, peak_memory=3.050 GB, invar_size=0.298 GB, outvar_size=0.688 GB, temp_buffer_size=2.064 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.798 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=3.259 GB, invar_size=1.133 GB, outvar_size=0.125 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=2.548 GB, invar_size=0.360 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=3.221 GB, invar_size=0.595 GB, outvar_size=0.625 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=2.674 GB, invar_size=0.422 GB, outvar_size=0.125 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.053, peak_memory=3.010 GB, invar_size=0.883 GB, outvar_size=0.125 GB, temp_buffer_size=2.002 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=2.971 GB, invar_size=0.345 GB, outvar_size=0.625 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.097, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.798 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.798 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.118, peak_memory=6.910 GB, invar_size=0.906 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.097, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.736 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=2.548 GB, invar_size=0.360 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.736 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=6.785 GB, invar_size=0.782 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.144, peak_memory=7.632 GB, invar_size=1.314 GB, outvar_size=0.344 GB, temp_buffer_size=6.318 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.230, peak_memory=7.180 GB, invar_size=0.922 GB, outvar_size=0.398 GB, temp_buffer_size=6.133 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.212, peak_memory=7.051 GB, invar_size=0.922 GB, outvar_size=0.336 GB, temp_buffer_size=6.066 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.294, peak_memory=8.706 GB, invar_size=1.446 GB, outvar_size=0.910 GB, temp_buffer_size=6.636 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.230, peak_memory=7.180 GB, invar_size=0.922 GB, outvar_size=0.398 GB, temp_buffer_size=6.133 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=6.785 GB, invar_size=0.782 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.249, peak_memory=8.103 GB, invar_size=1.345 GB, outvar_size=0.297 GB, temp_buffer_size=6.758 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.222 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.878 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.285 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.285 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.348 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=6.004 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=8.007 GB, invar_size=1.814 GB, outvar_size=0.594 GB, temp_buffer_size=6.193 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.118, peak_memory=6.910 GB, invar_size=0.906 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.798 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=2.674 GB, invar_size=0.422 GB, outvar_size=0.125 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.212, peak_memory=7.051 GB, invar_size=0.922 GB, outvar_size=0.336 GB, temp_buffer_size=6.066 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=8.396 GB, invar_size=1.828 GB, outvar_size=1.133 GB, temp_buffer_size=6.006 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.222 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.878 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=2.548 GB, invar_size=0.360 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.230, peak_memory=7.306 GB, invar_size=0.922 GB, outvar_size=0.398 GB, temp_buffer_size=6.258 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.136, peak_memory=8.086 GB, invar_size=1.391 GB, outvar_size=0.883 GB, temp_buffer_size=6.133 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.736 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=2.674 GB, invar_size=0.422 GB, outvar_size=0.125 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.097, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=6.848 GB, invar_size=0.782 GB, outvar_size=0.359 GB, temp_buffer_size=6.004 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.222 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.878 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.798 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.097, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=2.548 GB, invar_size=0.360 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.736 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.001 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.230, peak_memory=7.180 GB, invar_size=0.922 GB, outvar_size=0.398 GB, temp_buffer_size=6.133 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.212, peak_memory=7.051 GB, invar_size=0.922 GB, outvar_size=0.336 GB, temp_buffer_size=6.066 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.222 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.878 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.285 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=2.689 GB, invar_size=0.563 GB, outvar_size=0.062 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=6.785 GB, invar_size=0.782 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.034, peak_memory=2.798 GB, invar_size=0.610 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.040, peak_memory=2.674 GB, invar_size=0.422 GB, outvar_size=0.125 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.212, peak_memory=7.051 GB, invar_size=0.922 GB, outvar_size=0.336 GB, temp_buffer_size=6.066 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.097, peak_memory=2.713 GB, invar_size=0.399 GB, outvar_size=0.188 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.118, peak_memory=6.910 GB, invar_size=0.906 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.027, peak_memory=2.564 GB, invar_size=0.375 GB, outvar_size=0.062 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=2.564 GB, invar_size=0.375 GB, outvar_size=0.062 GB, temp_buffer_size=2.127 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=2.548 GB, invar_size=0.360 GB, outvar_size=0.125 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=6.785 GB, invar_size=0.782 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.230, peak_memory=7.180 GB, invar_size=0.922 GB, outvar_size=0.398 GB, temp_buffer_size=6.133 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.118, peak_memory=6.910 GB, invar_size=0.906 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.285 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.222 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.878 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.212, peak_memory=7.051 GB, invar_size=0.922 GB, outvar_size=0.336 GB, temp_buffer_size=6.066 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.230, peak_memory=7.180 GB, invar_size=0.922 GB, outvar_size=0.398 GB, temp_buffer_size=6.133 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.104, peak_memory=7.285 GB, invar_size=1.282 GB, outvar_size=0.609 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.142, peak_memory=8.736 GB, invar_size=1.326 GB, outvar_size=0.663 GB, temp_buffer_size=7.347 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.118, peak_memory=6.910 GB, invar_size=0.906 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=6.785 GB, invar_size=0.782 GB, outvar_size=0.359 GB, temp_buffer_size=5.941 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.255, peak_memory=8.638 GB, invar_size=0.788 GB, outvar_size=0.363 GB, temp_buffer_size=7.787 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.155, peak_memory=8.298 GB, invar_size=0.888 GB, outvar_size=0.413 GB, temp_buffer_size=7.347 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 26.61 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=0.038, peak_memory=4.449 GB, invar_size=0.072 GB, outvar_size=0.125 GB, temp_buffer_size=4.252 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=0.038, peak_memory=4.449 GB, invar_size=0.072 GB, outvar_size=0.125 GB, temp_buffer_size=4.252 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=0.082, peak_memory=9.555 GB, invar_size=0.267 GB, outvar_size=0.071 GB, temp_buffer_size=9.288 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.082, peak_memory=9.555 GB, invar_size=0.267 GB, outvar_size=0.071 GB, temp_buffer_size=9.288 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.158, peak_memory=15.146 GB, invar_size=0.326 GB, outvar_size=0.225 GB, temp_buffer_size=14.695 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.039, peak_memory=4.877 GB, invar_size=0.625 GB, outvar_size=0.125 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.158, peak_memory=15.146 GB, invar_size=0.326 GB, outvar_size=0.225 GB, temp_buffer_size=14.695 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.114, peak_memory=9.132 GB, invar_size=1.250 GB, outvar_size=0.625 GB, temp_buffer_size=7.757 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 11.30 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 2, 2, 0) has been pruned...
[TMP] Stage (0, 2, 2, 1) has been pruned...
[TMP] Stage (0, 2, 2, 2) has been pruned...
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (2, 11, 3, 0) has been pruned...
[TMP] Stage (2, 11, 3, 1) has been pruned...
[TMP] Stage (2, 11, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 7, 2, 0) has been pruned...
[TMP] Stage (3, 7, 2, 1) has been pruned...
[TMP] Stage (3, 7, 2, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (3, 12, 3, 0) has been pruned...
[TMP] Stage (3, 12, 3, 1) has been pruned...
[TMP] Stage (3, 12, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 9, 2, 0) has been pruned...
[TMP] Stage (5, 9, 2, 1) has been pruned...
[TMP] Stage (5, 9, 2, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 10, 2, 0) has been pruned...
[TMP] Stage (6, 10, 2, 1) has been pruned...
[TMP] Stage (6, 10, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=674728)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO comm 0x3a8fd10 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO comm 0x42c0fc0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO comm 0x5dbcf00 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO comm 0x6e65b30 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO comm 0xb18dea0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO comm 0x3a27950 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO comm 0xa0b6cc0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO comm 0x5a76b80 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO comm 0x9d73ee0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO comm 0x4906390 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO comm 0x9d7acb0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO comm 0x694dca0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO comm 0xa6e0360 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO comm 0x5476ee0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO comm 0xa93d710 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO comm 0x74bea30 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO comm 0xb252550 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO comm 0x5396f10 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO comm 0xb4af900 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO comm 0x7248c60 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO comm 0xb48bc40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO comm 0x3abe5b0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO comm 0xb7e84c0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO comm 0x8ec0b50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO comm 0x9d85620 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [0] NCCL INFO comm 0x409cb00 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO comm 0x9dee760 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:981938 [1] NCCL INFO comm 0x60e58a0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 81.14 s
compilation time breakdown: {'stage-construction': '41.48', 'stage-construction-dp': '1.44', 'stage-construction-compilation': '9.25', 'stage-construction-profiling': '11.38'}
 - Compile (worker): 6.60 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674728 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537482 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162516 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2356975 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302029 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653144 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483323 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO comm 0x7f9cc733cf10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982003 [1] NCCL INFO comm 0x7f9cca4fb9d0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=981938, ip=192.168.0.26)[0m gpu11:981938:982001 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483429 [1] NCCL INFO comm 0x7ef06a8ba8e0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO comm 0x7ef0657d83f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653251 [1] NCCL INFO comm 0x7f51a6bf2b70 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO comm 0x7f51b149b8a0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO comm 0x7f306a63fd00 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302177 [1] NCCL INFO comm 0x7f30611ffea0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO comm 0x7fdc515f2b10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357081 [1] NCCL INFO comm 0x7fdc4bbc7530 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162634 [1] NCCL INFO comm 0x7fd9f5844f30 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO comm 0x7fd9edbe30a0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO comm 0x7f24ac0d0a50 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537588 [1] NCCL INFO comm 0x7f24b2cbd940 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=674728)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=674728)[0m 
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO comm 0x7f5437b03a90 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674851 [1] NCCL INFO comm 0x7f5447e93690 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1483323, ip=192.168.0.32)[0m gpu17:1483323:1483427 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3653144, ip=192.168.0.39)[0m gpu24:3653144:3653249 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3302029, ip=192.168.0.31)[0m gpu16:3302029:3302175 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2356975, ip=192.168.0.27)[0m gpu12:2356975:2357079 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2162516, ip=192.168.0.18)[0m gpu3:2162516:2162632 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2537482, ip=192.168.0.38)[0m gpu23:2537482:2537586 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=674728)[0m gpu2:674728:674849 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 135.22 s

[27.149001121520996, 16.911813020706177, 16.890713930130005, 17.111103057861328, 17.22906517982483, 17.434563398361206, 17.184027671813965]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 88.894 s.
 - Average e2e iteration time: 17.779001235961914 s.
 - Total local training time: 85.84900665283203 s.
 - Average local iteration time: 17.170000076293945 s.
 - Max allocated memory among devices: 18.408 GB.
 - Compilation times:  {'stage-construction': 41.47984004020691, 'stage-construction-dp': 1.4428913593292236, 'stage-construction-compilation': 9.246287107467651, 'stage-construction-profiling': 11.380805730819702}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 17.16989517211914
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_2.4B_1024.pkl`...
