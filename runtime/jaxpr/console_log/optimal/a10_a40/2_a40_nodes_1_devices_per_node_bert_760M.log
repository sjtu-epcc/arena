
------------------------------------------------------------------
- (1/3) Profiling bert_760M with batch size: 128...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_760M_128.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f0f8e619a30>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.670 GB, invar_size=1.221 GB, outvar_size=0.059 GB, temp_buffer_size=0.391 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.052, peak_memory=1.620 GB, invar_size=1.147 GB, outvar_size=0.059 GB, temp_buffer_size=0.414 GB, available_memory=35.242 GB)
result[(0, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.193, peak_memory=5.425 GB, invar_size=3.190 GB, outvar_size=1.565 GB, temp_buffer_size=2.236 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.200, peak_memory=5.143 GB, invar_size=2.895 GB, outvar_size=1.418 GB, temp_buffer_size=2.248 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 48.51 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=1.650 GB, invar_size=0.799 GB, outvar_size=0.070 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.065 GB, invar_size=0.674 GB, outvar_size=0.070 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.065 GB, invar_size=0.674 GB, outvar_size=0.070 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.065, peak_memory=1.299 GB, invar_size=0.885 GB, outvar_size=0.094 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=1.650 GB, invar_size=0.799 GB, outvar_size=0.070 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.065, peak_memory=1.299 GB, invar_size=0.885 GB, outvar_size=0.094 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.065 GB, invar_size=0.674 GB, outvar_size=0.070 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.065 GB, invar_size=0.674 GB, outvar_size=0.070 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.031, peak_memory=0.813 GB, invar_size=0.446 GB, outvar_size=0.047 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.164, peak_memory=7.023 GB, invar_size=1.668 GB, outvar_size=0.799 GB, temp_buffer_size=5.355 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.164, peak_memory=7.023 GB, invar_size=1.668 GB, outvar_size=0.799 GB, temp_buffer_size=5.355 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.146, peak_memory=6.757 GB, invar_size=1.395 GB, outvar_size=0.674 GB, temp_buffer_size=5.338 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.031, peak_memory=0.813 GB, invar_size=0.446 GB, outvar_size=0.047 GB, temp_buffer_size=0.320 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.146, peak_memory=6.757 GB, invar_size=1.395 GB, outvar_size=0.674 GB, temp_buffer_size=5.338 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.193, peak_memory=7.820 GB, invar_size=1.841 GB, outvar_size=0.885 GB, temp_buffer_size=5.955 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.146, peak_memory=7.024 GB, invar_size=1.395 GB, outvar_size=0.674 GB, temp_buffer_size=5.605 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.146, peak_memory=7.024 GB, invar_size=1.395 GB, outvar_size=0.674 GB, temp_buffer_size=5.605 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.193, peak_memory=7.820 GB, invar_size=1.841 GB, outvar_size=0.885 GB, temp_buffer_size=5.955 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.165, peak_memory=6.075 GB, invar_size=1.604 GB, outvar_size=0.790 GB, temp_buffer_size=4.448 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.165, peak_memory=6.075 GB, invar_size=1.604 GB, outvar_size=0.790 GB, temp_buffer_size=4.448 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 36.82 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2], [3, 4, 5]]
Result mesh_shapes: [(1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1)]
Result autosharding_option_dicts: [{}, {}]
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=895747)[0m 
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=895747)[0m 
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=895747)[0m 
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO comm 0x53da220 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=895747)[0m gpu2:895747:895747 [0] NCCL INFO comm 0x496d360 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
 - Compile (driver): 121.46 s
compilation time breakdown: {'stage-construction': '88.89', 'stage-construction-dp': '1.36', 'stage-construction-compilation': '29.87', 'stage-construction-profiling': '28.42'}
 - Compile (worker): 10.03 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=2280401, ip=192.168.0.18)[0m gpu3:2280401:2280401 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 62.49 s

[9.541006326675415, 7.78004264831543, 7.8706090450286865, 7.785707712173462, 7.781490802764893, 7.78148341178894, 7.7844178676605225]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 40.897 s.
 - Average e2e iteration time: 8.179000854492188 s.
 - Total local training time: 39.00400161743164 s.
 - Average local iteration time: 7.801000595092773 s.
 - Max allocated memory among devices: 12.543 GB.
 - Compilation times:  {'stage-construction': 88.88510966300964, 'stage-construction-dp': 1.3588478565216064, 'stage-construction-compilation': 29.868590593338013, 'stage-construction-profiling': 28.41742992401123}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 7.800741672515869
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_760M_128.pkl`...

------------------------------------------------------------------
- (2/3) Profiling bert_760M with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_760M_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fea0451e520>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=2.120 GB, invar_size=1.221 GB, outvar_size=0.117 GB, temp_buffer_size=0.781 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.097, peak_memory=2.092 GB, invar_size=1.147 GB, outvar_size=0.117 GB, temp_buffer_size=0.828 GB, available_memory=35.242 GB)
result[(0, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.344, peak_memory=7.720 GB, invar_size=3.248 GB, outvar_size=1.565 GB, temp_buffer_size=4.472 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.360, peak_memory=7.449 GB, invar_size=2.954 GB, outvar_size=1.418 GB, temp_buffer_size=4.495 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 50.22 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.114, peak_memory=2.502 GB, invar_size=0.799 GB, outvar_size=0.141 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=1.479 GB, invar_size=0.698 GB, outvar_size=0.141 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.114, peak_memory=2.502 GB, invar_size=0.799 GB, outvar_size=0.141 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=1.479 GB, invar_size=0.698 GB, outvar_size=0.141 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.127, peak_memory=1.737 GB, invar_size=0.909 GB, outvar_size=0.188 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.127, peak_memory=1.737 GB, invar_size=0.909 GB, outvar_size=0.188 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=1.479 GB, invar_size=0.698 GB, outvar_size=0.141 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.096, peak_memory=1.479 GB, invar_size=0.698 GB, outvar_size=0.141 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.322, peak_memory=12.443 GB, invar_size=1.738 GB, outvar_size=0.799 GB, temp_buffer_size=10.705 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.062, peak_memory=1.204 GB, invar_size=0.469 GB, outvar_size=0.094 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.322, peak_memory=12.443 GB, invar_size=1.738 GB, outvar_size=0.799 GB, temp_buffer_size=10.705 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.287, peak_memory=12.209 GB, invar_size=1.489 GB, outvar_size=0.698 GB, temp_buffer_size=10.673 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.287, peak_memory=12.209 GB, invar_size=1.489 GB, outvar_size=0.698 GB, temp_buffer_size=10.673 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.062, peak_memory=1.204 GB, invar_size=0.469 GB, outvar_size=0.094 GB, temp_buffer_size=0.641 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.379, peak_memory=13.908 GB, invar_size=1.958 GB, outvar_size=0.909 GB, temp_buffer_size=11.902 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.287, peak_memory=12.741 GB, invar_size=1.489 GB, outvar_size=0.698 GB, temp_buffer_size=11.205 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.287, peak_memory=12.741 GB, invar_size=1.489 GB, outvar_size=0.698 GB, temp_buffer_size=11.205 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.379, peak_memory=13.908 GB, invar_size=1.958 GB, outvar_size=0.909 GB, temp_buffer_size=11.902 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.325, peak_memory=10.617 GB, invar_size=1.674 GB, outvar_size=0.814 GB, temp_buffer_size=8.896 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.325, peak_memory=10.617 GB, invar_size=1.674 GB, outvar_size=0.814 GB, temp_buffer_size=8.896 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 36.88 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5]]
Result mesh_shapes: [(2, 1)]
Result logical_mesh_shapes: [(2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 132.42 s
compilation time breakdown: {'stage-construction': '90.58', 'stage-construction-dp': '1.33', 'stage-construction-compilation': '31.18', 'stage-construction-profiling': '29.60'}
 - Compile (worker): 19.40 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=903015)[0m 
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=903015)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=903015)[0m 
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=903015)[0m 
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO comm 0x3b1d3020 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=903015)[0m gpu2:903015:903015 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 110.56 s

[17.99086856842041, 13.866352796554565, 13.87136197090149, 13.835403203964233, 13.86105990409851, 13.870655298233032, 13.851771116256714]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 73.074 s.
 - Average e2e iteration time: 14.61500072479248 s.
 - Total local training time: 69.29000091552734 s.
 - Average local iteration time: 13.858000755310059 s.
 - Max allocated memory among devices: 14.176 GB.
 - Compilation times:  {'stage-construction': 90.57993483543396, 'stage-construction-dp': 1.3310766220092773, 'stage-construction-compilation': 31.179027318954468, 'stage-construction-profiling': 29.597790241241455}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 13.858050346374512
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_760M_256.pkl`...

------------------------------------------------------------------
- (3/3) Profiling bert_760M with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_760M_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 2
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f610db3a760>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (2, 1))
- Profiling for submesh 1 (2, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.176, peak_memory=3.018 GB, invar_size=1.221 GB, outvar_size=0.234 GB, temp_buffer_size=1.563 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.176, peak_memory=3.018 GB, invar_size=1.221 GB, outvar_size=0.234 GB, temp_buffer_size=1.563 GB, available_memory=35.242 GB)
result[(0, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.662, peak_memory=12.309 GB, invar_size=3.366 GB, outvar_size=1.565 GB, temp_buffer_size=8.943 GB, available_memory=35.242 GB)
result[(0, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.662, peak_memory=12.309 GB, invar_size=3.366 GB, outvar_size=1.565 GB, temp_buffer_size=8.943 GB, available_memory=35.242 GB)
Profiling for submesh 1 (2, 1) takes 47.64 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.222, peak_memory=4.205 GB, invar_size=0.799 GB, outvar_size=0.281 GB, temp_buffer_size=3.125 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.185, peak_memory=2.307 GB, invar_size=0.745 GB, outvar_size=0.281 GB, temp_buffer_size=1.281 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.222, peak_memory=4.205 GB, invar_size=0.799 GB, outvar_size=0.281 GB, temp_buffer_size=3.125 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.185, peak_memory=2.307 GB, invar_size=0.745 GB, outvar_size=0.281 GB, temp_buffer_size=1.281 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.244, peak_memory=2.612 GB, invar_size=0.956 GB, outvar_size=0.375 GB, temp_buffer_size=1.281 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.244, peak_memory=2.612 GB, invar_size=0.956 GB, outvar_size=0.375 GB, temp_buffer_size=1.281 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.185, peak_memory=2.307 GB, invar_size=0.745 GB, outvar_size=0.281 GB, temp_buffer_size=1.281 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.185, peak_memory=2.307 GB, invar_size=0.745 GB, outvar_size=0.281 GB, temp_buffer_size=1.281 GB, available_memory=35.446 GB)
result[(0, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.623, peak_memory=23.283 GB, invar_size=1.879 GB, outvar_size=0.799 GB, temp_buffer_size=21.404 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.119, peak_memory=1.985 GB, invar_size=0.516 GB, outvar_size=0.188 GB, temp_buffer_size=1.281 GB, available_memory=35.446 GB)
result[(0, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.623, peak_memory=23.283 GB, invar_size=1.879 GB, outvar_size=0.799 GB, temp_buffer_size=21.404 GB, available_memory=35.446 GB)
result[(1, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.553, peak_memory=23.111 GB, invar_size=1.677 GB, outvar_size=0.745 GB, temp_buffer_size=21.340 GB, available_memory=35.446 GB)
result[(1, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.553, peak_memory=23.111 GB, invar_size=1.677 GB, outvar_size=0.745 GB, temp_buffer_size=21.340 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.119, peak_memory=1.985 GB, invar_size=0.516 GB, outvar_size=0.188 GB, temp_buffer_size=1.281 GB, available_memory=35.446 GB)
result[(1, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.732, peak_memory=26.086 GB, invar_size=2.193 GB, outvar_size=0.956 GB, temp_buffer_size=23.800 GB, available_memory=35.446 GB)
result[(2, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.553, peak_memory=24.174 GB, invar_size=1.677 GB, outvar_size=0.745 GB, temp_buffer_size=22.404 GB, available_memory=35.446 GB)
result[(2, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.553, peak_memory=24.174 GB, invar_size=1.677 GB, outvar_size=0.745 GB, temp_buffer_size=22.404 GB, available_memory=35.446 GB)
result[(1, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.732, peak_memory=26.086 GB, invar_size=2.193 GB, outvar_size=0.956 GB, temp_buffer_size=23.800 GB, available_memory=35.446 GB)
result[(3, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.632, peak_memory=19.701 GB, invar_size=1.815 GB, outvar_size=0.860 GB, temp_buffer_size=17.792 GB, available_memory=35.446 GB)
result[(3, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.632, peak_memory=19.701 GB, invar_size=1.815 GB, outvar_size=0.860 GB, temp_buffer_size=17.792 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 37.46 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5]]
Result mesh_shapes: [(2, 1)]
Result logical_mesh_shapes: [(2, 1)]
Result autosharding_option_dicts: [{}]
 - Compile (driver): 143.99 s
compilation time breakdown: {'stage-construction': '88.69', 'stage-construction-dp': '1.36', 'stage-construction-compilation': '29.59', 'stage-construction-profiling': '28.95'}
 - Compile (worker): 19.71 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=912689)[0m 
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=912689)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=912689)[0m 
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=912689)[0m 
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO comm 0x3991b950 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=912689)[0m gpu2:912689:912689 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 203.22 s

[29.449416875839233, 27.38991403579712, 27.3988196849823, 27.397109031677246, 27.389506578445435, 27.39397883415222, 27.391913890838623]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 141.945 s.
 - Average e2e iteration time: 28.389001846313477 s.
 - Total local training time: 136.97100830078125 s.
 - Average local iteration time: 27.394001007080078 s.
 - Max allocated memory among devices: 20.524 GB.
 - Compilation times:  {'stage-construction': 88.69452047348022, 'stage-construction-dp': 1.3565974235534668, 'stage-construction-compilation': 29.586801052093506, 'stage-construction-profiling': 28.952760696411133}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `2_a40_2_n_1_d`: 27.39426612854004
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_760M_512.pkl`...
