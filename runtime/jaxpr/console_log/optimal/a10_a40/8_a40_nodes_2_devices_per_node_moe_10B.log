
------------------------------------------------------------------
- (1/3) Profiling moe_10B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_10B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7efc8b15da90>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=2.075 GB, invar_size=1.254 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=2.075 GB, invar_size=1.254 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=2.216 GB, invar_size=1.208 GB, outvar_size=0.258 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.200 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=2.122 GB, invar_size=1.277 GB, outvar_size=0.047 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.176 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=2.075 GB, invar_size=1.254 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.200 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.027, peak_memory=2.292 GB, invar_size=1.494 GB, outvar_size=0.047 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.022, peak_memory=3.400 GB, invar_size=2.415 GB, outvar_size=0.234 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=2.275 GB, invar_size=1.290 GB, outvar_size=0.234 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.064, peak_memory=4.282 GB, invar_size=2.532 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.022, peak_memory=3.416 GB, invar_size=2.619 GB, outvar_size=0.047 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.060, peak_memory=2.259 GB, invar_size=1.439 GB, outvar_size=0.070 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.176 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.103, peak_memory=4.344 GB, invar_size=2.496 GB, outvar_size=1.225 GB, temp_buffer_size=1.801 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=4.917 GB, invar_size=2.696 GB, outvar_size=1.207 GB, temp_buffer_size=2.220 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.064, peak_memory=4.306 GB, invar_size=2.532 GB, outvar_size=1.254 GB, temp_buffer_size=1.751 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.097, peak_memory=4.310 GB, invar_size=2.496 GB, outvar_size=1.201 GB, temp_buffer_size=1.790 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=2.122 GB, invar_size=1.277 GB, outvar_size=0.047 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=2.122 GB, invar_size=1.277 GB, outvar_size=0.047 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.103, peak_memory=4.344 GB, invar_size=2.496 GB, outvar_size=1.225 GB, temp_buffer_size=1.801 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.103, peak_memory=4.391 GB, invar_size=2.496 GB, outvar_size=1.225 GB, temp_buffer_size=1.848 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.062, peak_memory=4.329 GB, invar_size=2.578 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.200 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.064, peak_memory=4.282 GB, invar_size=2.532 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.176 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.200 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.082, peak_memory=7.479 GB, invar_size=5.064 GB, outvar_size=2.415 GB, temp_buffer_size=2.415 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=2.075 GB, invar_size=1.254 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.070, peak_memory=4.878 GB, invar_size=2.814 GB, outvar_size=1.290 GB, temp_buffer_size=2.063 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.073, peak_memory=5.132 GB, invar_size=2.848 GB, outvar_size=1.494 GB, temp_buffer_size=2.073 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.176 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.131, peak_memory=5.179 GB, invar_size=2.737 GB, outvar_size=1.439 GB, temp_buffer_size=2.208 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.087, peak_memory=7.694 GB, invar_size=5.075 GB, outvar_size=2.619 GB, temp_buffer_size=2.408 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=2.122 GB, invar_size=1.277 GB, outvar_size=0.047 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.200 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.103, peak_memory=4.344 GB, invar_size=2.496 GB, outvar_size=1.225 GB, temp_buffer_size=1.801 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.200 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.018, peak_memory=3.176 GB, invar_size=2.379 GB, outvar_size=0.047 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.097, peak_memory=4.310 GB, invar_size=2.496 GB, outvar_size=1.201 GB, temp_buffer_size=1.790 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=2.075 GB, invar_size=1.254 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.062, peak_memory=4.329 GB, invar_size=2.578 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.010, peak_memory=3.071 GB, invar_size=2.274 GB, outvar_size=0.023 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.097, peak_memory=4.310 GB, invar_size=2.496 GB, outvar_size=1.201 GB, temp_buffer_size=1.790 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=2.122 GB, invar_size=1.277 GB, outvar_size=0.047 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.064, peak_memory=4.282 GB, invar_size=2.532 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.062, peak_memory=4.329 GB, invar_size=2.578 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.013, peak_memory=1.993 GB, invar_size=1.172 GB, outvar_size=0.023 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.013, peak_memory=1.993 GB, invar_size=1.172 GB, outvar_size=0.023 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.042, peak_memory=2.092 GB, invar_size=1.225 GB, outvar_size=0.070 GB, temp_buffer_size=0.797 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.023, peak_memory=2.075 GB, invar_size=1.254 GB, outvar_size=0.047 GB, temp_buffer_size=0.774 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.097, peak_memory=4.310 GB, invar_size=2.496 GB, outvar_size=1.201 GB, temp_buffer_size=1.790 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.062, peak_memory=4.329 GB, invar_size=2.578 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.062, peak_memory=4.329 GB, invar_size=2.578 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.103, peak_memory=4.344 GB, invar_size=2.496 GB, outvar_size=1.225 GB, temp_buffer_size=1.801 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.064, peak_memory=4.282 GB, invar_size=2.532 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=7.161 GB, invar_size=4.782 GB, outvar_size=2.379 GB, temp_buffer_size=2.356 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.103, peak_memory=4.344 GB, invar_size=2.496 GB, outvar_size=1.225 GB, temp_buffer_size=1.801 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.097, peak_memory=4.310 GB, invar_size=2.496 GB, outvar_size=1.201 GB, temp_buffer_size=1.790 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.064, peak_memory=4.282 GB, invar_size=2.532 GB, outvar_size=1.254 GB, temp_buffer_size=1.727 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.089, peak_memory=7.359 GB, invar_size=4.906 GB, outvar_size=2.453 GB, temp_buffer_size=2.430 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.111, peak_memory=4.706 GB, invar_size=2.500 GB, outvar_size=1.238 GB, temp_buffer_size=2.182 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.084, peak_memory=4.651 GB, invar_size=2.588 GB, outvar_size=1.282 GB, temp_buffer_size=2.040 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 26.90 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.070, peak_memory=4.486 GB, invar_size=0.406 GB, outvar_size=0.226 GB, temp_buffer_size=4.033 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.070, peak_memory=4.486 GB, invar_size=0.406 GB, outvar_size=0.226 GB, temp_buffer_size=4.033 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.061, peak_memory=7.425 GB, invar_size=4.594 GB, outvar_size=2.297 GB, temp_buffer_size=2.784 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.80 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 2, 2, 0) has been pruned...
[TMP] Stage (0, 2, 2, 1) has been pruned...
[TMP] Stage (0, 2, 2, 2) has been pruned...
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (2, 11, 3, 0) has been pruned...
[TMP] Stage (2, 11, 3, 1) has been pruned...
[TMP] Stage (2, 11, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 7, 2, 0) has been pruned...
[TMP] Stage (3, 7, 2, 1) has been pruned...
[TMP] Stage (3, 7, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (3, 12, 3, 0) has been pruned...
[TMP] Stage (3, 12, 3, 1) has been pruned...
[TMP] Stage (3, 12, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 9, 2, 0) has been pruned...
[TMP] Stage (5, 9, 2, 1) has been pruned...
[TMP] Stage (5, 9, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 10, 2, 0) has been pruned...
[TMP] Stage (6, 10, 2, 1) has been pruned...
[TMP] Stage (6, 10, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{}, {}, {}, {}, {}, {}, {}, {}]
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO comm 0x42fd270 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO comm 0x4071360 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO comm 0x9474530 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO comm 0x635fb20 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO comm 0xa0f3d70 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO comm 0x3668290 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO comm 0x61a4b20 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO comm 0xa34ab40 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=756016)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO comm 0x547d830 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO comm 0x94843f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO comm 0x95aacd0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO comm 0x74a2dc0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO comm 0x3d3e0f0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO comm 0xb238d50 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO comm 0x5f15f90 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO comm 0xb496100 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO comm 0x9d60c60 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO comm 0x5240e20 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO comm 0x754ac50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO comm 0x9cf7b20 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO comm 0xcb281f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO comm 0x50dd120 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO comm 0xc9252c0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO comm 0x71cbb10 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO comm 0xaf5d560 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [0] NCCL INFO comm 0x3e70d20 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1010839 [1] NCCL INFO comm 0x5d5ff10 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO comm 0xb03b470 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 81.91 s
compilation time breakdown: {'stage-construction': '41.27', 'stage-construction-dp': '1.43', 'stage-construction-compilation': '9.55', 'stage-construction-profiling': '11.63'}
 - Compile (worker): 6.59 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201604 [1] NCCL INFO comm 0x7f6ecbf78ae0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO comm 0x7f6eb8a61950 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201602 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2201498, ip=192.168.0.18)[0m gpu3:2201498:2201498 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO comm 0x7f7b059c4320 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553898 [1] NCCL INFO comm 0x7f7b0c223030 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553896 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2553834, ip=192.168.0.38)[0m gpu23:2553834:2553834 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=756016)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m 
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO comm 0x7fb5a7bc3290 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756097 [1] NCCL INFO comm 0x7fb54b400130 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756095 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=756016)[0m gpu2:756016:756016 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO comm 0x7f954ba7c840 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693016 [1] NCCL INFO comm 0x7f94ef40e770 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3693014 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3692814, ip=192.168.0.39)[0m gpu24:3692814:3692814 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO comm 0x7f282fbc3a00 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316827 [1] NCCL INFO comm 0x7f281740da60 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316825 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3316763, ip=192.168.0.31)[0m gpu16:3316763:3316763 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO comm 0x7f4267bc4a10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399913 [1] NCCL INFO comm 0x7f420b40db90 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399911 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2399807, ip=192.168.0.27)[0m gpu12:2399807:2399807 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO comm 0x7fd53bbd7fd0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521467 [1] NCCL INFO comm 0x7fd4ef3f9a40 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521465 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1521391, ip=192.168.0.32)[0m gpu17:1521391:1521391 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011029 [1] NCCL INFO comm 0x7f3f1b405940 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO comm 0x7f3f73bcd4a0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1010839, ip=192.168.0.26)[0m gpu11:1010839:1011027 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 120.33 s

[68.58709073066711, 7.434767723083496, 7.467533826828003, 7.580290079116821, 7.548519611358643, 7.732695579528809, 7.53337550163269]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 39.387 s.
 - Average e2e iteration time: 7.877000331878662 s.
 - Total local training time: 37.862003326416016 s.
 - Average local iteration time: 7.572000503540039 s.
 - Max allocated memory among devices: 11.082 GB.
 - Compilation times:  {'stage-construction': 41.27129554748535, 'stage-construction-dp': 1.431671142578125, 'stage-construction-compilation': 9.55043649673462, 'stage-construction-profiling': 11.631522178649902}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 7.572483062744141
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_10B_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling moe_10B with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_10B_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7efdcd237850>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=3.997 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=2.919 GB, invar_size=1.278 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.043, peak_memory=4.384 GB, invar_size=2.415 GB, outvar_size=0.469 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=2.919 GB, invar_size=1.278 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=3.997 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=4.044 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=3.013 GB, invar_size=1.324 GB, outvar_size=0.094 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=3.997 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=3.013 GB, invar_size=1.324 GB, outvar_size=0.094 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=2.919 GB, invar_size=1.278 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.140, peak_memory=9.425 GB, invar_size=5.299 GB, outvar_size=2.415 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=6.104 GB, invar_size=2.602 GB, outvar_size=1.278 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=4.044 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.053, peak_memory=3.424 GB, invar_size=1.705 GB, outvar_size=0.094 GB, temp_buffer_size=1.625 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.119, peak_memory=3.315 GB, invar_size=1.673 GB, outvar_size=0.141 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=4.044 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=3.224 GB, invar_size=1.208 GB, outvar_size=0.516 GB, temp_buffer_size=1.501 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.052, peak_memory=3.259 GB, invar_size=1.290 GB, outvar_size=0.469 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.204, peak_memory=6.331 GB, invar_size=2.637 GB, outvar_size=1.272 GB, temp_buffer_size=3.600 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.043, peak_memory=4.425 GB, invar_size=2.830 GB, outvar_size=0.094 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=4.044 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.204, peak_memory=6.331 GB, invar_size=2.637 GB, outvar_size=1.272 GB, temp_buffer_size=3.600 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=6.104 GB, invar_size=2.602 GB, outvar_size=1.278 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.307 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.408 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.204, peak_memory=6.425 GB, invar_size=2.637 GB, outvar_size=1.272 GB, temp_buffer_size=3.694 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.191, peak_memory=6.264 GB, invar_size=2.637 GB, outvar_size=1.225 GB, temp_buffer_size=3.580 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.354 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=3.013 GB, invar_size=1.324 GB, outvar_size=0.094 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.120, peak_memory=6.197 GB, invar_size=2.696 GB, outvar_size=1.277 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.307 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.408 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=6.151 GB, invar_size=2.602 GB, outvar_size=1.278 GB, temp_buffer_size=3.502 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.191, peak_memory=6.264 GB, invar_size=2.637 GB, outvar_size=1.225 GB, temp_buffer_size=3.580 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.120, peak_memory=6.197 GB, invar_size=2.696 GB, outvar_size=1.277 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=2.919 GB, invar_size=1.278 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=3.997 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.307 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.408 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.259, peak_memory=7.947 GB, invar_size=3.065 GB, outvar_size=1.673 GB, temp_buffer_size=4.414 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.214, peak_memory=7.389 GB, invar_size=2.978 GB, outvar_size=1.207 GB, temp_buffer_size=4.411 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.401 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.502 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.136, peak_memory=7.175 GB, invar_size=3.049 GB, outvar_size=1.290 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=3.013 GB, invar_size=1.324 GB, outvar_size=0.094 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=4.044 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.354 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.149, peak_memory=9.695 GB, invar_size=5.332 GB, outvar_size=2.830 GB, temp_buffer_size=3.941 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.141, peak_memory=7.587 GB, invar_size=3.129 GB, outvar_size=1.705 GB, temp_buffer_size=4.036 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=3.997 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.191, peak_memory=6.264 GB, invar_size=2.637 GB, outvar_size=1.225 GB, temp_buffer_size=3.580 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.354 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=2.919 GB, invar_size=1.278 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.204, peak_memory=6.331 GB, invar_size=2.637 GB, outvar_size=1.272 GB, temp_buffer_size=3.600 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.120, peak_memory=6.197 GB, invar_size=2.696 GB, outvar_size=1.277 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.021, peak_memory=3.891 GB, invar_size=2.297 GB, outvar_size=0.047 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=3.013 GB, invar_size=1.324 GB, outvar_size=0.094 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.036, peak_memory=4.044 GB, invar_size=2.403 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=6.104 GB, invar_size=2.602 GB, outvar_size=1.278 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=3.007 GB, invar_size=1.272 GB, outvar_size=0.141 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.045, peak_memory=2.919 GB, invar_size=1.278 GB, outvar_size=0.094 GB, temp_buffer_size=1.547 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.307 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.408 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=2.860 GB, invar_size=1.219 GB, outvar_size=0.047 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.026, peak_memory=2.860 GB, invar_size=1.219 GB, outvar_size=0.047 GB, temp_buffer_size=1.595 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.204, peak_memory=6.331 GB, invar_size=2.637 GB, outvar_size=1.272 GB, temp_buffer_size=3.600 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.354 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.191, peak_memory=6.264 GB, invar_size=2.637 GB, outvar_size=1.225 GB, temp_buffer_size=3.580 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.120, peak_memory=6.197 GB, invar_size=2.696 GB, outvar_size=1.277 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.307 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.408 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=6.104 GB, invar_size=2.602 GB, outvar_size=1.278 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.204, peak_memory=6.331 GB, invar_size=2.637 GB, outvar_size=1.272 GB, temp_buffer_size=3.600 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=8.354 GB, invar_size=4.852 GB, outvar_size=2.403 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.191, peak_memory=6.264 GB, invar_size=2.637 GB, outvar_size=1.225 GB, temp_buffer_size=3.580 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.124, peak_memory=6.104 GB, invar_size=2.602 GB, outvar_size=1.278 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.120, peak_memory=6.197 GB, invar_size=2.696 GB, outvar_size=1.277 GB, temp_buffer_size=3.455 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.154, peak_memory=9.033 GB, invar_size=4.953 GB, outvar_size=2.477 GB, temp_buffer_size=4.033 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.221, peak_memory=6.981 GB, invar_size=2.571 GB, outvar_size=1.262 GB, temp_buffer_size=4.363 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.147, peak_memory=6.830 GB, invar_size=2.750 GB, outvar_size=1.351 GB, temp_buffer_size=4.033 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 26.64 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.140, peak_memory=8.613 GB, invar_size=0.453 GB, outvar_size=0.273 GB, temp_buffer_size=8.066 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.140, peak_memory=8.613 GB, invar_size=0.453 GB, outvar_size=0.273 GB, temp_buffer_size=8.066 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.121, peak_memory=10.349 GB, invar_size=4.688 GB, outvar_size=2.344 GB, temp_buffer_size=5.568 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.72 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 2, 2, 0) has been pruned...
[TMP] Stage (0, 2, 2, 1) has been pruned...
[TMP] Stage (0, 2, 2, 2) has been pruned...
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (2, 11, 3, 0) has been pruned...
[TMP] Stage (2, 11, 3, 1) has been pruned...
[TMP] Stage (2, 11, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 7, 2, 0) has been pruned...
[TMP] Stage (3, 7, 2, 1) has been pruned...
[TMP] Stage (3, 7, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (3, 12, 3, 0) has been pruned...
[TMP] Stage (3, 12, 3, 1) has been pruned...
[TMP] Stage (3, 12, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 9, 2, 0) has been pruned...
[TMP] Stage (5, 9, 2, 1) has been pruned...
[TMP] Stage (5, 9, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 10, 2, 0) has been pruned...
[TMP] Stage (6, 10, 2, 1) has been pruned...
[TMP] Stage (6, 10, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}, {}, {}, {}, {}, {}]
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=765643)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO comm 0x517fbc0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO comm 0x48213d0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO comm 0x71cec80 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO comm 0x6c07b70 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO comm 0xc6edf70 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO comm 0x5234e40 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO comm 0xb00e560 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO comm 0x8684d70 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO comm 0x3ba43a0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO comm 0xb6ca230 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO comm 0x5d50b90 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO comm 0xb6610f0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO comm 0x9b9ec70 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO comm 0x428b6e0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO comm 0x630d690 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO comm 0x9b35b30 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO comm 0x48e7780 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO comm 0xa3bca80 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO comm 0x6bd5270 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO comm 0xa82b810 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO comm 0xc1abd10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO comm 0x48af340 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO comm 0xbfa9df0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO comm 0x6a79e50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO comm 0xa8c7a10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [0] NCCL INFO comm 0x3df6100 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO comm 0xa85e8d0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555327 [1] NCCL INFO comm 0x5b5c510 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 81.89 s
compilation time breakdown: {'stage-construction': '40.95', 'stage-construction-dp': '1.42', 'stage-construction-compilation': '10.36', 'stage-construction-profiling': '11.51'}
 - Compile (worker): 6.71 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765643 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698648 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO comm 0x7f00ebbca9b0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525260 [1] NCCL INFO comm 0x7f008f407550 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525258 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1525154, ip=192.168.0.32)[0m gpu17:1525154:1525154 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO comm 0x7f58b3bc4970 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012939 [1] NCCL INFO comm 0x7f585740da90 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012937 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1012875, ip=192.168.0.26)[0m gpu11:1012875:1012875 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO comm 0x7fa7afbc8ff0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317771 [1] NCCL INFO comm 0x7fa7633fabc0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317769 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3317665, ip=192.168.0.31)[0m gpu16:3317665:3317665 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO comm 0x7f6e9bbd1ed0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206154 [1] NCCL INFO comm 0x7f6e53400250 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206152 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2206048, ip=192.168.0.18)[0m gpu3:2206048:2206048 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO comm 0x7f24aba7d460 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406119 [1] NCCL INFO comm 0x7f246b40da30 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2406117 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2405970, ip=192.168.0.27)[0m gpu12:2405970:2405970 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO comm 0x7fb047a8ebb0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555433 [1] NCCL INFO comm 0x7fafeb3fe220 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2555327, ip=192.168.0.38)[0m gpu23:2555327:2555431 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO comm 0x7fc479538640 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698754 [1] NCCL INFO comm 0x7fc473ff5be0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=765643)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=765643)[0m 
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO comm 0x7eed442e7540 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765766 [1] NCCL INFO comm 0x7f030b06b100 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3698648, ip=192.168.0.39)[0m gpu24:3698648:3698752 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=765643)[0m gpu2:765643:765764 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 165.74 s

[62.38019013404846, 15.615140199661255, 15.082702159881592, 16.06177043914795, 16.28466796875, 16.02619433403015, 15.179827213287354]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 82.626 s.
 - Average e2e iteration time: 16.525001525878906 s.
 - Total local training time: 78.63500213623047 s.
 - Average local iteration time: 15.727001190185547 s.
 - Max allocated memory among devices: 19.186 GB.
 - Compilation times:  {'stage-construction': 40.945640325546265, 'stage-construction-dp': 1.4233486652374268, 'stage-construction-compilation': 10.361708402633667, 'stage-construction-profiling': 11.505858659744263}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 15.727032661437988
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_10B_512.pkl`...

------------------------------------------------------------------
- (3/3) Profiling moe_10B with batch size: 1024...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/moe_10B_1024.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f648ab774f0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.732 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.732 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=4.607 GB, invar_size=1.325 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.166, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.795 GB, invar_size=1.418 GB, outvar_size=0.188 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.164, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.177, peak_memory=5.241 GB, invar_size=1.208 GB, outvar_size=1.031 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.638 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.001 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=4.607 GB, invar_size=1.325 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.166, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.086, peak_memory=6.354 GB, invar_size=2.415 GB, outvar_size=0.938 GB, temp_buffer_size=3.001 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.638 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.001 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.379, peak_memory=10.172 GB, invar_size=2.918 GB, outvar_size=1.272 GB, temp_buffer_size=7.160 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.638 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.001 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.732 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.426, peak_memory=12.363 GB, invar_size=3.541 GB, outvar_size=1.207 GB, temp_buffer_size=8.823 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.795 GB, invar_size=1.418 GB, outvar_size=0.188 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=4.607 GB, invar_size=1.325 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.164, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.104, peak_memory=5.229 GB, invar_size=1.290 GB, outvar_size=0.938 GB, temp_buffer_size=3.001 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.166, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.086, peak_memory=6.441 GB, invar_size=3.252 GB, outvar_size=0.188 GB, temp_buffer_size=3.001 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.406, peak_memory=10.303 GB, invar_size=2.918 GB, outvar_size=1.365 GB, temp_buffer_size=7.197 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.236, peak_memory=5.425 GB, invar_size=2.142 GB, outvar_size=0.281 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.105, peak_memory=5.317 GB, invar_size=2.127 GB, outvar_size=0.188 GB, temp_buffer_size=3.002 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.996 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.902 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.816 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.254, peak_memory=13.930 GB, invar_size=5.768 GB, outvar_size=2.415 GB, temp_buffer_size=8.163 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.246, peak_memory=9.746 GB, invar_size=2.743 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.795 GB, invar_size=1.418 GB, outvar_size=0.188 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.406, peak_memory=10.303 GB, invar_size=2.918 GB, outvar_size=1.365 GB, temp_buffer_size=7.197 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=9.933 GB, invar_size=2.930 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.246, peak_memory=9.746 GB, invar_size=2.743 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.164, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.379, peak_memory=10.172 GB, invar_size=2.918 GB, outvar_size=1.272 GB, temp_buffer_size=7.160 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.732 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.902 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.816 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.166, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.795 GB, invar_size=1.418 GB, outvar_size=0.188 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.996 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=9.933 GB, invar_size=2.930 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.638 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.001 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.246, peak_memory=9.840 GB, invar_size=2.743 GB, outvar_size=1.324 GB, temp_buffer_size=7.004 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=4.607 GB, invar_size=1.325 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.406, peak_memory=10.491 GB, invar_size=2.918 GB, outvar_size=1.365 GB, temp_buffer_size=7.385 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=12.090 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=7.004 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.164, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.268, peak_memory=11.771 GB, invar_size=3.518 GB, outvar_size=1.290 GB, temp_buffer_size=8.253 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.732 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.274, peak_memory=14.573 GB, invar_size=5.848 GB, outvar_size=3.252 GB, temp_buffer_size=7.882 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.514, peak_memory=13.485 GB, invar_size=3.721 GB, outvar_size=2.142 GB, temp_buffer_size=8.826 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.902 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.816 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.166, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.638 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.001 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.279, peak_memory=12.608 GB, invar_size=3.692 GB, outvar_size=2.127 GB, temp_buffer_size=8.072 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=4.607 GB, invar_size=1.325 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.379, peak_memory=10.172 GB, invar_size=2.918 GB, outvar_size=1.272 GB, temp_buffer_size=7.160 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=5.732 GB, invar_size=2.450 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.164, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.996 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.081, peak_memory=4.795 GB, invar_size=1.418 GB, outvar_size=0.188 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.406, peak_memory=10.303 GB, invar_size=2.918 GB, outvar_size=1.365 GB, temp_buffer_size=7.197 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=9.933 GB, invar_size=2.930 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.166, peak_memory=4.836 GB, invar_size=1.366 GB, outvar_size=0.281 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.902 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.816 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.041, peak_memory=5.532 GB, invar_size=2.344 GB, outvar_size=0.094 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.051, peak_memory=4.596 GB, invar_size=1.313 GB, outvar_size=0.094 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.051, peak_memory=4.596 GB, invar_size=1.313 GB, outvar_size=0.094 GB, temp_buffer_size=3.189 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.089, peak_memory=4.607 GB, invar_size=1.325 GB, outvar_size=0.188 GB, temp_buffer_size=3.095 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.246, peak_memory=9.746 GB, invar_size=2.743 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.996 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.406, peak_memory=10.303 GB, invar_size=2.918 GB, outvar_size=1.365 GB, temp_buffer_size=7.197 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.379, peak_memory=10.172 GB, invar_size=2.918 GB, outvar_size=1.272 GB, temp_buffer_size=7.160 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.246, peak_memory=9.746 GB, invar_size=2.743 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.902 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.816 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=9.933 GB, invar_size=2.930 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.232, peak_memory=11.996 GB, invar_size=4.993 GB, outvar_size=2.449 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.406, peak_memory=10.303 GB, invar_size=2.918 GB, outvar_size=1.365 GB, temp_buffer_size=7.197 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.379, peak_memory=10.172 GB, invar_size=2.918 GB, outvar_size=1.272 GB, temp_buffer_size=7.160 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.237, peak_memory=9.933 GB, invar_size=2.930 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.246, peak_memory=9.746 GB, invar_size=2.743 GB, outvar_size=1.324 GB, temp_buffer_size=6.910 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.284, peak_memory=13.207 GB, invar_size=5.047 GB, outvar_size=2.523 GB, temp_buffer_size=8.066 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.290, peak_memory=11.051 GB, invar_size=2.891 GB, outvar_size=1.398 GB, temp_buffer_size=8.066 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.439, peak_memory=11.531 GB, invar_size=2.711 GB, outvar_size=1.309 GB, temp_buffer_size=8.726 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 26.80 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.280, peak_memory=16.867 GB, invar_size=0.547 GB, outvar_size=0.367 GB, temp_buffer_size=16.132 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.280, peak_memory=16.867 GB, invar_size=0.547 GB, outvar_size=0.367 GB, temp_buffer_size=16.132 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.083, peak_memory=8.815 GB, invar_size=2.438 GB, outvar_size=0.188 GB, temp_buffer_size=6.189 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=16.198 GB, invar_size=4.875 GB, outvar_size=2.438 GB, temp_buffer_size=11.136 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 10.60 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 2, 2, 0) has been pruned...
[TMP] Stage (0, 2, 2, 1) has been pruned...
[TMP] Stage (0, 2, 2, 2) has been pruned...
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 3, 2, 0) has been pruned...
[TMP] Stage (1, 3, 2, 1) has been pruned...
[TMP] Stage (1, 3, 2, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 6, 3, 0) has been pruned...
[TMP] Stage (1, 6, 3, 1) has been pruned...
[TMP] Stage (1, 6, 3, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 4, 2, 0) has been pruned...
[TMP] Stage (2, 4, 2, 1) has been pruned...
[TMP] Stage (2, 4, 2, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (2, 11, 3, 0) has been pruned...
[TMP] Stage (2, 11, 3, 1) has been pruned...
[TMP] Stage (2, 11, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 7, 2, 0) has been pruned...
[TMP] Stage (3, 7, 2, 1) has been pruned...
[TMP] Stage (3, 7, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (3, 12, 3, 0) has been pruned...
[TMP] Stage (3, 12, 3, 1) has been pruned...
[TMP] Stage (3, 12, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 9, 2, 0) has been pruned...
[TMP] Stage (5, 9, 2, 1) has been pruned...
[TMP] Stage (5, 9, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 10, 2, 0) has been pruned...
[TMP] Stage (6, 10, 2, 1) has been pruned...
[TMP] Stage (6, 10, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 11, 2, 0) has been pruned...
[TMP] Stage (7, 11, 2, 1) has been pruned...
[TMP] Stage (7, 11, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO comm 0x41c25a0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO comm 0x49cc8e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO comm 0x6882810 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO comm 0x957b920 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO comm 0xa442790 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO comm 0x4cfeaa0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO comm 0xa4ab8d0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO comm 0x6aa4ab0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=775361)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO comm 0xaa7dd20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO comm 0x522aa50 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO comm 0xa625d50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO comm 0xb1897d0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO comm 0x4510b30 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO comm 0xb4eaa40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO comm 0x78b2e90 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO comm 0xb553b80 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO comm 0xa904070 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO comm 0x3843340 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO comm 0x8c45850 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO comm 0xa89af30 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO comm 0x9b0cdc0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO comm 0x49eafb0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO comm 0x9ded110 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO comm 0x9b75f00 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO comm 0xacb1b20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [0] NCCL INFO comm 0x43bb600 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO comm 0xad1ac60 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411334 [1] NCCL INFO comm 0x97be240 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 81.10 s
compilation time breakdown: {'stage-construction': '40.96', 'stage-construction-dp': '1.45', 'stage-construction-compilation': '8.94', 'stage-construction-profiling': '11.55'}
 - Compile (worker): 6.43 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211041 [1] NCCL INFO comm 0x7f5af87d1e70 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO comm 0x7f5b0019fc80 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2211039 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2210850, ip=192.168.0.18)[0m gpu3:2210850:2210850 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO comm 0x7f54d26bbea0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556868 [1] NCCL INFO comm 0x7f54cbe1b5a0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556866 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2556761, ip=192.168.0.38)[0m gpu23:2556761:2556761 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775361 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318680 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703423 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530038 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1015972 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411524 [1] NCCL INFO comm 0x7f32dba1da30 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO comm 0x7f32e3ce5350 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2411334, ip=192.168.0.27)[0m gpu12:2411334:2411522 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016037 [1] NCCL INFO comm 0x7ee9aec17130 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO comm 0x7ee9b30aa8c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530102 [1] NCCL INFO comm 0x7f8ffc9a7bf0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO comm 0x7f9000ab7d50 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703529 [1] NCCL INFO comm 0x7f2ba032f990 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO comm 0x7f2ba91af870 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO comm 0x7efcd8ef9930 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318744 [1] NCCL INFO comm 0x7efcd09365e0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=775361)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m 
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775439 [1] NCCL INFO comm 0x7f6caf93e410 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO comm 0x7f6cb5223130 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1015972, ip=192.168.0.26)[0m gpu11:1015972:1016035 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1530038, ip=192.168.0.32)[0m gpu17:1530038:1530100 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3703423, ip=192.168.0.39)[0m gpu24:3703423:3703527 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3318680, ip=192.168.0.31)[0m gpu16:3318680:3318742 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=775361)[0m gpu2:775361:775437 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 280.61 s

[99.38621234893799, 28.29293179512024, 28.506038904190063, 28.614609956741333, 28.590320348739624, 28.549276113510132, 28.583988428115845]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 147.562 s.
 - Average e2e iteration time: 29.512001037597656 s.
 - Total local training time: 142.84400939941406 s.
 - Average local iteration time: 28.569002151489258 s.
 - Max allocated memory among devices: 29.176 GB.
 - Compilation times:  {'stage-construction': 40.95640587806702, 'stage-construction-dp': 1.4479179382324219, 'stage-construction-compilation': 8.944505453109741, 'stage-construction-profiling': 11.55236029624939}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 28.56884765625
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/moe_10B_1024.pkl`...
