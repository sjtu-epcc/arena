
------------------------------------------------------------------
- (1/3) Profiling wide_resnet_1B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_1B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 3, 1), 0] = ModuleProfileResult(compute_cost=0.102, peak_memory=2.434 GB, invar_size=1.877 GB, outvar_size=0.209 GB, temp_buffer_size=0.347 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 1] = ModuleProfileResult(compute_cost=5.985, peak_memory=12.505 GB, invar_size=7.688 GB, outvar_size=3.746 GB, temp_buffer_size=4.817 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 0] = ModuleProfileResult(compute_cost=5.543, peak_memory=1.369 GB, invar_size=0.883 GB, outvar_size=0.247 GB, temp_buffer_size=0.240 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 1] = ModuleProfileResult(compute_cost=2.751, peak_memory=6.369 GB, invar_size=3.962 GB, outvar_size=1.876 GB, temp_buffer_size=2.407 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 1] = ModuleProfileResult(compute_cost=9.376, peak_memory=2.734 GB, invar_size=2.010 GB, outvar_size=0.881 GB, temp_buffer_size=0.724 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 0] = ModuleProfileResult(compute_cost=0.059, peak_memory=4.502 GB, invar_size=3.748 GB, outvar_size=0.195 GB, temp_buffer_size=0.560 GB, available_memory=35.242 GB)
Profiling for submesh 3 (4, 2) takes 80.98 seconds
--------------------------------------------------
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.711 GB, invar_size=0.232 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.938 GB, invar_size=0.278 GB, outvar_size=0.299 GB, temp_buffer_size=0.361 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.124, peak_memory=1.216 GB, invar_size=0.318 GB, outvar_size=0.419 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.938 GB, invar_size=0.278 GB, outvar_size=0.299 GB, temp_buffer_size=0.361 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.711 GB, invar_size=0.232 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=1.494 GB, invar_size=0.643 GB, outvar_size=0.232 GB, temp_buffer_size=0.731 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.110, peak_memory=1.133 GB, invar_size=0.296 GB, outvar_size=0.359 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.672 GB, invar_size=0.266 GB, outvar_size=0.180 GB, temp_buffer_size=0.226 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.929 GB, invar_size=0.218 GB, outvar_size=0.359 GB, temp_buffer_size=0.351 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.171, peak_memory=1.251 GB, invar_size=0.244 GB, outvar_size=0.479 GB, temp_buffer_size=0.529 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=0.793 GB, invar_size=0.313 GB, outvar_size=0.180 GB, temp_buffer_size=0.301 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.598 GB, invar_size=0.735 GB, outvar_size=0.278 GB, temp_buffer_size=0.744 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.598 GB, invar_size=0.735 GB, outvar_size=0.278 GB, temp_buffer_size=0.744 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.202, peak_memory=1.957 GB, invar_size=0.816 GB, outvar_size=0.199 GB, temp_buffer_size=1.022 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.672 GB, invar_size=0.266 GB, outvar_size=0.180 GB, temp_buffer_size=0.226 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=1.434 GB, invar_size=0.583 GB, outvar_size=0.232 GB, temp_buffer_size=0.731 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.168, peak_memory=1.851 GB, invar_size=0.710 GB, outvar_size=0.176 GB, temp_buffer_size=1.021 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.791 GB, invar_size=0.312 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.251 GB, invar_size=0.592 GB, outvar_size=0.266 GB, temp_buffer_size=0.539 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.791 GB, invar_size=0.312 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.060, peak_memory=0.876 GB, invar_size=0.336 GB, outvar_size=0.239 GB, temp_buffer_size=0.301 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.125, peak_memory=1.425 GB, invar_size=0.565 GB, outvar_size=0.193 GB, temp_buffer_size=0.740 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.827 GB, invar_size=0.765 GB, outvar_size=0.218 GB, temp_buffer_size=0.972 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.893 GB, invar_size=0.417 GB, outvar_size=0.209 GB, temp_buffer_size=0.267 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.088, peak_memory=0.807 GB, invar_size=0.299 GB, outvar_size=0.269 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.251 GB, invar_size=0.592 GB, outvar_size=0.266 GB, temp_buffer_size=0.539 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.723 GB, invar_size=0.371 GB, outvar_size=0.150 GB, temp_buffer_size=0.202 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.279, peak_memory=2.108 GB, invar_size=0.786 GB, outvar_size=0.244 GB, temp_buffer_size=1.142 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.893 GB, invar_size=0.417 GB, outvar_size=0.209 GB, temp_buffer_size=0.267 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.090, peak_memory=0.784 GB, invar_size=0.276 GB, outvar_size=0.269 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.723 GB, invar_size=0.371 GB, outvar_size=0.150 GB, temp_buffer_size=0.202 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=1.413 GB, invar_size=0.744 GB, outvar_size=0.312 GB, temp_buffer_size=0.550 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=0.911 GB, invar_size=0.555 GB, outvar_size=0.180 GB, temp_buffer_size=0.177 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.159, peak_memory=1.530 GB, invar_size=0.671 GB, outvar_size=0.216 GB, temp_buffer_size=0.740 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=1.413 GB, invar_size=0.744 GB, outvar_size=0.312 GB, temp_buffer_size=0.550 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=1.605 GB, invar_size=0.984 GB, outvar_size=0.417 GB, temp_buffer_size=0.562 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.014, peak_memory=0.800 GB, invar_size=0.475 GB, outvar_size=0.180 GB, temp_buffer_size=0.145 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.097, peak_memory=0.906 GB, invar_size=0.367 GB, outvar_size=0.299 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=1.408 GB, invar_size=0.832 GB, outvar_size=0.371 GB, temp_buffer_size=0.516 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.109, peak_memory=1.286 GB, invar_size=0.700 GB, outvar_size=0.216 GB, temp_buffer_size=0.526 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.138, peak_memory=1.332 GB, invar_size=0.746 GB, outvar_size=0.238 GB, temp_buffer_size=0.526 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.049 GB, invar_size=0.692 GB, outvar_size=0.150 GB, temp_buffer_size=0.207 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=1.635 GB, invar_size=1.014 GB, outvar_size=0.417 GB, temp_buffer_size=0.562 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=1.438 GB, invar_size=0.862 GB, outvar_size=0.371 GB, temp_buffer_size=0.516 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.064, peak_memory=0.885 GB, invar_size=0.436 GB, outvar_size=0.209 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.156 GB, invar_size=0.829 GB, outvar_size=0.120 GB, temp_buffer_size=0.207 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.020, peak_memory=0.872 GB, invar_size=0.521 GB, outvar_size=0.150 GB, temp_buffer_size=0.202 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=2.012 GB, invar_size=1.228 GB, outvar_size=0.554 GB, temp_buffer_size=0.723 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=0.831 GB, invar_size=0.505 GB, outvar_size=0.120 GB, temp_buffer_size=0.207 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=1.499 GB, invar_size=0.913 GB, outvar_size=0.307 GB, temp_buffer_size=0.526 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=2.409 GB, invar_size=1.473 GB, outvar_size=0.692 GB, temp_buffer_size=0.876 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=0.831 GB, invar_size=0.505 GB, outvar_size=0.120 GB, temp_buffer_size=0.207 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.370 GB, invar_size=1.013 GB, outvar_size=0.150 GB, temp_buffer_size=0.207 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.029, peak_memory=1.724 GB, invar_size=1.100 GB, outvar_size=0.475 GB, temp_buffer_size=0.565 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=2.779 GB, invar_size=1.718 GB, outvar_size=0.829 GB, temp_buffer_size=1.001 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.037, peak_memory=0.952 GB, invar_size=0.596 GB, outvar_size=0.150 GB, temp_buffer_size=0.207 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.030 GB, invar_size=0.763 GB, outvar_size=0.120 GB, temp_buffer_size=0.147 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=1.445 GB, invar_size=0.961 GB, outvar_size=0.376 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.037, peak_memory=0.952 GB, invar_size=0.596 GB, outvar_size=0.150 GB, temp_buffer_size=0.207 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=0.691 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.145 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=0.691 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.145 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.050, peak_memory=1.692 GB, invar_size=1.131 GB, outvar_size=0.521 GB, temp_buffer_size=0.500 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.088, peak_memory=1.493 GB, invar_size=1.009 GB, outvar_size=0.445 GB, temp_buffer_size=0.425 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.086, peak_memory=1.461 GB, invar_size=1.009 GB, outvar_size=0.445 GB, temp_buffer_size=0.392 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.359 GB, invar_size=2.114 GB, outvar_size=1.012 GB, temp_buffer_size=1.184 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.243 GB, invar_size=0.946 GB, outvar_size=0.150 GB, temp_buffer_size=0.147 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=2.561 GB, invar_size=1.615 GB, outvar_size=0.763 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.037, peak_memory=0.812 GB, invar_size=0.518 GB, outvar_size=0.150 GB, temp_buffer_size=0.145 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.030 GB, invar_size=0.763 GB, outvar_size=0.120 GB, temp_buffer_size=0.147 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.037, peak_memory=0.812 GB, invar_size=0.518 GB, outvar_size=0.150 GB, temp_buffer_size=0.145 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.078, peak_memory=1.265 GB, invar_size=0.912 GB, outvar_size=0.396 GB, temp_buffer_size=0.323 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=0.661 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.115 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.105, peak_memory=1.674 GB, invar_size=1.222 GB, outvar_size=0.536 GB, temp_buffer_size=0.393 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=1.705 GB, invar_size=1.222 GB, outvar_size=0.536 GB, temp_buffer_size=0.423 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.078, peak_memory=1.265 GB, invar_size=0.912 GB, outvar_size=0.396 GB, temp_buffer_size=0.323 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=0.661 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.115 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.263 GB, invar_size=1.642 GB, outvar_size=0.135 GB, temp_buffer_size=0.487 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=2.561 GB, invar_size=1.615 GB, outvar_size=0.763 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.140 GB, invar_size=2.011 GB, outvar_size=0.946 GB, temp_buffer_size=1.099 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.050 GB, invar_size=1.459 GB, outvar_size=0.105 GB, temp_buffer_size=0.487 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.053, peak_memory=1.268 GB, invar_size=0.866 GB, outvar_size=0.135 GB, temp_buffer_size=0.267 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.097, peak_memory=1.478 GB, invar_size=1.125 GB, outvar_size=0.488 GB, temp_buffer_size=0.323 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.053, peak_memory=1.268 GB, invar_size=0.866 GB, outvar_size=0.135 GB, temp_buffer_size=0.267 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.078, peak_memory=1.265 GB, invar_size=0.912 GB, outvar_size=0.396 GB, temp_buffer_size=0.323 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=1.146 GB, invar_size=0.775 GB, outvar_size=0.105 GB, temp_buffer_size=0.267 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.097, peak_memory=1.478 GB, invar_size=1.125 GB, outvar_size=0.488 GB, temp_buffer_size=0.323 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.078, peak_memory=1.265 GB, invar_size=0.912 GB, outvar_size=0.396 GB, temp_buffer_size=0.323 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.046, peak_memory=1.146 GB, invar_size=0.775 GB, outvar_size=0.105 GB, temp_buffer_size=0.267 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=5.848 GB, invar_size=3.388 GB, outvar_size=1.642 GB, temp_buffer_size=2.430 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.798 GB, invar_size=2.192 GB, outvar_size=0.120 GB, temp_buffer_size=0.487 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.268 GB, invar_size=2.992 GB, outvar_size=1.459 GB, temp_buffer_size=2.246 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.585 GB, invar_size=2.008 GB, outvar_size=0.090 GB, temp_buffer_size=0.487 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=1.600 GB, invar_size=1.141 GB, outvar_size=0.150 GB, temp_buffer_size=0.310 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=1.600 GB, invar_size=1.141 GB, outvar_size=0.150 GB, temp_buffer_size=0.310 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=2.433 GB, invar_size=1.821 GB, outvar_size=0.836 GB, temp_buffer_size=0.582 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.050, peak_memory=1.479 GB, invar_size=1.049 GB, outvar_size=0.120 GB, temp_buffer_size=0.310 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.089, peak_memory=2.220 GB, invar_size=1.608 GB, outvar_size=0.744 GB, temp_buffer_size=0.582 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.108, peak_memory=2.433 GB, invar_size=1.821 GB, outvar_size=0.836 GB, temp_buffer_size=0.582 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.050, peak_memory=1.479 GB, invar_size=1.049 GB, outvar_size=0.120 GB, temp_buffer_size=0.310 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.089, peak_memory=2.220 GB, invar_size=1.608 GB, outvar_size=0.744 GB, temp_buffer_size=0.582 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.042, peak_memory=7.530 GB, invar_size=4.472 GB, outvar_size=2.191 GB, temp_buffer_size=3.028 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.038, peak_memory=6.950 GB, invar_size=4.076 GB, outvar_size=2.008 GB, temp_buffer_size=2.845 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.357 GB, invar_size=2.780 GB, outvar_size=0.090 GB, temp_buffer_size=0.487 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.057, peak_memory=1.865 GB, invar_size=1.435 GB, outvar_size=0.120 GB, temp_buffer_size=0.310 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.143 GB, invar_size=2.597 GB, outvar_size=0.060 GB, temp_buffer_size=0.487 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.095, peak_memory=2.997 GB, invar_size=2.371 GB, outvar_size=1.111 GB, temp_buffer_size=0.596 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=2.827 GB, invar_size=2.158 GB, outvar_size=1.019 GB, temp_buffer_size=0.639 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.057, peak_memory=1.865 GB, invar_size=1.435 GB, outvar_size=0.120 GB, temp_buffer_size=0.310 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.095, peak_memory=2.997 GB, invar_size=2.371 GB, outvar_size=1.111 GB, temp_buffer_size=0.596 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=1.687 GB, invar_size=1.344 GB, outvar_size=0.090 GB, temp_buffer_size=0.254 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=2.827 GB, invar_size=2.158 GB, outvar_size=1.019 GB, temp_buffer_size=0.639 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.032, peak_memory=1.687 GB, invar_size=1.344 GB, outvar_size=0.090 GB, temp_buffer_size=0.254 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.053, peak_memory=9.350 GB, invar_size=5.619 GB, outvar_size=2.780 GB, temp_buffer_size=3.702 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.049, peak_memory=8.771 GB, invar_size=5.223 GB, outvar_size=2.596 GB, temp_buffer_size=3.519 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.088, peak_memory=3.670 GB, invar_size=2.929 GB, outvar_size=1.405 GB, temp_buffer_size=0.711 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.088, peak_memory=3.670 GB, invar_size=2.929 GB, outvar_size=1.405 GB, temp_buffer_size=0.711 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.069, peak_memory=3.457 GB, invar_size=2.716 GB, outvar_size=1.313 GB, temp_buffer_size=0.711 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.069, peak_memory=3.457 GB, invar_size=2.716 GB, outvar_size=1.313 GB, temp_buffer_size=0.711 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.789 GB, invar_size=0.095 GB, outvar_size=0.389 GB, temp_buffer_size=0.306 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.789 GB, invar_size=0.095 GB, outvar_size=0.389 GB, temp_buffer_size=0.306 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.198, peak_memory=1.128 GB, invar_size=0.054 GB, outvar_size=0.568 GB, temp_buffer_size=0.506 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=1.608 GB, invar_size=0.573 GB, outvar_size=0.090 GB, temp_buffer_size=1.034 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=1.667 GB, invar_size=0.633 GB, outvar_size=0.090 GB, temp_buffer_size=1.034 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.271, peak_memory=1.990 GB, invar_size=0.667 GB, outvar_size=0.045 GB, temp_buffer_size=1.323 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.777 GB, invar_size=0.172 GB, outvar_size=0.299 GB, temp_buffer_size=0.306 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.157, peak_memory=1.146 GB, invar_size=0.221 GB, outvar_size=0.419 GB, temp_buffer_size=0.506 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.777 GB, invar_size=0.172 GB, outvar_size=0.299 GB, temp_buffer_size=0.306 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=0.929 GB, invar_size=0.218 GB, outvar_size=0.359 GB, temp_buffer_size=0.351 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.648 GB, invar_size=0.614 GB, outvar_size=0.172 GB, temp_buffer_size=0.945 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=1.767 GB, invar_size=0.705 GB, outvar_size=0.218 GB, temp_buffer_size=0.972 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.588 GB, invar_size=0.554 GB, outvar_size=0.172 GB, temp_buffer_size=0.945 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.245, peak_memory=2.002 GB, invar_size=0.681 GB, outvar_size=0.221 GB, temp_buffer_size=1.142 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 76.14 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(1, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.287 GB, invar_size=0.207 GB, outvar_size=0.479 GB, temp_buffer_size=0.601 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.049 GB, invar_size=0.033 GB, outvar_size=0.419 GB, temp_buffer_size=0.598 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.049 GB, invar_size=0.033 GB, outvar_size=0.419 GB, temp_buffer_size=0.598 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.804 GB, invar_size=0.386 GB, outvar_size=0.180 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.936 GB, invar_size=0.523 GB, outvar_size=0.120 GB, temp_buffer_size=0.294 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.690 GB, invar_size=0.211 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.936 GB, invar_size=0.523 GB, outvar_size=0.120 GB, temp_buffer_size=0.294 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.804 GB, invar_size=0.386 GB, outvar_size=0.180 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.791 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.244 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.455 GB, invar_size=0.475 GB, outvar_size=0.023 GB, temp_buffer_size=1.981 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.455 GB, invar_size=0.475 GB, outvar_size=0.023 GB, temp_buffer_size=1.981 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.791 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.244 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.731 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.541 GB, invar_size=0.830 GB, outvar_size=0.385 GB, temp_buffer_size=0.592 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.348 GB, invar_size=0.542 GB, outvar_size=0.211 GB, temp_buffer_size=0.686 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.694 GB, invar_size=0.713 GB, outvar_size=0.207 GB, temp_buffer_size=1.802 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.807 GB, invar_size=1.045 GB, outvar_size=0.523 GB, temp_buffer_size=0.642 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.731 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.541 GB, invar_size=0.830 GB, outvar_size=0.385 GB, temp_buffer_size=0.592 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.807 GB, invar_size=1.045 GB, outvar_size=0.523 GB, temp_buffer_size=0.642 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.427 GB, invar_size=0.912 GB, outvar_size=0.426 GB, temp_buffer_size=0.455 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.731 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.690 GB, invar_size=0.211 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.762 GB, invar_size=0.211 GB, outvar_size=0.239 GB, temp_buffer_size=0.311 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.762 GB, invar_size=0.211 GB, outvar_size=0.239 GB, temp_buffer_size=0.311 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.427 GB, invar_size=0.912 GB, outvar_size=0.426 GB, temp_buffer_size=0.455 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.999 GB, invar_size=0.340 GB, outvar_size=0.239 GB, temp_buffer_size=0.420 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.731 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.412 GB, invar_size=0.912 GB, outvar_size=0.426 GB, temp_buffer_size=0.440 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.731 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.731 GB, invar_size=0.426 GB, outvar_size=0.120 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.412 GB, invar_size=0.912 GB, outvar_size=0.426 GB, temp_buffer_size=0.440 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.716 GB, invar_size=1.123 GB, outvar_size=0.090 GB, temp_buffer_size=0.504 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.716 GB, invar_size=1.123 GB, outvar_size=0.090 GB, temp_buffer_size=0.504 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.367 GB, invar_size=0.912 GB, outvar_size=0.426 GB, temp_buffer_size=0.395 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.348 GB, invar_size=0.542 GB, outvar_size=0.211 GB, temp_buffer_size=0.686 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.143 GB, invar_size=0.306 GB, outvar_size=0.359 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.348 GB, invar_size=0.542 GB, outvar_size=0.211 GB, temp_buffer_size=0.686 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.266 GB, invar_size=1.672 GB, outvar_size=0.060 GB, temp_buffer_size=0.534 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.287 GB, invar_size=0.207 GB, outvar_size=0.479 GB, temp_buffer_size=0.601 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.266 GB, invar_size=1.672 GB, outvar_size=0.060 GB, temp_buffer_size=0.534 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.999 GB, invar_size=0.340 GB, outvar_size=0.239 GB, temp_buffer_size=0.420 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.367 GB, invar_size=0.912 GB, outvar_size=0.426 GB, temp_buffer_size=0.395 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.367 GB, invar_size=0.912 GB, outvar_size=0.426 GB, temp_buffer_size=0.395 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.348 GB, invar_size=0.542 GB, outvar_size=0.211 GB, temp_buffer_size=0.686 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.367 GB, invar_size=0.912 GB, outvar_size=0.426 GB, temp_buffer_size=0.395 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.056 GB, invar_size=1.535 GB, outvar_size=0.030 GB, temp_buffer_size=0.492 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.921 GB, invar_size=0.680 GB, outvar_size=0.340 GB, temp_buffer_size=1.001 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.143 GB, invar_size=0.306 GB, outvar_size=0.359 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.349 GB, invar_size=2.274 GB, outvar_size=1.122 GB, temp_buffer_size=1.015 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.056 GB, invar_size=1.535 GB, outvar_size=0.030 GB, temp_buffer_size=0.492 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.349 GB, invar_size=2.274 GB, outvar_size=1.122 GB, temp_buffer_size=1.015 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.553 GB, invar_size=3.343 GB, outvar_size=1.671 GB, temp_buffer_size=1.150 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.553 GB, invar_size=3.343 GB, outvar_size=1.671 GB, temp_buffer_size=1.150 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.937 GB, invar_size=3.068 GB, outvar_size=1.534 GB, temp_buffer_size=0.839 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.293 GB, invar_size=0.731 GB, outvar_size=0.306 GB, temp_buffer_size=1.323 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.937 GB, invar_size=3.068 GB, outvar_size=1.534 GB, temp_buffer_size=0.839 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.293 GB, invar_size=0.731 GB, outvar_size=0.306 GB, temp_buffer_size=1.323 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.921 GB, invar_size=0.680 GB, outvar_size=0.340 GB, temp_buffer_size=1.001 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.694 GB, invar_size=0.713 GB, outvar_size=0.207 GB, temp_buffer_size=1.802 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 17.66 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 2, 0) has been pruned...
[TMP] Stage (0, 6, 2, 1) has been pruned...
[TMP] Stage (0, 6, 2, 2) has been pruned...
[TMP] Stage (0, 7, 2, 0) has been pruned...
[TMP] Stage (0, 7, 2, 1) has been pruned...
[TMP] Stage (0, 7, 2, 2) has been pruned...
[TMP] Stage (0, 8, 2, 0) has been pruned...
[TMP] Stage (0, 8, 2, 1) has been pruned...
[TMP] Stage (0, 8, 2, 2) has been pruned...
[TMP] Stage (1, 6, 2, 0) has been pruned...
[TMP] Stage (1, 6, 2, 1) has been pruned...
[TMP] Stage (1, 6, 2, 2) has been pruned...
[TMP] Stage (1, 7, 2, 0) has been pruned...
[TMP] Stage (1, 7, 2, 1) has been pruned...
[TMP] Stage (1, 7, 2, 2) has been pruned...
[TMP] Stage (1, 8, 2, 0) has been pruned...
[TMP] Stage (1, 8, 2, 1) has been pruned...
[TMP] Stage (1, 8, 2, 2) has been pruned...
[TMP] Stage (1, 9, 2, 0) has been pruned...
[TMP] Stage (1, 9, 2, 1) has been pruned...
[TMP] Stage (1, 9, 2, 2) has been pruned...
[TMP] Stage (2, 7, 2, 0) has been pruned...
[TMP] Stage (2, 7, 2, 1) has been pruned...
[TMP] Stage (2, 7, 2, 2) has been pruned...
[TMP] Stage (2, 8, 2, 0) has been pruned...
[TMP] Stage (2, 8, 2, 1) has been pruned...
[TMP] Stage (2, 8, 2, 2) has been pruned...
[TMP] Stage (2, 9, 2, 0) has been pruned...
[TMP] Stage (2, 9, 2, 1) has been pruned...
[TMP] Stage (2, 9, 2, 2) has been pruned...
[TMP] Stage (2, 10, 2, 0) has been pruned...
[TMP] Stage (2, 10, 2, 1) has been pruned...
[TMP] Stage (2, 10, 2, 2) has been pruned...
[TMP] Stage (3, 9, 2, 0) has been pruned...
[TMP] Stage (3, 9, 2, 1) has been pruned...
[TMP] Stage (3, 9, 2, 2) has been pruned...
[TMP] Stage (3, 10, 2, 0) has been pruned...
[TMP] Stage (3, 10, 2, 1) has been pruned...
[TMP] Stage (3, 10, 2, 2) has been pruned...
[TMP] Stage (3, 11, 2, 0) has been pruned...
[TMP] Stage (3, 11, 2, 1) has been pruned...
[TMP] Stage (3, 11, 2, 2) has been pruned...
[TMP] Stage (4, 10, 2, 0) has been pruned...
[TMP] Stage (4, 10, 2, 1) has been pruned...
[TMP] Stage (4, 10, 2, 2) has been pruned...
[TMP] Stage (4, 11, 2, 0) has been pruned...
[TMP] Stage (4, 11, 2, 1) has been pruned...
[TMP] Stage (4, 11, 2, 2) has been pruned...
[TMP] Stage (4, 12, 2, 0) has been pruned...
[TMP] Stage (4, 12, 2, 1) has been pruned...
[TMP] Stage (4, 12, 2, 2) has been pruned...
[TMP] Stage (5, 11, 2, 0) has been pruned...
[TMP] Stage (5, 11, 2, 1) has been pruned...
[TMP] Stage (5, 11, 2, 2) has been pruned...
[TMP] Stage (5, 12, 2, 0) has been pruned...
[TMP] Stage (5, 12, 2, 1) has been pruned...
[TMP] Stage (5, 12, 2, 2) has been pruned...
[TMP] Stage (5, 13, 2, 0) has been pruned...
[TMP] Stage (5, 13, 2, 1) has been pruned...
[TMP] Stage (5, 13, 2, 2) has been pruned...
[TMP] Stage (5, 14, 2, 0) has been pruned...
[TMP] Stage (5, 14, 2, 1) has been pruned...
[TMP] Stage (5, 14, 2, 2) has been pruned...
[TMP] Stage (6, 12, 2, 0) has been pruned...
[TMP] Stage (6, 12, 2, 1) has been pruned...
[TMP] Stage (6, 12, 2, 2) has been pruned...
[TMP] Stage (6, 13, 2, 0) has been pruned...
[TMP] Stage (6, 13, 2, 1) has been pruned...
[TMP] Stage (6, 13, 2, 2) has been pruned...
[TMP] Stage (6, 14, 2, 0) has been pruned...
[TMP] Stage (6, 14, 2, 1) has been pruned...
[TMP] Stage (6, 14, 2, 2) has been pruned...
[TMP] Stage (6, 15, 2, 0) has been pruned...
[TMP] Stage (6, 15, 2, 1) has been pruned...
[TMP] Stage (6, 15, 2, 2) has been pruned...
[TMP] Stage (7, 13, 2, 0) has been pruned...
[TMP] Stage (7, 13, 2, 1) has been pruned...
[TMP] Stage (7, 13, 2, 2) has been pruned...
[TMP] Stage (7, 14, 2, 0) has been pruned...
[TMP] Stage (7, 14, 2, 1) has been pruned...
[TMP] Stage (7, 14, 2, 2) has been pruned...
[TMP] Stage (7, 15, 2, 0) has been pruned...
[TMP] Stage (7, 15, 2, 1) has been pruned...
[TMP] Stage (7, 15, 2, 2) has been pruned...
[TMP] Stage (8, 14, 2, 0) has been pruned...
[TMP] Stage (8, 14, 2, 1) has been pruned...
[TMP] Stage (8, 14, 2, 2) has been pruned...
[TMP] Stage (8, 15, 2, 0) has been pruned...
[TMP] Stage (8, 15, 2, 1) has been pruned...
[TMP] Stage (8, 15, 2, 2) has been pruned...
[TMP] Stage (9, 15, 2, 0) has been pruned...
[TMP] Stage (9, 15, 2, 1) has been pruned...
[TMP] Stage (9, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO comm 0x53105c0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO comm 0x4173bf0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO comm 0x3be9190 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO comm 0x989a4c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO comm 0x467d170 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO comm 0x92fa070 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2480381)[0m 
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2480381)[0m 
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2480381)[0m 
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO comm 0x5493ea0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO comm 0x9be33f0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2480382)[0m 
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2480381)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2480382)[0m 
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2480382)[0m 
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO comm 0x372e5e0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2480382)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2480381)[0m 
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO comm 0x853cbf0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO comm 0x4748fb0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2480382)[0m 
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO comm 0x936bc30 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1734870, ip=192.168.0.34)[0m gpu19:1734870:1734870 [0] NCCL INFO comm 0x3ef5da0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO comm 0x75391d0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 212.81 s
compilation time breakdown: {'stage-construction': '177.12', 'stage-construction-dp': '1.39', 'stage-construction-compilation': '56.76', 'stage-construction-profiling': '83.97'}
 - Compile (worker): 4.27 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1831874, ip=192.168.0.35)[0m gpu20:1831874:1831874 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1831873, ip=192.168.0.35)[0m gpu20:1831873:1831873 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3031639, ip=192.168.0.39)[0m gpu24:3031639:3031639 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3031640, ip=192.168.0.39)[0m gpu24:3031640:3031640 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2480381)[0m gpu16:2480381:2480381 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2480382)[0m gpu16:2480382:2480382 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1734871, ip=192.168.0.34)[0m gpu19:1734871:1734871 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 324.68 s

[226.7103831768036, 16.17347741127014, 15.980983972549438, 15.751156568527222, 16.064212560653687, 15.943326950073242, 16.10328197479248]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 80.927 s.
 - Average e2e iteration time: 16.185001373291016 s.
 - Total local training time: 79.84300231933594 s.
 - Average local iteration time: 15.969000816345215 s.
 - Max allocated memory among devices: 9.777 GB.
 - Compilation times:  {'stage-construction': 177.11798787117004, 'stage-construction-dp': 1.389388084411621, 'stage-construction-compilation': 56.757384061813354, 'stage-construction-profiling': 83.97409152984619}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `4_a40_4_n_2_d`: 15.968592643737793
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_1B_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling wide_resnet_1B with batch size: 512...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_1B_512.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 3, 1), 0] = ModuleProfileResult(compute_cost=0.142, peak_memory=2.778 GB, invar_size=1.885 GB, outvar_size=0.419 GB, temp_buffer_size=0.475 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 0] = ModuleProfileResult(compute_cost=0.059, peak_memory=4.832 GB, invar_size=3.749 GB, outvar_size=0.389 GB, temp_buffer_size=0.695 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 0] = ModuleProfileResult(compute_cost=6.288, peak_memory=2.651 GB, invar_size=1.664 GB, outvar_size=0.494 GB, temp_buffer_size=0.494 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 1] = ModuleProfileResult(compute_cost=5.985, peak_memory=12.743 GB, invar_size=7.884 GB, outvar_size=3.746 GB, temp_buffer_size=4.859 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 1] = ModuleProfileResult(compute_cost=2.839, peak_memory=6.706 GB, invar_size=4.185 GB, outvar_size=1.882 GB, temp_buffer_size=2.521 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 1] = ModuleProfileResult(compute_cost=7.757, peak_memory=5.476 GB, invar_size=3.818 GB, outvar_size=1.661 GB, temp_buffer_size=1.658 GB, available_memory=35.242 GB)
Profiling for submesh 3 (4, 2) takes 44.60 seconds
--------------------------------------------------
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.412 GB, invar_size=0.352 GB, outvar_size=0.479 GB, temp_buffer_size=0.582 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.596 GB, invar_size=0.397 GB, outvar_size=0.598 GB, temp_buffer_size=0.601 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.596 GB, invar_size=0.397 GB, outvar_size=0.598 GB, temp_buffer_size=0.601 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.422 GB, invar_size=0.099 GB, outvar_size=0.778 GB, temp_buffer_size=0.545 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.422 GB, invar_size=0.099 GB, outvar_size=0.778 GB, temp_buffer_size=0.545 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.246, peak_memory=2.352 GB, invar_size=0.558 GB, outvar_size=0.837 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.091 GB, invar_size=0.386 GB, outvar_size=0.359 GB, temp_buffer_size=0.346 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.402, peak_memory=3.752 GB, invar_size=1.474 GB, outvar_size=0.318 GB, temp_buffer_size=2.038 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.412 GB, invar_size=0.352 GB, outvar_size=0.479 GB, temp_buffer_size=0.582 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=2.691 GB, invar_size=1.062 GB, outvar_size=0.352 GB, temp_buffer_size=1.390 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.405 GB, invar_size=0.262 GB, outvar_size=0.598 GB, temp_buffer_size=0.545 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.218, peak_memory=2.210 GB, invar_size=0.535 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.795 GB, invar_size=1.154 GB, outvar_size=0.397 GB, temp_buffer_size=1.402 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.616 GB, invar_size=0.308 GB, outvar_size=0.718 GB, temp_buffer_size=0.591 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.092, peak_memory=1.556 GB, invar_size=0.552 GB, outvar_size=0.359 GB, temp_buffer_size=0.645 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.395, peak_memory=2.184 GB, invar_size=0.063 GB, outvar_size=1.137 GB, temp_buffer_size=0.985 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.795 GB, invar_size=1.154 GB, outvar_size=0.397 GB, temp_buffer_size=1.402 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.091 GB, invar_size=0.386 GB, outvar_size=0.359 GB, temp_buffer_size=0.346 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.405 GB, invar_size=0.262 GB, outvar_size=0.598 GB, temp_buffer_size=0.545 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.538, peak_memory=3.883 GB, invar_size=1.244 GB, outvar_size=0.045 GB, temp_buffer_size=2.639 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.334, peak_memory=3.586 GB, invar_size=1.309 GB, outvar_size=0.295 GB, temp_buffer_size=2.038 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.616 GB, invar_size=0.308 GB, outvar_size=0.718 GB, temp_buffer_size=0.591 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.290 GB, invar_size=0.432 GB, outvar_size=0.479 GB, temp_buffer_size=0.380 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.151 GB, invar_size=0.891 GB, outvar_size=0.386 GB, temp_buffer_size=1.020 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.312, peak_memory=2.222 GB, invar_size=0.400 GB, outvar_size=0.837 GB, temp_buffer_size=0.985 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=2.571 GB, invar_size=0.942 GB, outvar_size=0.352 GB, temp_buffer_size=1.390 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=2.988 GB, invar_size=0.966 GB, outvar_size=0.090 GB, temp_buffer_size=2.022 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=3.108 GB, invar_size=1.086 GB, outvar_size=0.090 GB, temp_buffer_size=2.022 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.340, peak_memory=2.388 GB, invar_size=0.423 GB, outvar_size=0.957 GB, temp_buffer_size=1.007 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.487, peak_memory=3.916 GB, invar_size=1.279 GB, outvar_size=0.400 GB, temp_buffer_size=2.278 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.248, peak_memory=2.681 GB, invar_size=0.984 GB, outvar_size=0.313 GB, temp_buffer_size=1.457 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.290 GB, invar_size=0.432 GB, outvar_size=0.479 GB, temp_buffer_size=0.380 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.339 GB, invar_size=0.477 GB, outvar_size=0.419 GB, temp_buffer_size=0.443 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=2.151 GB, invar_size=0.891 GB, outvar_size=0.386 GB, temp_buffer_size=1.020 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.554, peak_memory=4.082 GB, invar_size=1.444 GB, outvar_size=0.423 GB, temp_buffer_size=2.278 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.054 GB, invar_size=0.431 GB, outvar_size=0.299 GB, temp_buffer_size=0.324 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.339 GB, invar_size=0.477 GB, outvar_size=0.419 GB, temp_buffer_size=0.443 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=1.602 GB, invar_size=0.575 GB, outvar_size=0.479 GB, temp_buffer_size=0.549 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.054 GB, invar_size=0.431 GB, outvar_size=0.299 GB, temp_buffer_size=0.324 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.323 GB, invar_size=1.273 GB, outvar_size=0.308 GB, temp_buffer_size=1.870 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.964 GB, invar_size=0.943 GB, outvar_size=0.262 GB, temp_buffer_size=1.842 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.176, peak_memory=1.412 GB, invar_size=0.395 GB, outvar_size=0.538 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.174, peak_memory=1.435 GB, invar_size=0.418 GB, outvar_size=0.538 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=3.084 GB, invar_size=1.062 GB, outvar_size=0.262 GB, temp_buffer_size=1.842 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.218 GB, invar_size=0.614 GB, outvar_size=0.359 GB, temp_buffer_size=0.245 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.218 GB, invar_size=0.614 GB, outvar_size=0.359 GB, temp_buffer_size=0.245 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.355 GB, invar_size=0.752 GB, outvar_size=0.299 GB, temp_buffer_size=0.304 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=2.362 GB, invar_size=1.103 GB, outvar_size=0.432 GB, temp_buffer_size=1.021 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.315, peak_memory=2.846 GB, invar_size=1.149 GB, outvar_size=0.335 GB, temp_buffer_size=1.458 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.203 GB, invar_size=1.154 GB, outvar_size=0.308 GB, temp_buffer_size=1.870 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=1.935 GB, invar_size=1.042 GB, outvar_size=0.431 GB, temp_buffer_size=0.774 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.271, peak_memory=2.289 GB, invar_size=1.135 GB, outvar_size=0.298 GB, temp_buffer_size=1.034 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.355 GB, invar_size=0.752 GB, outvar_size=0.299 GB, temp_buffer_size=0.304 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=2.362 GB, invar_size=1.103 GB, outvar_size=0.432 GB, temp_buffer_size=1.021 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=2.146 GB, invar_size=1.253 GB, outvar_size=0.477 GB, temp_buffer_size=0.774 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.433 GB, invar_size=0.889 GB, outvar_size=0.239 GB, temp_buffer_size=0.304 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.214, peak_memory=2.243 GB, invar_size=1.089 GB, outvar_size=0.275 GB, temp_buffer_size=1.034 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.190, peak_memory=1.564 GB, invar_size=0.487 GB, outvar_size=0.598 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=1.995 GB, invar_size=1.101 GB, outvar_size=0.431 GB, temp_buffer_size=0.774 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=2.206 GB, invar_size=1.313 GB, outvar_size=0.477 GB, temp_buffer_size=0.774 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.433 GB, invar_size=0.889 GB, outvar_size=0.239 GB, temp_buffer_size=0.304 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.125, peak_memory=1.453 GB, invar_size=0.556 GB, outvar_size=0.419 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.250, peak_memory=2.486 GB, invar_size=1.332 GB, outvar_size=0.367 GB, temp_buffer_size=1.035 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.676 GB, invar_size=1.073 GB, outvar_size=0.299 GB, temp_buffer_size=0.305 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.217 GB, invar_size=0.793 GB, outvar_size=0.239 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=2.388 GB, invar_size=1.468 GB, outvar_size=0.614 GB, temp_buffer_size=0.801 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.060, peak_memory=1.167 GB, invar_size=0.624 GB, outvar_size=0.239 GB, temp_buffer_size=0.303 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.676 GB, invar_size=1.073 GB, outvar_size=0.299 GB, temp_buffer_size=0.305 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.208, peak_memory=2.214 GB, invar_size=1.290 GB, outvar_size=0.436 GB, temp_buffer_size=0.805 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.217 GB, invar_size=0.793 GB, outvar_size=0.239 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=2.448 GB, invar_size=1.528 GB, outvar_size=0.614 GB, temp_buffer_size=0.801 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.060, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.166, peak_memory=2.172 GB, invar_size=1.248 GB, outvar_size=0.504 GB, temp_buffer_size=0.805 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=3.019 GB, invar_size=1.898 GB, outvar_size=0.889 GB, temp_buffer_size=1.001 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.460 GB, invar_size=0.976 GB, outvar_size=0.299 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=2.786 GB, invar_size=1.683 GB, outvar_size=0.752 GB, temp_buffer_size=0.984 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.460 GB, invar_size=0.976 GB, outvar_size=0.299 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=2.786 GB, invar_size=1.683 GB, outvar_size=0.752 GB, temp_buffer_size=0.984 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.217 GB, invar_size=0.793 GB, outvar_size=0.239 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=3.019 GB, invar_size=1.898 GB, outvar_size=0.889 GB, temp_buffer_size=1.001 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.147, peak_memory=1.729 GB, invar_size=1.092 GB, outvar_size=0.426 GB, temp_buffer_size=0.577 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.217 GB, invar_size=0.793 GB, outvar_size=0.239 GB, temp_buffer_size=0.185 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.060, peak_memory=0.845 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.120 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=2.740 GB, invar_size=1.764 GB, outvar_size=0.793 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=2.740 GB, invar_size=1.764 GB, outvar_size=0.793 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.202, peak_memory=2.415 GB, invar_size=1.491 GB, outvar_size=0.596 GB, temp_buffer_size=0.805 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.475 GB, invar_size=1.672 GB, outvar_size=0.269 GB, temp_buffer_size=0.534 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.232 GB, invar_size=1.489 GB, outvar_size=0.210 GB, temp_buffer_size=0.534 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.182, peak_memory=1.972 GB, invar_size=1.335 GB, outvar_size=0.518 GB, temp_buffer_size=0.577 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.075, peak_memory=1.318 GB, invar_size=0.716 GB, outvar_size=0.299 GB, temp_buffer_size=0.303 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=1.935 GB, invar_size=1.233 GB, outvar_size=0.329 GB, temp_buffer_size=0.374 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.075, peak_memory=1.116 GB, invar_size=0.578 GB, outvar_size=0.299 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.058, peak_memory=1.692 GB, invar_size=1.049 GB, outvar_size=0.269 GB, temp_buffer_size=0.374 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.628 GB, invar_size=2.324 GB, outvar_size=1.072 GB, temp_buffer_size=1.185 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.350 GB, invar_size=2.191 GB, outvar_size=0.976 GB, temp_buffer_size=1.099 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.350 GB, invar_size=2.191 GB, outvar_size=0.976 GB, temp_buffer_size=1.099 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.628 GB, invar_size=2.324 GB, outvar_size=1.072 GB, temp_buffer_size=1.185 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.147, peak_memory=1.729 GB, invar_size=1.092 GB, outvar_size=0.426 GB, temp_buffer_size=0.577 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=2.740 GB, invar_size=1.764 GB, outvar_size=0.793 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=1.443 GB, invar_size=0.834 GB, outvar_size=0.269 GB, temp_buffer_size=0.340 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=2.740 GB, invar_size=1.764 GB, outvar_size=0.793 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.995 GB, invar_size=2.222 GB, outvar_size=0.240 GB, temp_buffer_size=0.534 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.751 GB, invar_size=2.038 GB, outvar_size=0.180 GB, temp_buffer_size=0.534 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.113, peak_memory=1.595 GB, invar_size=0.926 GB, outvar_size=0.329 GB, temp_buffer_size=0.340 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.072, peak_memory=2.089 GB, invar_size=1.416 GB, outvar_size=0.299 GB, temp_buffer_size=0.374 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.433 GB, invar_size=3.126 GB, outvar_size=1.488 GB, temp_buffer_size=2.247 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.071, peak_memory=1.846 GB, invar_size=1.233 GB, outvar_size=0.239 GB, temp_buffer_size=0.374 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=1.748 GB, invar_size=1.109 GB, outvar_size=0.239 GB, temp_buffer_size=0.399 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=6.042 GB, invar_size=3.553 GB, outvar_size=1.672 GB, temp_buffer_size=2.430 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.187, peak_memory=2.863 GB, invar_size=2.061 GB, outvar_size=0.866 GB, temp_buffer_size=0.743 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.151, peak_memory=2.620 GB, invar_size=1.818 GB, outvar_size=0.774 GB, temp_buffer_size=0.743 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.112, peak_memory=1.899 GB, invar_size=1.201 GB, outvar_size=0.299 GB, temp_buffer_size=0.399 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.280 GB, invar_size=2.627 GB, outvar_size=0.120 GB, temp_buffer_size=0.534 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.060, peak_memory=3.773 GB, invar_size=2.733 GB, outvar_size=1.232 GB, temp_buffer_size=0.980 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.056, peak_memory=3.174 GB, invar_size=2.307 GB, outvar_size=1.049 GB, temp_buffer_size=0.807 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.085, peak_memory=2.257 GB, invar_size=1.618 GB, outvar_size=0.240 GB, temp_buffer_size=0.399 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.523 GB, invar_size=2.810 GB, outvar_size=0.180 GB, temp_buffer_size=0.534 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.082, peak_memory=1.957 GB, invar_size=1.404 GB, outvar_size=0.180 GB, temp_buffer_size=0.374 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.112, peak_memory=2.112 GB, invar_size=1.495 GB, outvar_size=0.240 GB, temp_buffer_size=0.377 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.042, peak_memory=7.758 GB, invar_size=4.621 GB, outvar_size=2.221 GB, temp_buffer_size=3.077 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.082, peak_memory=1.957 GB, invar_size=1.404 GB, outvar_size=0.180 GB, temp_buffer_size=0.374 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.139, peak_memory=3.141 GB, invar_size=2.337 GB, outvar_size=1.049 GB, temp_buffer_size=0.745 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.038, peak_memory=7.149 GB, invar_size=4.195 GB, outvar_size=2.038 GB, temp_buffer_size=2.894 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.175, peak_memory=3.384 GB, invar_size=2.580 GB, outvar_size=1.140 GB, temp_buffer_size=0.745 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.076, peak_memory=3.419 GB, invar_size=2.643 GB, outvar_size=1.232 GB, temp_buffer_size=0.715 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=3.671 GB, invar_size=2.866 GB, outvar_size=1.343 GB, temp_buffer_size=0.746 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.049, peak_memory=8.940 GB, invar_size=5.312 GB, outvar_size=2.626 GB, temp_buffer_size=3.568 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.080, peak_memory=3.937 GB, invar_size=3.070 GB, outvar_size=1.415 GB, temp_buffer_size=0.807 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.128, peak_memory=3.671 GB, invar_size=2.866 GB, outvar_size=1.343 GB, temp_buffer_size=0.746 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.101, peak_memory=4.196 GB, invar_size=3.415 GB, outvar_size=1.618 GB, temp_buffer_size=0.721 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.053, peak_memory=9.549 GB, invar_size=5.738 GB, outvar_size=2.809 GB, temp_buffer_size=3.751 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.163, peak_memory=3.914 GB, invar_size=3.109 GB, outvar_size=1.435 GB, temp_buffer_size=0.746 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 40.79 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.075 GB, invar_size=0.042 GB, outvar_size=0.837 GB, temp_buffer_size=1.196 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.075 GB, invar_size=0.042 GB, outvar_size=0.837 GB, temp_buffer_size=1.196 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.288 GB, invar_size=0.331 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.290 GB, invar_size=0.643 GB, outvar_size=0.239 GB, temp_buffer_size=0.408 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.649 GB, invar_size=1.284 GB, outvar_size=0.642 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.343 GB, invar_size=0.505 GB, outvar_size=0.359 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.343 GB, invar_size=0.505 GB, outvar_size=0.359 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.543 GB, invar_size=0.387 GB, outvar_size=0.957 GB, temp_buffer_size=1.199 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.857 GB, invar_size=0.902 GB, outvar_size=0.023 GB, temp_buffer_size=3.955 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.473 GB, invar_size=1.129 GB, outvar_size=0.505 GB, temp_buffer_size=1.105 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.857 GB, invar_size=0.902 GB, outvar_size=0.023 GB, temp_buffer_size=3.955 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.473 GB, invar_size=1.129 GB, outvar_size=0.505 GB, temp_buffer_size=1.105 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.326 GB, invar_size=1.371 GB, outvar_size=0.386 GB, temp_buffer_size=3.596 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.484 GB, invar_size=0.901 GB, outvar_size=0.331 GB, temp_buffer_size=1.344 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.133 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.408 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.133 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.408 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.290 GB, invar_size=0.643 GB, outvar_size=0.239 GB, temp_buffer_size=0.408 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.980 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.769 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.220 GB, invar_size=0.545 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.649 GB, invar_size=1.284 GB, outvar_size=0.642 GB, temp_buffer_size=1.125 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.980 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.769 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.980 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.769 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.420 GB, invar_size=0.331 GB, outvar_size=0.479 GB, temp_buffer_size=0.610 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.980 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.769 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.861 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.650 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.420 GB, invar_size=0.331 GB, outvar_size=0.479 GB, temp_buffer_size=0.610 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.288 GB, invar_size=0.331 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.861 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.650 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.837 GB, invar_size=0.579 GB, outvar_size=0.479 GB, temp_buffer_size=0.779 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.965 GB, invar_size=0.486 GB, outvar_size=0.239 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.837 GB, invar_size=0.579 GB, outvar_size=0.479 GB, temp_buffer_size=0.779 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.220 GB, invar_size=0.545 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.861 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.650 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.484 GB, invar_size=0.901 GB, outvar_size=0.331 GB, temp_buffer_size=1.344 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.600 GB, invar_size=1.159 GB, outvar_size=0.579 GB, temp_buffer_size=1.963 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.600 GB, invar_size=1.159 GB, outvar_size=0.579 GB, temp_buffer_size=1.963 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.484 GB, invar_size=0.901 GB, outvar_size=0.331 GB, temp_buffer_size=1.344 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.484 GB, invar_size=0.901 GB, outvar_size=0.331 GB, temp_buffer_size=1.344 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.543 GB, invar_size=0.387 GB, outvar_size=0.957 GB, temp_buffer_size=1.199 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.861 GB, invar_size=1.092 GB, outvar_size=0.486 GB, temp_buffer_size=0.650 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.447 GB, invar_size=1.329 GB, outvar_size=0.545 GB, temp_buffer_size=2.639 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.990 GB, invar_size=1.182 GB, outvar_size=0.180 GB, temp_buffer_size=0.628 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.990 GB, invar_size=1.182 GB, outvar_size=0.180 GB, temp_buffer_size=0.628 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.530 GB, invar_size=1.732 GB, outvar_size=0.120 GB, temp_buffer_size=0.679 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.801 GB, invar_size=2.424 GB, outvar_size=1.182 GB, temp_buffer_size=1.258 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.447 GB, invar_size=1.329 GB, outvar_size=0.545 GB, temp_buffer_size=2.639 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.052 GB, invar_size=3.463 GB, outvar_size=1.731 GB, temp_buffer_size=1.469 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.530 GB, invar_size=1.732 GB, outvar_size=0.120 GB, temp_buffer_size=0.679 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.326 GB, invar_size=1.371 GB, outvar_size=0.386 GB, temp_buffer_size=3.596 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.801 GB, invar_size=2.424 GB, outvar_size=1.182 GB, temp_buffer_size=1.258 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.169 GB, invar_size=1.565 GB, outvar_size=0.060 GB, temp_buffer_size=0.544 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.169 GB, invar_size=1.565 GB, outvar_size=0.060 GB, temp_buffer_size=0.544 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.052 GB, invar_size=3.463 GB, outvar_size=1.731 GB, temp_buffer_size=1.469 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.305 GB, invar_size=3.128 GB, outvar_size=1.564 GB, temp_buffer_size=1.117 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.305 GB, invar_size=3.128 GB, outvar_size=1.564 GB, temp_buffer_size=1.117 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 17.57 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 2, 0) has been pruned...
[TMP] Stage (0, 6, 2, 1) has been pruned...
[TMP] Stage (0, 6, 2, 2) has been pruned...
[TMP] Stage (0, 7, 2, 0) has been pruned...
[TMP] Stage (0, 7, 2, 1) has been pruned...
[TMP] Stage (0, 7, 2, 2) has been pruned...
[TMP] Stage (0, 8, 2, 0) has been pruned...
[TMP] Stage (0, 8, 2, 1) has been pruned...
[TMP] Stage (0, 8, 2, 2) has been pruned...
[TMP] Stage (1, 6, 2, 0) has been pruned...
[TMP] Stage (1, 6, 2, 1) has been pruned...
[TMP] Stage (1, 6, 2, 2) has been pruned...
[TMP] Stage (1, 7, 2, 0) has been pruned...
[TMP] Stage (1, 7, 2, 1) has been pruned...
[TMP] Stage (1, 7, 2, 2) has been pruned...
[TMP] Stage (1, 8, 2, 0) has been pruned...
[TMP] Stage (1, 8, 2, 1) has been pruned...
[TMP] Stage (1, 8, 2, 2) has been pruned...
[TMP] Stage (1, 9, 2, 0) has been pruned...
[TMP] Stage (1, 9, 2, 1) has been pruned...
[TMP] Stage (1, 9, 2, 2) has been pruned...
[TMP] Stage (2, 7, 2, 0) has been pruned...
[TMP] Stage (2, 7, 2, 1) has been pruned...
[TMP] Stage (2, 7, 2, 2) has been pruned...
[TMP] Stage (2, 8, 2, 0) has been pruned...
[TMP] Stage (2, 8, 2, 1) has been pruned...
[TMP] Stage (2, 8, 2, 2) has been pruned...
[TMP] Stage (2, 9, 2, 0) has been pruned...
[TMP] Stage (2, 9, 2, 1) has been pruned...
[TMP] Stage (2, 9, 2, 2) has been pruned...
[TMP] Stage (2, 10, 2, 0) has been pruned...
[TMP] Stage (2, 10, 2, 1) has been pruned...
[TMP] Stage (2, 10, 2, 2) has been pruned...
[TMP] Stage (3, 9, 2, 0) has been pruned...
[TMP] Stage (3, 9, 2, 1) has been pruned...
[TMP] Stage (3, 9, 2, 2) has been pruned...
[TMP] Stage (3, 10, 2, 0) has been pruned...
[TMP] Stage (3, 10, 2, 1) has been pruned...
[TMP] Stage (3, 10, 2, 2) has been pruned...
[TMP] Stage (3, 11, 2, 0) has been pruned...
[TMP] Stage (3, 11, 2, 1) has been pruned...
[TMP] Stage (3, 11, 2, 2) has been pruned...
[TMP] Stage (4, 10, 2, 0) has been pruned...
[TMP] Stage (4, 10, 2, 1) has been pruned...
[TMP] Stage (4, 10, 2, 2) has been pruned...
[TMP] Stage (4, 11, 2, 0) has been pruned...
[TMP] Stage (4, 11, 2, 1) has been pruned...
[TMP] Stage (4, 11, 2, 2) has been pruned...
[TMP] Stage (4, 12, 2, 0) has been pruned...
[TMP] Stage (4, 12, 2, 1) has been pruned...
[TMP] Stage (4, 12, 2, 2) has been pruned...
[TMP] Stage (5, 11, 2, 0) has been pruned...
[TMP] Stage (5, 11, 2, 1) has been pruned...
[TMP] Stage (5, 11, 2, 2) has been pruned...
[TMP] Stage (5, 12, 2, 0) has been pruned...
[TMP] Stage (5, 12, 2, 1) has been pruned...
[TMP] Stage (5, 12, 2, 2) has been pruned...
[TMP] Stage (5, 13, 2, 0) has been pruned...
[TMP] Stage (5, 13, 2, 1) has been pruned...
[TMP] Stage (5, 13, 2, 2) has been pruned...
[TMP] Stage (5, 14, 2, 0) has been pruned...
[TMP] Stage (5, 14, 2, 1) has been pruned...
[TMP] Stage (5, 14, 2, 2) has been pruned...
[TMP] Stage (6, 12, 2, 0) has been pruned...
[TMP] Stage (6, 12, 2, 1) has been pruned...
[TMP] Stage (6, 12, 2, 2) has been pruned...
[TMP] Stage (6, 13, 2, 0) has been pruned...
[TMP] Stage (6, 13, 2, 1) has been pruned...
[TMP] Stage (6, 13, 2, 2) has been pruned...
[TMP] Stage (6, 14, 2, 0) has been pruned...
[TMP] Stage (6, 14, 2, 1) has been pruned...
[TMP] Stage (6, 14, 2, 2) has been pruned...
[TMP] Stage (6, 15, 2, 0) has been pruned...
[TMP] Stage (6, 15, 2, 1) has been pruned...
[TMP] Stage (6, 15, 2, 2) has been pruned...
[TMP] Stage (7, 13, 2, 0) has been pruned...
[TMP] Stage (7, 13, 2, 1) has been pruned...
[TMP] Stage (7, 13, 2, 2) has been pruned...
[TMP] Stage (7, 14, 2, 0) has been pruned...
[TMP] Stage (7, 14, 2, 1) has been pruned...
[TMP] Stage (7, 14, 2, 2) has been pruned...
[TMP] Stage (7, 15, 2, 0) has been pruned...
[TMP] Stage (7, 15, 2, 1) has been pruned...
[TMP] Stage (7, 15, 2, 2) has been pruned...
[TMP] Stage (8, 14, 2, 0) has been pruned...
[TMP] Stage (8, 14, 2, 1) has been pruned...
[TMP] Stage (8, 14, 2, 2) has been pruned...
[TMP] Stage (8, 15, 2, 0) has been pruned...
[TMP] Stage (8, 15, 2, 1) has been pruned...
[TMP] Stage (8, 15, 2, 2) has been pruned...
[TMP] Stage (9, 15, 2, 0) has been pruned...
[TMP] Stage (9, 15, 2, 1) has been pruned...
[TMP] Stage (9, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO comm 0x3d279e0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO comm 0x3ab97c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2494451)[0m 
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2494451)[0m 
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2494451)[0m 
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO comm 0x3d9ed80 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO comm 0x92fd8f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2494451)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2494452)[0m 
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2494451)[0m 
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO comm 0x6df1680 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2494452)[0m 
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2494452)[0m 
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO comm 0x3990a90 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2494452)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2494452)[0m 
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO comm 0x69e13e0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO comm 0x4fd4420 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO comm 0xa6f8cd0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO comm 0x3f08990 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO comm 0x5109750 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO comm 0x6a1d930 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO comm 0x7c1e690 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1839142, ip=192.168.0.35)[0m gpu20:1839142:1839142 [0] NCCL INFO comm 0x49590f0 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
 - Compile (driver): 126.01 s
compilation time breakdown: {'stage-construction': '105.29', 'stage-construction-dp': '1.40', 'stage-construction-compilation': '39.89', 'stage-construction-profiling': '30.08'}
 - Compile (worker): 4.45 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=3039921, ip=192.168.0.39)[0m gpu24:3039921:3039921 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3039920, ip=192.168.0.39)[0m gpu24:3039920:3039920 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2494451)[0m gpu16:2494451:2494451 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2494452)[0m gpu16:2494452:2494452 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1744737, ip=192.168.0.34)[0m gpu19:1744737:1744737 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1744738, ip=192.168.0.34)[0m gpu19:1744738:1744738 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1839141, ip=192.168.0.35)[0m gpu20:1839141:1839141 [0] NCCL INFO Launch mode Parallel
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 325.77 s

[80.89477443695068, 40.274056911468506, 40.158806562423706, 40.1966872215271, 40.18018841743469, 40.18431115150452, 40.21099495887756]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 203.084 s.
 - Average e2e iteration time: 40.617000579833984 s.
 - Total local training time: 200.93101501464844 s.
 - Average local iteration time: 40.18600082397461 s.
 - Max allocated memory among devices: 19.108 GB.
 - Compilation times:  {'stage-construction': 105.2873592376709, 'stage-construction-dp': 1.3958172798156738, 'stage-construction-compilation': 39.88636112213135, 'stage-construction-profiling': 30.07788372039795}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `4_a40_4_n_2_d`: 40.18619918823242
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/wide_resnet_1B_512.pkl`...

------------------------------------------------------------------
- (3/3) Profiling wide_resnet_1B with batch size: 1024...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/wide_resnet_1B_1024.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 320
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 3, 0), 0] = ModuleProfileResult(compute_cost=0.059, peak_memory=5.477 GB, invar_size=3.751 GB, outvar_size=0.778 GB, temp_buffer_size=0.949 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 0] = ModuleProfileResult(compute_cost=0.220, peak_memory=3.483 GB, invar_size=1.887 GB, outvar_size=0.867 GB, temp_buffer_size=0.729 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 0] = ModuleProfileResult(compute_cost=12.539, peak_memory=3.640 GB, invar_size=1.666 GB, outvar_size=0.987 GB, temp_buffer_size=0.988 GB, available_memory=35.242 GB)
result[(0, 15, 3, 0), 1] = ModuleProfileResult(compute_cost=5.985, peak_memory=13.266 GB, invar_size=8.275 GB, outvar_size=3.746 GB, temp_buffer_size=4.992 GB, available_memory=35.242 GB)
result[(0, 15, 3, 1), 1] = ModuleProfileResult(compute_cost=3.386, peak_memory=7.353 GB, invar_size=4.636 GB, outvar_size=1.882 GB, temp_buffer_size=2.718 GB, available_memory=35.242 GB)
result[(0, 15, 3, 2), 1] = ModuleProfileResult(compute_cost=13.258, peak_memory=5.994 GB, invar_size=4.314 GB, outvar_size=1.661 GB, temp_buffer_size=1.681 GB, available_memory=35.242 GB)
Profiling for submesh 3 (4, 2) takes 51.50 seconds
--------------------------------------------------
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.687 GB, invar_size=0.108 GB, outvar_size=1.555 GB, temp_buffer_size=1.023 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.912 GB, invar_size=0.637 GB, outvar_size=1.196 GB, temp_buffer_size=1.079 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.609 GB, invar_size=0.591 GB, outvar_size=0.957 GB, temp_buffer_size=1.061 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.687 GB, invar_size=0.108 GB, outvar_size=1.555 GB, temp_buffer_size=1.023 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.912 GB, invar_size=0.637 GB, outvar_size=1.196 GB, temp_buffer_size=1.079 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.491, peak_memory=4.625 GB, invar_size=1.036 GB, outvar_size=1.675 GB, temp_buffer_size=1.914 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.661 GB, invar_size=0.441 GB, outvar_size=1.196 GB, temp_buffer_size=1.023 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.953 GB, invar_size=0.625 GB, outvar_size=0.718 GB, temp_buffer_size=0.610 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.609 GB, invar_size=0.591 GB, outvar_size=0.957 GB, temp_buffer_size=1.061 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.435, peak_memory=4.363 GB, invar_size=1.013 GB, outvar_size=1.436 GB, temp_buffer_size=1.914 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.992 GB, invar_size=0.487 GB, outvar_size=1.436 GB, temp_buffer_size=1.069 GB, available_memory=35.242 GB)
result[(2, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=5.083 GB, invar_size=1.899 GB, outvar_size=0.591 GB, temp_buffer_size=2.705 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.787, peak_memory=4.296 GB, invar_size=0.081 GB, outvar_size=2.273 GB, temp_buffer_size=1.942 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.184, peak_memory=2.992 GB, invar_size=1.030 GB, outvar_size=0.718 GB, temp_buffer_size=1.244 GB, available_memory=35.242 GB)
result[(2, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=5.187 GB, invar_size=1.991 GB, outvar_size=0.637 GB, temp_buffer_size=2.718 GB, available_memory=35.242 GB)
result[(2, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.799, peak_memory=7.341 GB, invar_size=2.790 GB, outvar_size=0.558 GB, temp_buffer_size=4.072 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.953 GB, invar_size=0.625 GB, outvar_size=0.718 GB, temp_buffer_size=0.610 GB, available_memory=35.242 GB)
result[(0, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=1.073, peak_memory=7.669 GB, invar_size=2.399 GB, outvar_size=0.045 GB, temp_buffer_size=5.271 GB, available_memory=35.242 GB)
result[(0, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=5.748 GB, invar_size=1.753 GB, outvar_size=0.090 GB, temp_buffer_size=3.995 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.661 GB, invar_size=0.441 GB, outvar_size=1.196 GB, temp_buffer_size=1.023 GB, available_memory=35.242 GB)
result[(2, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.665, peak_memory=7.056 GB, invar_size=2.505 GB, outvar_size=0.535 GB, temp_buffer_size=4.072 GB, available_memory=35.242 GB)
result[(2, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=5.187 GB, invar_size=1.991 GB, outvar_size=0.637 GB, temp_buffer_size=2.718 GB, available_memory=35.242 GB)
result[(0, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=5.987 GB, invar_size=1.992 GB, outvar_size=0.090 GB, temp_buffer_size=3.995 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.623, peak_memory=4.376 GB, invar_size=0.759 GB, outvar_size=1.675 GB, temp_buffer_size=1.942 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.679, peak_memory=4.661 GB, invar_size=0.782 GB, outvar_size=1.914 GB, temp_buffer_size=1.964 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.247 GB, invar_size=0.671 GB, outvar_size=0.957 GB, temp_buffer_size=0.619 GB, available_memory=35.242 GB)
result[(3, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.492, peak_memory=5.193 GB, invar_size=1.821 GB, outvar_size=0.552 GB, temp_buffer_size=2.893 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.992 GB, invar_size=0.487 GB, outvar_size=1.436 GB, temp_buffer_size=1.069 GB, available_memory=35.242 GB)
result[(2, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.003, peak_memory=4.844 GB, invar_size=1.660 GB, outvar_size=0.591 GB, temp_buffer_size=2.705 GB, available_memory=35.242 GB)
result[(3, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.949 GB, invar_size=1.489 GB, outvar_size=0.625 GB, temp_buffer_size=1.981 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.247 GB, invar_size=0.671 GB, outvar_size=0.957 GB, temp_buffer_size=0.619 GB, available_memory=35.242 GB)
result[(1, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=6.313 GB, invar_size=2.290 GB, outvar_size=0.487 GB, temp_buffer_size=3.664 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.145 GB, invar_size=0.597 GB, outvar_size=0.838 GB, temp_buffer_size=0.711 GB, available_memory=35.242 GB)
result[(1, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=1.104, peak_memory=8.030 GB, invar_size=2.760 GB, outvar_size=0.782 GB, temp_buffer_size=4.552 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.628 GB, invar_size=0.551 GB, outvar_size=0.598 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=2.145 GB, invar_size=0.597 GB, outvar_size=0.838 GB, temp_buffer_size=0.711 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.240, peak_memory=3.038 GB, invar_size=1.053 GB, outvar_size=0.957 GB, temp_buffer_size=1.027 GB, available_memory=35.242 GB)
result[(3, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=3.949 GB, invar_size=1.489 GB, outvar_size=0.625 GB, temp_buffer_size=1.981 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.628 GB, invar_size=0.551 GB, outvar_size=0.598 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(1, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.971, peak_memory=7.744 GB, invar_size=2.475 GB, outvar_size=0.759 GB, temp_buffer_size=4.551 GB, available_memory=35.242 GB)
result[(1, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=5.955 GB, invar_size=1.959 GB, outvar_size=0.441 GB, temp_buffer_size=3.636 GB, available_memory=35.242 GB)
result[(1, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.002, peak_memory=5.715 GB, invar_size=1.720 GB, outvar_size=0.441 GB, temp_buffer_size=3.636 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.930 GB, invar_size=0.734 GB, outvar_size=0.718 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.347, peak_memory=2.668 GB, invar_size=0.634 GB, outvar_size=1.077 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(1, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.004, peak_memory=6.074 GB, invar_size=2.051 GB, outvar_size=0.487 GB, temp_buffer_size=3.664 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.997 GB, invar_size=0.872 GB, outvar_size=0.598 GB, temp_buffer_size=0.527 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.002, peak_memory=1.930 GB, invar_size=0.734 GB, outvar_size=0.718 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.344, peak_memory=2.691 GB, invar_size=0.657 GB, outvar_size=1.077 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(3, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=4.280 GB, invar_size=1.820 GB, outvar_size=0.671 GB, temp_buffer_size=1.981 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.997 GB, invar_size=0.872 GB, outvar_size=0.598 GB, temp_buffer_size=0.527 GB, available_memory=35.242 GB)
result[(3, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.625, peak_memory=5.479 GB, invar_size=2.106 GB, outvar_size=0.575 GB, temp_buffer_size=2.894 GB, available_memory=35.242 GB)
result[(3, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.005, peak_memory=4.280 GB, invar_size=1.820 GB, outvar_size=0.671 GB, temp_buffer_size=1.981 GB, available_memory=35.242 GB)
result[(4, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=3.464 GB, invar_size=1.791 GB, outvar_size=0.597 GB, temp_buffer_size=1.434 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.966 GB, invar_size=1.009 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(4, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.536, peak_memory=4.203 GB, invar_size=1.912 GB, outvar_size=0.418 GB, temp_buffer_size=2.052 GB, available_memory=35.242 GB)
result[(5, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.424, peak_memory=4.158 GB, invar_size=1.867 GB, outvar_size=0.395 GB, temp_buffer_size=2.052 GB, available_memory=35.242 GB)
result[(4, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.008, peak_memory=3.584 GB, invar_size=1.911 GB, outvar_size=0.597 GB, temp_buffer_size=1.434 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.375, peak_memory=2.880 GB, invar_size=0.726 GB, outvar_size=1.196 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(5, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=3.133 GB, invar_size=1.460 GB, outvar_size=0.551 GB, temp_buffer_size=1.434 GB, available_memory=35.242 GB)
result[(5, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.007, peak_memory=3.253 GB, invar_size=1.580 GB, outvar_size=0.551 GB, temp_buffer_size=1.434 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.966 GB, invar_size=1.009 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.248, peak_memory=2.589 GB, invar_size=0.795 GB, outvar_size=0.838 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.269 GB, invar_size=1.192 GB, outvar_size=0.598 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.619 GB, invar_size=0.853 GB, outvar_size=0.479 GB, temp_buffer_size=0.288 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.269 GB, invar_size=1.192 GB, outvar_size=0.598 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(5, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=3.751 GB, invar_size=2.066 GB, outvar_size=0.734 GB, temp_buffer_size=1.446 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.120, peak_memory=1.949 GB, invar_size=0.864 GB, outvar_size=0.479 GB, temp_buffer_size=0.607 GB, available_memory=35.242 GB)
result[(6, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=3.590 GB, invar_size=2.101 GB, outvar_size=0.871 GB, temp_buffer_size=1.250 GB, available_memory=35.242 GB)
result[(5, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.493, peak_memory=4.461 GB, invar_size=2.169 GB, outvar_size=0.487 GB, temp_buffer_size=2.052 GB, available_memory=35.242 GB)
result[(6, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.407, peak_memory=3.761 GB, invar_size=1.948 GB, outvar_size=0.555 GB, temp_buffer_size=1.573 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.619 GB, invar_size=0.853 GB, outvar_size=0.479 GB, temp_buffer_size=0.288 GB, available_memory=35.242 GB)
result[(5, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.010, peak_memory=3.632 GB, invar_size=1.946 GB, outvar_size=0.734 GB, temp_buffer_size=1.446 GB, available_memory=35.242 GB)
result[(6, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.013, peak_memory=3.590 GB, invar_size=2.101 GB, outvar_size=0.871 GB, temp_buffer_size=1.250 GB, available_memory=35.242 GB)
result[(7, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=3.746 GB, invar_size=2.256 GB, outvar_size=1.009 GB, temp_buffer_size=1.250 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.922 GB, invar_size=1.036 GB, outvar_size=0.598 GB, temp_buffer_size=0.288 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.571 GB, invar_size=0.853 GB, outvar_size=0.479 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.105, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.922 GB, invar_size=1.036 GB, outvar_size=0.598 GB, temp_buffer_size=0.288 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=1.571 GB, invar_size=0.853 GB, outvar_size=0.479 GB, temp_buffer_size=0.239 GB, available_memory=35.242 GB)
result[(8, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=3.099 GB, invar_size=2.064 GB, outvar_size=0.852 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(7, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.016, peak_memory=3.746 GB, invar_size=2.256 GB, outvar_size=1.009 GB, temp_buffer_size=1.250 GB, available_memory=35.242 GB)
result[(7, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.322, peak_memory=3.489 GB, invar_size=1.726 GB, outvar_size=0.624 GB, temp_buffer_size=1.524 GB, available_memory=35.242 GB)
result[(8, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.285, peak_memory=2.657 GB, invar_size=1.451 GB, outvar_size=0.486 GB, temp_buffer_size=1.087 GB, available_memory=35.242 GB)
result[(8, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=3.099 GB, invar_size=2.064 GB, outvar_size=0.852 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.595 GB, invar_size=1.549 GB, outvar_size=0.419 GB, temp_buffer_size=0.628 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=2.898 GB, invar_size=1.732 GB, outvar_size=0.539 GB, temp_buffer_size=0.628 GB, available_memory=35.242 GB)
result[(7, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=4.415 GB, invar_size=2.742 GB, outvar_size=1.192 GB, temp_buffer_size=1.434 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.391, peak_memory=3.792 GB, invar_size=2.029 GB, outvar_size=0.716 GB, temp_buffer_size=1.524 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.105, peak_memory=1.351 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.267 GB, available_memory=35.242 GB)
result[(7, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=4.415 GB, invar_size=2.742 GB, outvar_size=1.192 GB, temp_buffer_size=1.434 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=2.412 GB, invar_size=1.415 GB, outvar_size=0.539 GB, temp_buffer_size=0.459 GB, available_memory=35.242 GB)
result[(9, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=3.099 GB, invar_size=2.064 GB, outvar_size=0.852 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.353, peak_memory=2.960 GB, invar_size=1.753 GB, outvar_size=0.578 GB, temp_buffer_size=1.087 GB, available_memory=35.242 GB)
result[(7, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=2.160 GB, invar_size=0.955 GB, outvar_size=0.598 GB, temp_buffer_size=0.607 GB, available_memory=35.242 GB)
result[(8, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.768 GB, invar_size=2.549 GB, outvar_size=1.035 GB, temp_buffer_size=1.099 GB, available_memory=35.242 GB)
result[(9, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.285, peak_memory=2.657 GB, invar_size=1.451 GB, outvar_size=0.486 GB, temp_buffer_size=1.087 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.030, peak_memory=2.109 GB, invar_size=1.231 GB, outvar_size=0.419 GB, temp_buffer_size=0.459 GB, available_memory=35.242 GB)
result[(9, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.015, peak_memory=3.099 GB, invar_size=2.064 GB, outvar_size=0.852 GB, temp_buffer_size=0.916 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.388 GB, invar_size=2.281 GB, outvar_size=0.479 GB, temp_buffer_size=0.628 GB, available_memory=35.242 GB)
result[(8, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.133, peak_memory=1.898 GB, invar_size=0.698 GB, outvar_size=0.598 GB, temp_buffer_size=0.602 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.179, peak_memory=1.971 GB, invar_size=0.954 GB, outvar_size=0.539 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.143, peak_memory=2.602 GB, invar_size=1.476 GB, outvar_size=0.598 GB, temp_buffer_size=0.528 GB, available_memory=35.242 GB)
result[(10, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.028, peak_memory=5.762 GB, invar_size=3.396 GB, outvar_size=1.548 GB, temp_buffer_size=2.247 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.085 GB, invar_size=2.098 GB, outvar_size=0.359 GB, temp_buffer_size=0.628 GB, available_memory=35.242 GB)
result[(8, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.019, peak_memory=3.768 GB, invar_size=2.549 GB, outvar_size=1.035 GB, temp_buffer_size=1.099 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.142, peak_memory=2.299 GB, invar_size=1.292 GB, outvar_size=0.479 GB, temp_buffer_size=0.528 GB, available_memory=35.242 GB)
result[(9, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.032, peak_memory=6.431 GB, invar_size=3.881 GB, outvar_size=1.731 GB, temp_buffer_size=2.430 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.857 GB, invar_size=2.870 GB, outvar_size=0.359 GB, temp_buffer_size=0.628 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.353, peak_memory=3.716 GB, invar_size=2.509 GB, outvar_size=0.926 GB, temp_buffer_size=1.087 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.001, peak_memory=3.554 GB, invar_size=2.687 GB, outvar_size=0.240 GB, temp_buffer_size=0.628 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.214, peak_memory=2.355 GB, invar_size=1.229 GB, outvar_size=0.598 GB, temp_buffer_size=0.528 GB, available_memory=35.242 GB)
result[(9, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.207, peak_memory=2.182 GB, invar_size=1.046 GB, outvar_size=0.658 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(10, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.078, peak_memory=4.231 GB, invar_size=2.821 GB, outvar_size=1.231 GB, temp_buffer_size=1.290 GB, available_memory=35.242 GB)
result[(10, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.284, peak_memory=3.395 GB, invar_size=2.206 GB, outvar_size=0.834 GB, temp_buffer_size=1.069 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.170, peak_memory=2.685 GB, invar_size=1.678 GB, outvar_size=0.479 GB, temp_buffer_size=0.528 GB, available_memory=35.242 GB)
result[(9, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.082, peak_memory=4.782 GB, invar_size=3.307 GB, outvar_size=1.414 GB, temp_buffer_size=1.356 GB, available_memory=35.242 GB)
result[(11, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.038, peak_memory=7.448 GB, invar_size=4.434 GB, outvar_size=2.098 GB, temp_buffer_size=2.894 GB, available_memory=35.242 GB)
result[(10, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.042, peak_memory=8.117 GB, invar_size=4.920 GB, outvar_size=2.281 GB, temp_buffer_size=3.077 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.206, peak_memory=2.397 GB, invar_size=1.321 GB, outvar_size=0.598 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(11, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.272, peak_memory=4.068 GB, invar_size=2.816 GB, outvar_size=1.109 GB, temp_buffer_size=1.133 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.157, peak_memory=2.410 GB, invar_size=1.523 GB, outvar_size=0.359 GB, temp_buffer_size=0.528 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.149, peak_memory=2.502 GB, invar_size=1.615 GB, outvar_size=0.359 GB, temp_buffer_size=0.528 GB, available_memory=35.242 GB)
result[(11, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.123, peak_memory=4.073 GB, invar_size=2.943 GB, outvar_size=1.292 GB, temp_buffer_size=1.010 GB, available_memory=35.242 GB)
result[(10, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.127, peak_memory=4.559 GB, invar_size=3.429 GB, outvar_size=1.475 GB, temp_buffer_size=1.010 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.241, peak_memory=2.741 GB, invar_size=1.615 GB, outvar_size=0.598 GB, temp_buffer_size=0.528 GB, available_memory=35.242 GB)
result[(10, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.331, peak_memory=4.251 GB, invar_size=2.999 GB, outvar_size=1.200 GB, temp_buffer_size=1.133 GB, available_memory=35.242 GB)
result[(11, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.053, peak_memory=9.958 GB, invar_size=5.978 GB, outvar_size=2.869 GB, temp_buffer_size=3.861 GB, available_memory=35.242 GB)
result[(12, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.049, peak_memory=9.155 GB, invar_size=5.492 GB, outvar_size=2.686 GB, temp_buffer_size=3.543 GB, available_memory=35.242 GB)
result[(12, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.242, peak_memory=4.419 GB, invar_size=3.165 GB, outvar_size=1.403 GB, temp_buffer_size=1.134 GB, available_memory=35.242 GB)
result[(12, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.168, peak_memory=4.480 GB, invar_size=3.348 GB, outvar_size=1.495 GB, temp_buffer_size=1.012 GB, available_memory=35.242 GB)
result[(11, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.171, peak_memory=4.846 GB, invar_size=3.714 GB, outvar_size=1.678 GB, temp_buffer_size=1.012 GB, available_memory=35.242 GB)
result[(11, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.320, peak_memory=4.841 GB, invar_size=3.587 GB, outvar_size=1.495 GB, temp_buffer_size=1.134 GB, available_memory=35.242 GB)
Profiling for submesh 1 (1, 2) takes 40.23 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.198 GB, invar_size=0.882 GB, outvar_size=0.479 GB, temp_buffer_size=0.838 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.484 GB, invar_size=0.570 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.419 GB, invar_size=0.744 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(7, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.333 GB, invar_size=1.763 GB, outvar_size=0.882 GB, temp_buffer_size=2.091 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.419 GB, invar_size=0.744 GB, outvar_size=0.718 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.127 GB, invar_size=0.059 GB, outvar_size=1.675 GB, temp_buffer_size=2.393 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.127 GB, invar_size=0.059 GB, outvar_size=1.675 GB, temp_buffer_size=2.393 GB, available_memory=35.242 GB)
result[(6, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.388 GB, invar_size=1.728 GB, outvar_size=0.744 GB, temp_buffer_size=2.182 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.055 GB, invar_size=0.745 GB, outvar_size=1.914 GB, temp_buffer_size=2.396 GB, available_memory=35.242 GB)
result[(5, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.758 GB, invar_size=1.619 GB, outvar_size=0.570 GB, temp_buffer_size=2.660 GB, available_memory=35.242 GB)
result[(6, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.388 GB, invar_size=1.728 GB, outvar_size=0.744 GB, temp_buffer_size=2.182 GB, available_memory=35.242 GB)
result[(0, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.660 GB, invar_size=1.758 GB, outvar_size=0.023 GB, temp_buffer_size=7.902 GB, available_memory=35.242 GB)
result[(0, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.660 GB, invar_size=1.758 GB, outvar_size=0.023 GB, temp_buffer_size=7.902 GB, available_memory=35.242 GB)
result[(1, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=10.590 GB, invar_size=2.687 GB, outvar_size=0.745 GB, temp_buffer_size=7.186 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.198 GB, invar_size=0.882 GB, outvar_size=0.479 GB, temp_buffer_size=0.838 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.756 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.671 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.756 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.671 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.373 GB, invar_size=1.024 GB, outvar_size=1.436 GB, temp_buffer_size=1.914 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.736 GB, invar_size=0.570 GB, outvar_size=0.957 GB, temp_buffer_size=1.209 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.484 GB, invar_size=0.570 GB, outvar_size=0.957 GB, temp_buffer_size=0.957 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.736 GB, invar_size=0.570 GB, outvar_size=0.957 GB, temp_buffer_size=1.209 GB, available_memory=35.242 GB)
result[(7, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.333 GB, invar_size=1.763 GB, outvar_size=0.882 GB, temp_buffer_size=2.091 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.512 GB, invar_size=1.058 GB, outvar_size=0.957 GB, temp_buffer_size=1.497 GB, available_memory=35.242 GB)
result[(8, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.118 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.428 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(8, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.118 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.428 GB, available_memory=35.242 GB)
result[(9, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.118 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.428 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(9, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.118 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.428 GB, available_memory=35.242 GB)
result[(10, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.879 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.189 GB, available_memory=35.242 GB)
result[(10, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.879 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.189 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=1.563 GB, invar_size=0.606 GB, outvar_size=0.479 GB, temp_buffer_size=0.479 GB, available_memory=35.242 GB)
result[(5, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.758 GB, invar_size=1.619 GB, outvar_size=0.570 GB, temp_buffer_size=2.660 GB, available_memory=35.242 GB)
result[(4, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.758 GB, invar_size=1.619 GB, outvar_size=0.570 GB, temp_buffer_size=2.660 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.373 GB, invar_size=1.024 GB, outvar_size=1.436 GB, temp_buffer_size=1.914 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.512 GB, invar_size=1.058 GB, outvar_size=0.957 GB, temp_buffer_size=1.497 GB, available_memory=35.242 GB)
result[(4, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.758 GB, invar_size=1.619 GB, outvar_size=0.570 GB, temp_buffer_size=2.660 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.055 GB, invar_size=0.745 GB, outvar_size=1.914 GB, temp_buffer_size=2.396 GB, available_memory=35.242 GB)
result[(11, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.879 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.189 GB, available_memory=35.242 GB)
result[(3, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.958 GB, invar_size=2.116 GB, outvar_size=1.058 GB, temp_buffer_size=3.886 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.477 GB, invar_size=1.302 GB, outvar_size=0.359 GB, temp_buffer_size=0.816 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.477 GB, invar_size=1.302 GB, outvar_size=0.359 GB, temp_buffer_size=0.816 GB, available_memory=35.242 GB)
result[(11, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.879 GB, invar_size=1.450 GB, outvar_size=0.606 GB, temp_buffer_size=1.189 GB, available_memory=35.242 GB)
result[(2, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.754 GB, invar_size=2.525 GB, outvar_size=1.023 GB, temp_buffer_size=5.271 GB, available_memory=35.242 GB)
result[(2, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.754 GB, invar_size=2.525 GB, outvar_size=1.023 GB, temp_buffer_size=5.271 GB, available_memory=35.242 GB)
result[(3, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.958 GB, invar_size=2.116 GB, outvar_size=1.058 GB, temp_buffer_size=3.886 GB, available_memory=35.242 GB)
result[(12, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.709 GB, invar_size=2.723 GB, outvar_size=1.302 GB, temp_buffer_size=1.747 GB, available_memory=35.242 GB)
result[(1, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=10.590 GB, invar_size=2.687 GB, outvar_size=0.745 GB, temp_buffer_size=7.186 GB, available_memory=35.242 GB)
result[(12, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.709 GB, invar_size=2.723 GB, outvar_size=1.302 GB, temp_buffer_size=1.747 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.009 GB, invar_size=1.852 GB, outvar_size=0.240 GB, temp_buffer_size=0.918 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.393 GB, invar_size=1.624 GB, outvar_size=0.120 GB, temp_buffer_size=0.649 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.009 GB, invar_size=1.852 GB, outvar_size=0.240 GB, temp_buffer_size=0.918 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=2.393 GB, invar_size=1.624 GB, outvar_size=0.120 GB, temp_buffer_size=0.649 GB, available_memory=35.242 GB)
result[(13, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.751 GB, invar_size=3.702 GB, outvar_size=1.851 GB, temp_buffer_size=1.810 GB, available_memory=35.242 GB)
result[(13, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.751 GB, invar_size=3.702 GB, outvar_size=1.851 GB, temp_buffer_size=1.810 GB, available_memory=35.242 GB)
result[(14, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.545 GB, invar_size=3.248 GB, outvar_size=1.624 GB, temp_buffer_size=1.178 GB, available_memory=35.242 GB)
result[(14, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=4.545 GB, invar_size=3.248 GB, outvar_size=1.624 GB, temp_buffer_size=1.178 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 17.25 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 6, 2, 0) has been pruned...
[TMP] Stage (0, 6, 2, 1) has been pruned...
[TMP] Stage (0, 6, 2, 2) has been pruned...
[TMP] Stage (0, 7, 2, 0) has been pruned...
[TMP] Stage (0, 7, 2, 1) has been pruned...
[TMP] Stage (0, 7, 2, 2) has been pruned...
[TMP] Stage (0, 8, 2, 0) has been pruned...
[TMP] Stage (0, 8, 2, 1) has been pruned...
[TMP] Stage (0, 8, 2, 2) has been pruned...
[TMP] Stage (1, 6, 2, 0) has been pruned...
[TMP] Stage (1, 6, 2, 1) has been pruned...
[TMP] Stage (1, 6, 2, 2) has been pruned...
[TMP] Stage (1, 7, 2, 0) has been pruned...
[TMP] Stage (1, 7, 2, 1) has been pruned...
[TMP] Stage (1, 7, 2, 2) has been pruned...
[TMP] Stage (1, 8, 2, 0) has been pruned...
[TMP] Stage (1, 8, 2, 1) has been pruned...
[TMP] Stage (1, 8, 2, 2) has been pruned...
[TMP] Stage (1, 9, 2, 0) has been pruned...
[TMP] Stage (1, 9, 2, 1) has been pruned...
[TMP] Stage (1, 9, 2, 2) has been pruned...
[TMP] Stage (2, 7, 2, 0) has been pruned...
[TMP] Stage (2, 7, 2, 1) has been pruned...
[TMP] Stage (2, 7, 2, 2) has been pruned...
[TMP] Stage (2, 8, 2, 0) has been pruned...
[TMP] Stage (2, 8, 2, 1) has been pruned...
[TMP] Stage (2, 8, 2, 2) has been pruned...
[TMP] Stage (2, 9, 2, 0) has been pruned...
[TMP] Stage (2, 9, 2, 1) has been pruned...
[TMP] Stage (2, 9, 2, 2) has been pruned...
[TMP] Stage (2, 10, 2, 0) has been pruned...
[TMP] Stage (2, 10, 2, 1) has been pruned...
[TMP] Stage (2, 10, 2, 2) has been pruned...
[TMP] Stage (3, 9, 2, 0) has been pruned...
[TMP] Stage (3, 9, 2, 1) has been pruned...
[TMP] Stage (3, 9, 2, 2) has been pruned...
[TMP] Stage (3, 10, 2, 0) has been pruned...
[TMP] Stage (3, 10, 2, 1) has been pruned...
[TMP] Stage (3, 10, 2, 2) has been pruned...
[TMP] Stage (3, 11, 2, 0) has been pruned...
[TMP] Stage (3, 11, 2, 1) has been pruned...
[TMP] Stage (3, 11, 2, 2) has been pruned...
[TMP] Stage (4, 10, 2, 0) has been pruned...
[TMP] Stage (4, 10, 2, 1) has been pruned...
[TMP] Stage (4, 10, 2, 2) has been pruned...
[TMP] Stage (4, 11, 2, 0) has been pruned...
[TMP] Stage (4, 11, 2, 1) has been pruned...
[TMP] Stage (4, 11, 2, 2) has been pruned...
[TMP] Stage (4, 12, 2, 0) has been pruned...
[TMP] Stage (4, 12, 2, 1) has been pruned...
[TMP] Stage (4, 12, 2, 2) has been pruned...
[TMP] Stage (5, 11, 2, 0) has been pruned...
[TMP] Stage (5, 11, 2, 1) has been pruned...
[TMP] Stage (5, 11, 2, 2) has been pruned...
[TMP] Stage (5, 12, 2, 0) has been pruned...
[TMP] Stage (5, 12, 2, 1) has been pruned...
[TMP] Stage (5, 12, 2, 2) has been pruned...
[TMP] Stage (5, 13, 2, 0) has been pruned...
[TMP] Stage (5, 13, 2, 1) has been pruned...
[TMP] Stage (5, 13, 2, 2) has been pruned...
[TMP] Stage (5, 14, 2, 0) has been pruned...
[TMP] Stage (5, 14, 2, 1) has been pruned...
[TMP] Stage (5, 14, 2, 2) has been pruned...
[TMP] Stage (6, 12, 2, 0) has been pruned...
[TMP] Stage (6, 12, 2, 1) has been pruned...
[TMP] Stage (6, 12, 2, 2) has been pruned...
[TMP] Stage (6, 13, 2, 0) has been pruned...
[TMP] Stage (6, 13, 2, 1) has been pruned...
[TMP] Stage (6, 13, 2, 2) has been pruned...
[TMP] Stage (6, 14, 2, 0) has been pruned...
[TMP] Stage (6, 14, 2, 1) has been pruned...
[TMP] Stage (6, 14, 2, 2) has been pruned...
[TMP] Stage (6, 15, 2, 0) has been pruned...
[TMP] Stage (6, 15, 2, 1) has been pruned...
[TMP] Stage (6, 15, 2, 2) has been pruned...
[TMP] Stage (7, 13, 2, 0) has been pruned...
[TMP] Stage (7, 13, 2, 1) has been pruned...
[TMP] Stage (7, 13, 2, 2) has been pruned...
[TMP] Stage (7, 14, 2, 0) has been pruned...
[TMP] Stage (7, 14, 2, 1) has been pruned...
[TMP] Stage (7, 14, 2, 2) has been pruned...
[TMP] Stage (7, 15, 2, 0) has been pruned...
[TMP] Stage (7, 15, 2, 1) has been pruned...
[TMP] Stage (7, 15, 2, 2) has been pruned...
[TMP] Stage (8, 14, 2, 0) has been pruned...
[TMP] Stage (8, 14, 2, 1) has been pruned...
[TMP] Stage (8, 14, 2, 2) has been pruned...
[TMP] Stage (8, 15, 2, 0) has been pruned...
[TMP] Stage (8, 15, 2, 1) has been pruned...
[TMP] Stage (8, 15, 2, 2) has been pruned...
[TMP] Stage (9, 15, 2, 0) has been pruned...
[TMP] Stage (9, 15, 2, 1) has been pruned...
[TMP] Stage (9, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result logical_mesh_shapes: [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}]
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2508306)[0m 
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2508307)[0m 
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2508307)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2508306)[0m 
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2508306)[0m 
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO comm 0x53dd430 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2508307)[0m 
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2508307)[0m 
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2508307)[0m gpu16:2508307:2508307 [0] NCCL INFO comm 0x474eca0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2508306)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO comm 0x3ce6560 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2508306)[0m 
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2508306)[0m gpu16:2508306:2508306 [0] NCCL INFO comm 0x8429bf0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3048074, ip=192.168.0.39)[0m gpu24:3048074:3048074 [0] NCCL INFO comm 0x5cdcac0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO comm 0x3d1eef0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3048073, ip=192.168.0.39)[0m gpu24:3048073:3048073 [0] NCCL INFO comm 0x9289ed0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO comm 0x3ee6190 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.34<0>
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Channel 00 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Channel 01 : 1[31000] -> 0[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO comm 0x354c350 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Channel 00 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Channel 01 : 0[98000] -> 1[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1846772, ip=192.168.0.35)[0m gpu20:1846772:1846772 [0] NCCL INFO comm 0x6cd6fd0 rank 0 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m 
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1846771, ip=192.168.0.35)[0m gpu20:1846771:1846771 [0] NCCL INFO comm 0x8ab76f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Setting affinity for GPU 0 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Channel 00 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Channel 01 : 1[98000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO comm 0x4b49480 rank 1 nranks 2 cudaDev 0 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.33<0>
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1754533, ip=192.168.0.34)[0m gpu19:1754533:1754533 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m 
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1754534, ip=192.168.0.34)[0m gpu19:1754534:1754534 [0] NCCL INFO Channel 00/02 :    0   1
