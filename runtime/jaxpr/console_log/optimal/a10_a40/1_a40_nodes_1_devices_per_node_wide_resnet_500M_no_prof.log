
------------------------------------------------------------------
- (1/3) Profiling wide_resnet_500M with batch size: 256...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal_no_prof/wide_resnet_500M_256.pkl`, creating it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1),)
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.148, peak_memory=4.141 GB, invar_size=1.855 GB, outvar_size=1.089 GB, temp_buffer_size=1.198 GB, available_memory=35.242 GB)
result[(0, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.419, peak_memory=6.332 GB, invar_size=4.786 GB, outvar_size=1.844 GB, temp_buffer_size=1.546 GB, available_memory=35.242 GB)
result[(0, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.148, peak_memory=4.141 GB, invar_size=1.855 GB, outvar_size=1.089 GB, temp_buffer_size=1.198 GB, available_memory=35.242 GB)
result[(0, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.419, peak_memory=6.332 GB, invar_size=4.786 GB, outvar_size=1.844 GB, temp_buffer_size=1.546 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 66.46 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 1)]
Result logical_mesh_shapes: [(1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 85.85 s
compilation time breakdown: {'stage-construction': '68.65', 'stage-construction-dp': '1.36', 'stage-construction-compilation': '2.69', 'stage-construction-profiling': '60.27'}
 - Compile (worker): 12.61 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 122.03 s

[18.549917936325073, 15.97914457321167, 15.936455488204956, 15.9162118434906, 15.92281985282898, 15.913542985916138, 15.907320499420166]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 84.984 s.
 - Average e2e iteration time: 16.99700164794922 s.
 - Total local training time: 79.59600067138672 s.
 - Average local iteration time: 15.919000625610352 s.
 - Max allocated memory among devices: 24.519 GB.
 - Compilation times:  {'stage-construction': 68.65145921707153, 'stage-construction-dp': 1.3637328147888184, 'stage-construction-compilation': 2.6913864612579346, 'stage-construction-profiling': 60.269240617752075}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `1_a40_1_n_1_d`: 15.919270515441895
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal_no_prof/wide_resnet_500M_256.pkl`...

------------------------------------------------------------------
- (2/3) Profiling wide_resnet_500M with batch size: 512...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal_no_prof/wide_resnet_500M_512.pkl`, creating it...
[I] Alpa's built-in profiling database is disabled.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Compiling and executing model with timeout = 1200 s...
[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1),)
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.238, peak_memory=5.680 GB, invar_size=1.864 GB, outvar_size=2.178 GB, temp_buffer_size=1.640 GB, available_memory=35.242 GB)
result[(0, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.704, peak_memory=8.827 GB, invar_size=5.883 GB, outvar_size=1.844 GB, temp_buffer_size=2.944 GB, available_memory=35.242 GB)
result[(0, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.236, peak_memory=5.680 GB, invar_size=1.864 GB, outvar_size=2.178 GB, temp_buffer_size=1.640 GB, available_memory=35.242 GB)
result[(0, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.690, peak_memory=8.827 GB, invar_size=5.883 GB, outvar_size=1.844 GB, temp_buffer_size=2.944 GB, available_memory=35.242 GB)
Profiling for submesh 0 (1, 1) takes 88.08 seconds
--------------------------------------------------
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 1)]
Result logical_mesh_shapes: [(1, 1)]
Result autosharding_option_dicts: [{}]
 - Compile (driver): 106.94 s
compilation time breakdown: {'stage-construction': '90.26', 'stage-construction-dp': '1.32', 'stage-construction-compilation': '2.73', 'stage-construction-profiling': '82.01'}
 - Compile (worker): 23.92 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[E] Meet unexpected error in profiling executables...

[E] Unexpected error occurred in compiling or profiling executables...

------------------------------------------------------------------
- (3/3) Profiling wide_resnet_500M with batch size: 1024...
------------------------------------------------------------------
