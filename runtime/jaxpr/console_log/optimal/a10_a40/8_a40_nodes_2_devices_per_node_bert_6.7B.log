
------------------------------------------------------------------
- (1/3) Profiling bert_6.7B with batch size: 128...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_6.7B_128.pkl`, updating/rewriting it...
[WARN] Local batch size 8 is not divisible by num devices 16, skipping...
[TMP] Current profiling results of key `8_a40_8_n_2_d`: none
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_6.7B_128.pkl`...

------------------------------------------------------------------
- (2/3) Profiling bert_6.7B with batch size: 256...
------------------------------------------------------------------
[TMP] Existed profiling results in `./jaxpr/prof_log/optimal/bert_6.7B_256.pkl`, updating/rewriting it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7feb5884d7f0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.114, peak_memory=2.625 GB, invar_size=1.813 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.250, peak_memory=1.875 GB, invar_size=0.875 GB, outvar_size=0.250 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.250, peak_memory=1.875 GB, invar_size=0.875 GB, outvar_size=0.250 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.114, peak_memory=2.625 GB, invar_size=1.813 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.250, peak_memory=1.875 GB, invar_size=0.875 GB, outvar_size=0.250 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.284, peak_memory=2.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.250, peak_memory=1.875 GB, invar_size=0.875 GB, outvar_size=0.250 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.114, peak_memory=2.625 GB, invar_size=1.813 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.237, peak_memory=1.813 GB, invar_size=0.875 GB, outvar_size=0.188 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.108, peak_memory=2.551 GB, invar_size=1.645 GB, outvar_size=0.125 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.284, peak_memory=2.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.246, peak_memory=1.791 GB, invar_size=0.823 GB, outvar_size=0.188 GB, temp_buffer_size=0.781 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.127, peak_memory=2.479 GB, invar_size=1.448 GB, outvar_size=0.125 GB, temp_buffer_size=0.906 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.114, peak_memory=2.625 GB, invar_size=1.813 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.108, peak_memory=2.500 GB, invar_size=1.688 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.108, peak_memory=2.500 GB, invar_size=1.688 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.108, peak_memory=2.500 GB, invar_size=1.688 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.265, peak_memory=1.875 GB, invar_size=0.938 GB, outvar_size=0.188 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.108, peak_memory=2.500 GB, invar_size=1.688 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=0.598, peak_memory=7.525 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.525 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.265, peak_memory=1.875 GB, invar_size=0.938 GB, outvar_size=0.188 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.237, peak_memory=1.813 GB, invar_size=0.875 GB, outvar_size=0.188 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=0.598, peak_memory=7.653 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.653 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.237, peak_memory=1.813 GB, invar_size=0.875 GB, outvar_size=0.188 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=7.504 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.253 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=7.442 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.191 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=0.582, peak_memory=7.700 GB, invar_size=1.895 GB, outvar_size=0.823 GB, temp_buffer_size=5.804 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=7.504 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.253 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=7.442 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.191 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=0.656, peak_memory=8.026 GB, invar_size=2.126 GB, outvar_size=1.000 GB, temp_buffer_size=5.775 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.314, peak_memory=7.773 GB, invar_size=3.415 GB, outvar_size=1.645 GB, temp_buffer_size=4.358 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=7.257 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.006 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.344, peak_memory=8.567 GB, invar_size=3.688 GB, outvar_size=1.813 GB, temp_buffer_size=4.816 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.302, peak_memory=7.504 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.253 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=7.257 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.006 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.352, peak_memory=8.629 GB, invar_size=3.751 GB, outvar_size=1.813 GB, temp_buffer_size=4.816 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.352, peak_memory=8.504 GB, invar_size=3.688 GB, outvar_size=1.813 GB, temp_buffer_size=4.753 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.335, peak_memory=7.438 GB, invar_size=3.020 GB, outvar_size=1.448 GB, temp_buffer_size=4.417 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=7.442 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.191 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=0.598, peak_memory=7.525 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.524 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=0.571, peak_memory=7.519 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.518 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=0.598, peak_memory=7.525 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.525 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=0.630, peak_memory=8.396 GB, invar_size=2.126 GB, outvar_size=1.000 GB, temp_buffer_size=6.145 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.310, peak_memory=7.442 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.191 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.563 GB, invar_size=0.813 GB, outvar_size=0.062 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.125, peak_memory=1.375 GB, invar_size=0.500 GB, outvar_size=0.125 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.237, peak_memory=1.813 GB, invar_size=0.875 GB, outvar_size=0.188 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.049, peak_memory=1.563 GB, invar_size=0.813 GB, outvar_size=0.062 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.344, peak_memory=8.629 GB, invar_size=3.751 GB, outvar_size=1.813 GB, temp_buffer_size=4.816 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.099, peak_memory=2.375 GB, invar_size=1.563 GB, outvar_size=0.125 GB, temp_buffer_size=0.688 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.302, peak_memory=7.504 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.253 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=0.642, peak_memory=7.879 GB, invar_size=2.001 GB, outvar_size=0.938 GB, temp_buffer_size=5.753 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.336, peak_memory=8.879 GB, invar_size=3.438 GB, outvar_size=1.688 GB, temp_buffer_size=5.378 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.237, peak_memory=1.813 GB, invar_size=0.875 GB, outvar_size=0.188 GB, temp_buffer_size=0.750 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=0.556, peak_memory=7.385 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.384 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.336, peak_memory=9.004 GB, invar_size=3.501 GB, outvar_size=1.688 GB, temp_buffer_size=5.441 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=0.621, peak_memory=8.896 GB, invar_size=2.001 GB, outvar_size=0.938 GB, temp_buffer_size=6.770 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.334, peak_memory=8.942 GB, invar_size=3.438 GB, outvar_size=1.688 GB, temp_buffer_size=5.441 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.306, peak_memory=7.378 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.306, peak_memory=7.440 GB, invar_size=3.251 GB, outvar_size=1.563 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.334, peak_memory=9.004 GB, invar_size=3.501 GB, outvar_size=1.688 GB, temp_buffer_size=5.441 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.306, peak_memory=7.378 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=0.556, peak_memory=7.385 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.384 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=0.556, peak_memory=7.385 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.384 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.306, peak_memory=7.440 GB, invar_size=3.251 GB, outvar_size=1.563 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.306, peak_memory=7.440 GB, invar_size=3.251 GB, outvar_size=1.563 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.306, peak_memory=7.378 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=0.657, peak_memory=7.507 GB, invar_size=2.048 GB, outvar_size=1.024 GB, temp_buffer_size=5.335 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.308, peak_memory=7.378 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.308, peak_memory=7.378 GB, invar_size=3.188 GB, outvar_size=1.563 GB, temp_buffer_size=4.127 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=0.577, peak_memory=7.399 GB, invar_size=1.876 GB, outvar_size=0.875 GB, temp_buffer_size=5.398 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.366, peak_memory=8.034 GB, invar_size=3.720 GB, outvar_size=1.860 GB, temp_buffer_size=4.252 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.387, peak_memory=7.706 GB, invar_size=3.329 GB, outvar_size=1.664 GB, temp_buffer_size=4.314 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 34.72 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(7, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(3, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.130, peak_memory=2.625 GB, invar_size=1.125 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(3, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.130, peak_memory=2.625 GB, invar_size=1.125 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(7, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.261 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.385 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=0.117, peak_memory=2.598 GB, invar_size=0.895 GB, outvar_size=0.125 GB, temp_buffer_size=1.578 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=0.117, peak_memory=2.598 GB, invar_size=0.895 GB, outvar_size=0.125 GB, temp_buffer_size=1.578 GB, available_memory=35.446 GB)
result[(1, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(1, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(7, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.261 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.385 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.116, peak_memory=2.500 GB, invar_size=1.000 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=9.886 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.010 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.280, peak_memory=10.257 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.381 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.280, peak_memory=10.257 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.381 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=9.886 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.010 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.261 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.385 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.261 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.385 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.116, peak_memory=2.500 GB, invar_size=1.000 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(3, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.374, peak_memory=11.882 GB, invar_size=2.251 GB, outvar_size=1.125 GB, temp_buffer_size=9.506 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(3, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.374, peak_memory=11.882 GB, invar_size=2.251 GB, outvar_size=1.125 GB, temp_buffer_size=9.506 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.300, peak_memory=10.628 GB, invar_size=1.915 GB, outvar_size=0.895 GB, temp_buffer_size=8.713 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=0.300, peak_memory=10.628 GB, invar_size=1.915 GB, outvar_size=0.895 GB, temp_buffer_size=8.713 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(1, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=9.886 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.010 GB, available_memory=35.446 GB)
result[(1, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=9.886 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.010 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.261 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.385 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.261 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.385 GB, available_memory=35.446 GB)
result[(13, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(13, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=10.004 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.128 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.345, peak_memory=12.882 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=10.756 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.507 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.631 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=10.004 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.128 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.345, peak_memory=12.882 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=10.756 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.507 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.631 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=10.004 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.128 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=10.004 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.128 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=10.004 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.128 GB, available_memory=35.446 GB)
result[(13, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=10.004 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.128 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=10.004 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.128 GB, available_memory=35.446 GB)
result[(13, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.292, peak_memory=10.004 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.128 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.132 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.256 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.401, peak_memory=10.848 GB, invar_size=2.219 GB, outvar_size=1.172 GB, temp_buffer_size=8.504 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.401, peak_memory=10.848 GB, invar_size=2.219 GB, outvar_size=1.172 GB, temp_buffer_size=8.504 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.295, peak_memory=10.132 GB, invar_size=1.750 GB, outvar_size=0.875 GB, temp_buffer_size=8.256 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 18.40 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}, {}, {}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=722393)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO comm 0x519f810 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO comm 0x407fce0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO comm 0x856b530 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO comm 0x98b6c90 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO comm 0xb5b76e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO comm 0x444d9f0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO comm 0x675b070 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO comm 0xb54e5a0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO comm 0x454f570 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO comm 0xbb44910 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO comm 0x640a620 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO comm 0xbb4f840 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO comm 0x4deeaf0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO comm 0xa64d970 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO comm 0x815a710 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO comm 0xa9ba290 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO comm 0xb1a89e0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO comm 0x45ae0a0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO comm 0x79a6800 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO comm 0xb13f8a0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO comm 0xa9f48d0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO comm 0x53f78c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO comm 0xa98b790 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO comm 0x76c7070 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO comm 0xcab1bc0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [0] NCCL INFO comm 0x527c760 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO comm 0xc95c480 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547720 [1] NCCL INFO comm 0x72c9050 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 110.68 s
compilation time breakdown: {'stage-construction': '57.52', 'stage-construction-dp': '1.48', 'stage-construction-compilation': '10.70', 'stage-construction-profiling': '12.17'}
 - Compile (worker): 4.83 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722393 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509276 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680447 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001301 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188757 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386666 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311535 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO comm 0x7fd03198b260 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547952 [1] NCCL INFO comm 0x7fd025a29cb0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2547720, ip=192.168.0.38)[0m gpu23:2547720:2547950 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO comm 0x7f687f9aa1f0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311683 [1] NCCL INFO comm 0x7f6877a0ec00 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3311535, ip=192.168.0.31)[0m gpu16:3311535:3311681 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO comm 0x7f738dcc4c40 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386730 [1] NCCL INFO comm 0x7f7391a60050 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2386666, ip=192.168.0.27)[0m gpu12:2386666:2386728 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188905 [1] NCCL INFO comm 0x7f26b642daf0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO comm 0x7f26b283d630 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2188757, ip=192.168.0.18)[0m gpu3:2188757:2188903 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001365 [1] NCCL INFO comm 0x7f016f2a9c50 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO comm 0x7f016a4fafe0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680512 [1] NCCL INFO comm 0x7f67416573b0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO comm 0x7f673bcd2190 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509340 [1] NCCL INFO comm 0x7ee55d1c0d20 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO comm 0x7ee568f372b0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1509276, ip=192.168.0.32)[0m gpu17:1509276:1509338 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=722393)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=722393)[0m 
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO comm 0x7f9188dce8c0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722477 [1] NCCL INFO comm 0x7f91826e4370 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1001301, ip=192.168.0.26)[0m gpu11:1001301:1001363 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3680447, ip=192.168.0.39)[0m gpu24:3680447:3680510 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=722393)[0m gpu2:722393:722475 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 204.88 s

[68.973069190979, 21.04797124862671, 20.965161085128784, 20.982767820358276, 20.970932960510254, 20.884608507156372, 20.976133584976196]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 109.157 s.
 - Average e2e iteration time: 21.83100128173828 s.
 - Total local training time: 104.7800064086914 s.
 - Average local iteration time: 20.95600128173828 s.
 - Max allocated memory among devices: 15.818 GB.
 - Compilation times:  {'stage-construction': 57.51609969139099, 'stage-construction-dp': 1.479759693145752, 'stage-construction-compilation': 10.700306177139282, 'stage-construction-profiling': 12.169451236724854}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 20.955921173095703
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_6.7B_256.pkl`...

------------------------------------------------------------------
- (3/3) Profiling bert_6.7B with batch size: 512...
------------------------------------------------------------------
[TMP] Profiling results not found in `./jaxpr/prof_log/optimal/bert_6.7B_512.pkl`, creating it...
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 8
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fbc07665820>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1), (1, 2), (2, 2), (4, 2))
- Profiling for submesh 3 (4, 2):
[TMP] Skip profiling of 4 due to legacy error in tensorflow...
- Profiling for submesh 2 (2, 2):
[TMP] Skip profiling of 2 due to legacy error in tensorflow...
- Profiling for submesh 1 (1, 2):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(4, 5, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.251 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 0] = ModuleProfileResult(compute_cost=0.215, peak_memory=3.458 GB, invar_size=1.645 GB, outvar_size=0.250 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.251 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 0] = ModuleProfileResult(compute_cost=0.491, peak_memory=2.760 GB, invar_size=0.823 GB, outvar_size=0.375 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 0] = ModuleProfileResult(compute_cost=0.499, peak_memory=3.000 GB, invar_size=1.000 GB, outvar_size=0.500 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.251 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 0] = ModuleProfileResult(compute_cost=0.499, peak_memory=3.000 GB, invar_size=1.000 GB, outvar_size=0.500 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 0] = ModuleProfileResult(compute_cost=0.499, peak_memory=3.000 GB, invar_size=1.000 GB, outvar_size=0.500 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.251 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 0] = ModuleProfileResult(compute_cost=0.499, peak_memory=3.000 GB, invar_size=1.000 GB, outvar_size=0.500 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 0] = ModuleProfileResult(compute_cost=0.228, peak_memory=3.501 GB, invar_size=1.875 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 0] = ModuleProfileResult(compute_cost=0.472, peak_memory=2.875 GB, invar_size=1.000 GB, outvar_size=0.375 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 0] = ModuleProfileResult(compute_cost=0.568, peak_memory=3.125 GB, invar_size=1.125 GB, outvar_size=0.500 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 0] = ModuleProfileResult(compute_cost=0.228, peak_memory=3.501 GB, invar_size=1.875 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 0] = ModuleProfileResult(compute_cost=0.228, peak_memory=3.501 GB, invar_size=1.875 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(5, 6, 1, 1), 1] = ModuleProfileResult(compute_cost=1.194, peak_memory=13.792 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=11.292 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 0] = ModuleProfileResult(compute_cost=0.568, peak_memory=3.125 GB, invar_size=1.125 GB, outvar_size=0.500 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 0] = ModuleProfileResult(compute_cost=0.215, peak_memory=3.458 GB, invar_size=1.645 GB, outvar_size=0.250 GB, temp_buffer_size=1.563 GB, available_memory=35.446 GB)
result[(0, 1, 1, 0), 1] = ModuleProfileResult(compute_cost=0.611, peak_memory=12.254 GB, invar_size=3.540 GB, outvar_size=1.645 GB, temp_buffer_size=8.714 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 0] = ModuleProfileResult(compute_cost=0.228, peak_memory=3.501 GB, invar_size=1.875 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 0] = ModuleProfileResult(compute_cost=0.215, peak_memory=3.376 GB, invar_size=1.750 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 0] = ModuleProfileResult(compute_cost=0.215, peak_memory=3.376 GB, invar_size=1.750 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 0] = ModuleProfileResult(compute_cost=0.215, peak_memory=3.376 GB, invar_size=1.750 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 0] = ModuleProfileResult(compute_cost=0.528, peak_memory=2.938 GB, invar_size=1.063 GB, outvar_size=0.375 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(0, 1, 1, 1), 1] = ModuleProfileResult(compute_cost=1.161, peak_memory=13.747 GB, invar_size=2.145 GB, outvar_size=0.823 GB, temp_buffer_size=11.602 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 0] = ModuleProfileResult(compute_cost=0.528, peak_memory=2.938 GB, invar_size=1.063 GB, outvar_size=0.375 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 0] = ModuleProfileResult(compute_cost=0.215, peak_memory=3.376 GB, invar_size=1.750 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(0, 1, 1, 2), 1] = ModuleProfileResult(compute_cost=0.611, peak_memory=12.254 GB, invar_size=3.540 GB, outvar_size=1.645 GB, temp_buffer_size=8.714 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(6, 7, 1, 1), 1] = ModuleProfileResult(compute_cost=1.194, peak_memory=13.538 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=11.037 GB, available_memory=35.446 GB)
result[(6, 7, 1, 2), 1] = ModuleProfileResult(compute_cost=0.605, peak_memory=11.882 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.382 GB, available_memory=35.446 GB)
result[(7, 8, 1, 0), 1] = ModuleProfileResult(compute_cost=0.605, peak_memory=11.882 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.382 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 0] = ModuleProfileResult(compute_cost=0.472, peak_memory=2.875 GB, invar_size=1.000 GB, outvar_size=0.375 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(4, 5, 1, 1), 1] = ModuleProfileResult(compute_cost=1.141, peak_memory=13.528 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=11.028 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(5, 6, 1, 0), 1] = ModuleProfileResult(compute_cost=0.605, peak_memory=12.008 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.507 GB, available_memory=35.446 GB)
result[(1, 2, 1, 0), 1] = ModuleProfileResult(compute_cost=0.605, peak_memory=11.512 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.011 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(4, 5, 1, 2), 1] = ModuleProfileResult(compute_cost=0.589, peak_memory=12.008 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.507 GB, available_memory=35.446 GB)
result[(3, 4, 1, 1), 1] = ModuleProfileResult(compute_cost=1.257, peak_memory=15.031 GB, invar_size=2.501 GB, outvar_size=1.125 GB, temp_buffer_size=12.280 GB, available_memory=35.446 GB)
result[(4, 5, 1, 0), 1] = ModuleProfileResult(compute_cost=0.589, peak_memory=12.008 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.507 GB, available_memory=35.446 GB)
result[(5, 6, 1, 2), 1] = ModuleProfileResult(compute_cost=0.605, peak_memory=12.008 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.507 GB, available_memory=35.446 GB)
result[(3, 4, 1, 2), 1] = ModuleProfileResult(compute_cost=0.670, peak_memory=13.758 GB, invar_size=4.001 GB, outvar_size=1.875 GB, temp_buffer_size=9.632 GB, available_memory=35.446 GB)
result[(6, 7, 1, 0), 1] = ModuleProfileResult(compute_cost=0.605, peak_memory=11.882 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.382 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 0] = ModuleProfileResult(compute_cost=0.472, peak_memory=2.875 GB, invar_size=1.000 GB, outvar_size=0.375 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(7, 8, 1, 1), 1] = ModuleProfileResult(compute_cost=1.194, peak_memory=13.538 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=11.037 GB, available_memory=35.446 GB)
result[(3, 4, 1, 0), 1] = ModuleProfileResult(compute_cost=0.670, peak_memory=13.633 GB, invar_size=3.876 GB, outvar_size=1.875 GB, temp_buffer_size=9.632 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(2, 3, 1, 2), 1] = ModuleProfileResult(compute_cost=0.686, peak_memory=13.757 GB, invar_size=4.001 GB, outvar_size=1.875 GB, temp_buffer_size=9.631 GB, available_memory=35.446 GB)
result[(1, 2, 1, 1), 1] = ModuleProfileResult(compute_cost=1.194, peak_memory=13.537 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=11.036 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 0] = ModuleProfileResult(compute_cost=0.472, peak_memory=2.875 GB, invar_size=1.000 GB, outvar_size=0.375 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.250 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(2, 3, 1, 1), 1] = ModuleProfileResult(compute_cost=1.310, peak_memory=14.288 GB, invar_size=2.501 GB, outvar_size=1.125 GB, temp_buffer_size=11.538 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 0] = ModuleProfileResult(compute_cost=0.249, peak_memory=2.375 GB, invar_size=0.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.251 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(7, 8, 1, 2), 1] = ModuleProfileResult(compute_cost=0.605, peak_memory=11.882 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.382 GB, available_memory=35.446 GB)
result[(1, 2, 1, 2), 1] = ModuleProfileResult(compute_cost=0.605, peak_memory=11.512 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.011 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 0] = ModuleProfileResult(compute_cost=0.098, peak_memory=2.375 GB, invar_size=0.875 GB, outvar_size=0.125 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(2, 3, 1, 0), 1] = ModuleProfileResult(compute_cost=0.686, peak_memory=13.507 GB, invar_size=3.876 GB, outvar_size=1.875 GB, temp_buffer_size=9.506 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 0] = ModuleProfileResult(compute_cost=0.472, peak_memory=2.875 GB, invar_size=1.000 GB, outvar_size=0.375 GB, temp_buffer_size=1.500 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=3.251 GB, invar_size=1.625 GB, outvar_size=0.250 GB, temp_buffer_size=1.375 GB, available_memory=35.446 GB)
result[(8, 9, 1, 1), 1] = ModuleProfileResult(compute_cost=1.281, peak_memory=14.133 GB, invar_size=2.376 GB, outvar_size=1.063 GB, temp_buffer_size=11.507 GB, available_memory=35.446 GB)
result[(8, 9, 1, 0), 1] = ModuleProfileResult(compute_cost=0.656, peak_memory=14.507 GB, invar_size=3.626 GB, outvar_size=1.750 GB, temp_buffer_size=10.756 GB, available_memory=35.446 GB)
result[(8, 9, 1, 2), 1] = ModuleProfileResult(compute_cost=0.656, peak_memory=14.757 GB, invar_size=3.751 GB, outvar_size=1.750 GB, temp_buffer_size=10.881 GB, available_memory=35.446 GB)
result[(9, 10, 1, 1), 1] = ModuleProfileResult(compute_cost=1.240, peak_memory=16.156 GB, invar_size=2.376 GB, outvar_size=1.063 GB, temp_buffer_size=13.530 GB, available_memory=35.446 GB)
result[(11, 12, 1, 0), 1] = ModuleProfileResult(compute_cost=0.597, peak_memory=11.755 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.254 GB, available_memory=35.446 GB)
result[(9, 10, 1, 0), 1] = ModuleProfileResult(compute_cost=0.652, peak_memory=14.633 GB, invar_size=3.626 GB, outvar_size=1.750 GB, temp_buffer_size=10.882 GB, available_memory=35.446 GB)
result[(10, 11, 1, 1), 1] = ModuleProfileResult(compute_cost=1.111, peak_memory=13.264 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=10.764 GB, available_memory=35.446 GB)
result[(10, 11, 1, 0), 1] = ModuleProfileResult(compute_cost=0.597, peak_memory=11.755 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.254 GB, available_memory=35.446 GB)
result[(9, 10, 1, 2), 1] = ModuleProfileResult(compute_cost=0.652, peak_memory=14.758 GB, invar_size=3.751 GB, outvar_size=1.750 GB, temp_buffer_size=10.882 GB, available_memory=35.446 GB)
result[(11, 12, 1, 1), 1] = ModuleProfileResult(compute_cost=1.111, peak_memory=13.264 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=10.764 GB, available_memory=35.446 GB)
result[(10, 11, 1, 2), 1] = ModuleProfileResult(compute_cost=0.597, peak_memory=11.880 GB, invar_size=3.501 GB, outvar_size=1.625 GB, temp_buffer_size=8.254 GB, available_memory=35.446 GB)
result[(11, 12, 1, 2), 1] = ModuleProfileResult(compute_cost=0.597, peak_memory=11.880 GB, invar_size=3.501 GB, outvar_size=1.625 GB, temp_buffer_size=8.254 GB, available_memory=35.446 GB)
result[(12, 13, 1, 0), 1] = ModuleProfileResult(compute_cost=0.597, peak_memory=11.755 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.254 GB, available_memory=35.446 GB)
result[(12, 13, 1, 1), 1] = ModuleProfileResult(compute_cost=1.111, peak_memory=13.264 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=10.764 GB, available_memory=35.446 GB)
result[(12, 13, 1, 2), 1] = ModuleProfileResult(compute_cost=0.597, peak_memory=11.880 GB, invar_size=3.501 GB, outvar_size=1.625 GB, temp_buffer_size=8.254 GB, available_memory=35.446 GB)
result[(13, 14, 1, 1), 1] = ModuleProfileResult(compute_cost=1.152, peak_memory=13.285 GB, invar_size=2.251 GB, outvar_size=1.000 GB, temp_buffer_size=10.784 GB, available_memory=35.446 GB)
result[(14, 15, 1, 1), 1] = ModuleProfileResult(compute_cost=1.311, peak_memory=13.207 GB, invar_size=2.298 GB, outvar_size=1.149 GB, temp_buffer_size=10.659 GB, available_memory=35.446 GB)
result[(13, 14, 1, 0), 1] = ModuleProfileResult(compute_cost=0.601, peak_memory=11.755 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.254 GB, available_memory=35.446 GB)
result[(14, 15, 1, 0), 1] = ModuleProfileResult(compute_cost=0.714, peak_memory=12.473 GB, invar_size=3.845 GB, outvar_size=1.922 GB, temp_buffer_size=8.504 GB, available_memory=35.446 GB)
result[(13, 14, 1, 2), 1] = ModuleProfileResult(compute_cost=0.601, peak_memory=11.755 GB, invar_size=3.376 GB, outvar_size=1.625 GB, temp_buffer_size=8.254 GB, available_memory=35.446 GB)
result[(14, 15, 1, 2), 1] = ModuleProfileResult(compute_cost=0.758, peak_memory=12.208 GB, invar_size=3.454 GB, outvar_size=1.727 GB, temp_buffer_size=8.629 GB, available_memory=35.446 GB)
Profiling for submesh 1 (1, 2) takes 35.12 seconds
--------------------------------------------------
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(7, 7, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(7, 7, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(3, 3, 0, 1), 0] = ModuleProfileResult(compute_cost=0.260, peak_memory=4.250 GB, invar_size=1.250 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(3, 3, 0, 0), 0] = ModuleProfileResult(compute_cost=0.260, peak_memory=4.250 GB, invar_size=1.250 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 0] = ModuleProfileResult(compute_cost=0.233, peak_memory=4.301 GB, invar_size=0.895 GB, outvar_size=0.250 GB, temp_buffer_size=3.156 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 0] = ModuleProfileResult(compute_cost=0.233, peak_memory=4.301 GB, invar_size=0.895 GB, outvar_size=0.250 GB, temp_buffer_size=3.156 GB, available_memory=35.446 GB)
result[(1, 1, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(1, 1, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(7, 7, 0, 1), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=19.019 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.768 GB, available_memory=35.446 GB)
result[(7, 7, 0, 0), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=19.019 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.768 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(6, 6, 0, 0), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=19.019 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.768 GB, available_memory=35.446 GB)
result[(2, 2, 0, 1), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=18.269 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.018 GB, available_memory=35.446 GB)
result[(2, 2, 0, 0), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=18.269 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.018 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 0] = ModuleProfileResult(compute_cost=0.232, peak_memory=4.125 GB, invar_size=1.125 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(4, 4, 0, 1), 1] = ModuleProfileResult(compute_cost=0.559, peak_memory=19.013 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.763 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(6, 6, 0, 1), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=19.019 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.768 GB, available_memory=35.446 GB)
result[(0, 0, 0, 1), 1] = ModuleProfileResult(compute_cost=0.600, peak_memory=19.464 GB, invar_size=2.040 GB, outvar_size=0.895 GB, temp_buffer_size=17.424 GB, available_memory=35.446 GB)
result[(4, 4, 0, 0), 1] = ModuleProfileResult(compute_cost=0.559, peak_memory=19.013 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.763 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 0] = ModuleProfileResult(compute_cost=0.232, peak_memory=4.125 GB, invar_size=1.125 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(3, 3, 0, 1), 1] = ModuleProfileResult(compute_cost=0.748, peak_memory=21.763 GB, invar_size=2.501 GB, outvar_size=1.250 GB, temp_buffer_size=19.013 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=0.000 GB, invar_size=0.000 GB, outvar_size=0.000 GB, temp_buffer_size=0.000 GB, available_memory=35.446 GB)
result[(0, 0, 0, 0), 1] = ModuleProfileResult(compute_cost=0.600, peak_memory=19.464 GB, invar_size=2.040 GB, outvar_size=0.895 GB, temp_buffer_size=17.424 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(1, 1, 0, 1), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=18.269 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.018 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(3, 3, 0, 0), 1] = ModuleProfileResult(compute_cost=0.748, peak_memory=21.763 GB, invar_size=2.501 GB, outvar_size=1.250 GB, temp_buffer_size=19.013 GB, available_memory=35.446 GB)
result[(1, 1, 0, 0), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=18.269 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.018 GB, available_memory=35.446 GB)
result[(8, 8, 0, 0), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=19.019 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.768 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(13, 13, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(8, 8, 0, 1), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=19.019 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.768 GB, available_memory=35.446 GB)
result[(13, 13, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 0] = ModuleProfileResult(compute_cost=0.197, peak_memory=4.000 GB, invar_size=1.000 GB, outvar_size=0.250 GB, temp_buffer_size=2.750 GB, available_memory=35.446 GB)
result[(10, 10, 0, 0), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=18.507 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.257 GB, available_memory=35.446 GB)
result[(9, 9, 0, 0), 1] = ModuleProfileResult(compute_cost=0.689, peak_memory=24.013 GB, invar_size=2.251 GB, outvar_size=1.125 GB, temp_buffer_size=21.513 GB, available_memory=35.446 GB)
result[(9, 9, 0, 1), 1] = ModuleProfileResult(compute_cost=0.689, peak_memory=24.013 GB, invar_size=2.251 GB, outvar_size=1.125 GB, temp_buffer_size=21.513 GB, available_memory=35.446 GB)
result[(11, 11, 0, 0), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=18.507 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.257 GB, available_memory=35.446 GB)
result[(10, 10, 0, 1), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=18.507 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.257 GB, available_memory=35.446 GB)
result[(5, 5, 0, 0), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=19.513 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=17.263 GB, available_memory=35.446 GB)
result[(5, 5, 0, 1), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=19.513 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=17.263 GB, available_memory=35.446 GB)
result[(11, 11, 0, 1), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=18.507 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.257 GB, available_memory=35.446 GB)
result[(12, 12, 0, 0), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=18.507 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.257 GB, available_memory=35.446 GB)
result[(13, 13, 0, 0), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=18.507 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.257 GB, available_memory=35.446 GB)
result[(13, 13, 0, 1), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=18.507 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.257 GB, available_memory=35.446 GB)
result[(12, 12, 0, 1), 1] = ModuleProfileResult(compute_cost=0.583, peak_memory=18.507 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.257 GB, available_memory=35.446 GB)
result[(15, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.802, peak_memory=19.602 GB, invar_size=2.345 GB, outvar_size=1.297 GB, temp_buffer_size=17.007 GB, available_memory=35.446 GB)
result[(14, 14, 0, 0), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=18.763 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.513 GB, available_memory=35.446 GB)
result[(14, 14, 0, 1), 1] = ModuleProfileResult(compute_cost=0.590, peak_memory=18.763 GB, invar_size=2.001 GB, outvar_size=1.000 GB, temp_buffer_size=16.513 GB, available_memory=35.446 GB)
result[(15, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.802, peak_memory=19.602 GB, invar_size=2.345 GB, outvar_size=1.297 GB, temp_buffer_size=17.007 GB, available_memory=35.446 GB)
Profiling for submesh 0 (1, 1) takes 19.73 seconds
--------------------------------------------------
----------------------------------------------------------------------
[TMP] Stage (0, 3, 2, 0) has been pruned...
[TMP] Stage (0, 3, 2, 1) has been pruned...
[TMP] Stage (0, 3, 2, 2) has been pruned...
[TMP] Stage (0, 6, 3, 0) has been pruned...
[TMP] Stage (0, 6, 3, 1) has been pruned...
[TMP] Stage (0, 6, 3, 2) has been pruned...
[TMP] Stage (0, 7, 3, 0) has been pruned...
[TMP] Stage (0, 7, 3, 1) has been pruned...
[TMP] Stage (0, 7, 3, 2) has been pruned...
[TMP] Stage (0, 8, 3, 0) has been pruned...
[TMP] Stage (0, 8, 3, 1) has been pruned...
[TMP] Stage (0, 8, 3, 2) has been pruned...
[TMP] Stage (1, 4, 2, 0) has been pruned...
[TMP] Stage (1, 4, 2, 1) has been pruned...
[TMP] Stage (1, 4, 2, 2) has been pruned...
[TMP] Stage (1, 7, 3, 0) has been pruned...
[TMP] Stage (1, 7, 3, 1) has been pruned...
[TMP] Stage (1, 7, 3, 2) has been pruned...
[TMP] Stage (1, 8, 3, 0) has been pruned...
[TMP] Stage (1, 8, 3, 1) has been pruned...
[TMP] Stage (1, 8, 3, 2) has been pruned...
[TMP] Stage (1, 9, 3, 0) has been pruned...
[TMP] Stage (1, 9, 3, 1) has been pruned...
[TMP] Stage (1, 9, 3, 2) has been pruned...
[TMP] Stage (2, 5, 2, 0) has been pruned...
[TMP] Stage (2, 5, 2, 1) has been pruned...
[TMP] Stage (2, 5, 2, 2) has been pruned...
[TMP] Stage (2, 8, 3, 0) has been pruned...
[TMP] Stage (2, 8, 3, 1) has been pruned...
[TMP] Stage (2, 8, 3, 2) has been pruned...
[TMP] Stage (2, 9, 3, 0) has been pruned...
[TMP] Stage (2, 9, 3, 1) has been pruned...
[TMP] Stage (2, 9, 3, 2) has been pruned...
[TMP] Stage (2, 10, 3, 0) has been pruned...
[TMP] Stage (2, 10, 3, 1) has been pruned...
[TMP] Stage (2, 10, 3, 2) has been pruned...
[TMP] Stage (3, 6, 2, 0) has been pruned...
[TMP] Stage (3, 6, 2, 1) has been pruned...
[TMP] Stage (3, 6, 2, 2) has been pruned...
[TMP] Stage (3, 9, 3, 0) has been pruned...
[TMP] Stage (3, 9, 3, 1) has been pruned...
[TMP] Stage (3, 9, 3, 2) has been pruned...
[TMP] Stage (3, 10, 3, 0) has been pruned...
[TMP] Stage (3, 10, 3, 1) has been pruned...
[TMP] Stage (3, 10, 3, 2) has been pruned...
[TMP] Stage (3, 11, 3, 0) has been pruned...
[TMP] Stage (3, 11, 3, 1) has been pruned...
[TMP] Stage (3, 11, 3, 2) has been pruned...
[TMP] Stage (4, 7, 2, 0) has been pruned...
[TMP] Stage (4, 7, 2, 1) has been pruned...
[TMP] Stage (4, 7, 2, 2) has been pruned...
[TMP] Stage (4, 8, 2, 0) has been pruned...
[TMP] Stage (4, 8, 2, 1) has been pruned...
[TMP] Stage (4, 8, 2, 2) has been pruned...
[TMP] Stage (4, 10, 3, 0) has been pruned...
[TMP] Stage (4, 10, 3, 1) has been pruned...
[TMP] Stage (4, 10, 3, 2) has been pruned...
[TMP] Stage (4, 11, 3, 0) has been pruned...
[TMP] Stage (4, 11, 3, 1) has been pruned...
[TMP] Stage (4, 11, 3, 2) has been pruned...
[TMP] Stage (4, 12, 3, 0) has been pruned...
[TMP] Stage (4, 12, 3, 1) has been pruned...
[TMP] Stage (4, 12, 3, 2) has been pruned...
[TMP] Stage (4, 13, 3, 0) has been pruned...
[TMP] Stage (4, 13, 3, 1) has been pruned...
[TMP] Stage (4, 13, 3, 2) has been pruned...
[TMP] Stage (5, 8, 2, 0) has been pruned...
[TMP] Stage (5, 8, 2, 1) has been pruned...
[TMP] Stage (5, 8, 2, 2) has been pruned...
[TMP] Stage (5, 11, 3, 0) has been pruned...
[TMP] Stage (5, 11, 3, 1) has been pruned...
[TMP] Stage (5, 11, 3, 2) has been pruned...
[TMP] Stage (5, 12, 3, 0) has been pruned...
[TMP] Stage (5, 12, 3, 1) has been pruned...
[TMP] Stage (5, 12, 3, 2) has been pruned...
[TMP] Stage (5, 13, 3, 0) has been pruned...
[TMP] Stage (5, 13, 3, 1) has been pruned...
[TMP] Stage (5, 13, 3, 2) has been pruned...
[TMP] Stage (5, 14, 3, 0) has been pruned...
[TMP] Stage (5, 14, 3, 1) has been pruned...
[TMP] Stage (5, 14, 3, 2) has been pruned...
[TMP] Stage (6, 9, 2, 0) has been pruned...
[TMP] Stage (6, 9, 2, 1) has been pruned...
[TMP] Stage (6, 9, 2, 2) has been pruned...
[TMP] Stage (6, 12, 3, 0) has been pruned...
[TMP] Stage (6, 12, 3, 1) has been pruned...
[TMP] Stage (6, 12, 3, 2) has been pruned...
[TMP] Stage (6, 13, 3, 0) has been pruned...
[TMP] Stage (6, 13, 3, 1) has been pruned...
[TMP] Stage (6, 13, 3, 2) has been pruned...
[TMP] Stage (6, 14, 3, 0) has been pruned...
[TMP] Stage (6, 14, 3, 1) has been pruned...
[TMP] Stage (6, 14, 3, 2) has been pruned...
[TMP] Stage (6, 15, 3, 0) has been pruned...
[TMP] Stage (6, 15, 3, 1) has been pruned...
[TMP] Stage (6, 15, 3, 2) has been pruned...
[TMP] Stage (7, 10, 2, 0) has been pruned...
[TMP] Stage (7, 10, 2, 1) has been pruned...
[TMP] Stage (7, 10, 2, 2) has been pruned...
[TMP] Stage (7, 13, 3, 0) has been pruned...
[TMP] Stage (7, 13, 3, 1) has been pruned...
[TMP] Stage (7, 13, 3, 2) has been pruned...
[TMP] Stage (7, 14, 3, 0) has been pruned...
[TMP] Stage (7, 14, 3, 1) has been pruned...
[TMP] Stage (7, 14, 3, 2) has been pruned...
[TMP] Stage (7, 15, 3, 0) has been pruned...
[TMP] Stage (7, 15, 3, 1) has been pruned...
[TMP] Stage (7, 15, 3, 2) has been pruned...
[TMP] Stage (8, 11, 2, 0) has been pruned...
[TMP] Stage (8, 11, 2, 1) has been pruned...
[TMP] Stage (8, 11, 2, 2) has been pruned...
[TMP] Stage (8, 12, 2, 0) has been pruned...
[TMP] Stage (8, 12, 2, 1) has been pruned...
[TMP] Stage (8, 12, 2, 2) has been pruned...
[TMP] Stage (8, 14, 3, 0) has been pruned...
[TMP] Stage (8, 14, 3, 1) has been pruned...
[TMP] Stage (8, 14, 3, 2) has been pruned...
[TMP] Stage (8, 15, 3, 0) has been pruned...
[TMP] Stage (8, 15, 3, 1) has been pruned...
[TMP] Stage (8, 15, 3, 2) has been pruned...
[TMP] Stage (9, 12, 2, 0) has been pruned...
[TMP] Stage (9, 12, 2, 1) has been pruned...
[TMP] Stage (9, 12, 2, 2) has been pruned...
[TMP] Stage (9, 13, 2, 0) has been pruned...
[TMP] Stage (9, 13, 2, 1) has been pruned...
[TMP] Stage (9, 13, 2, 2) has been pruned...
[TMP] Stage (9, 15, 3, 0) has been pruned...
[TMP] Stage (9, 15, 3, 1) has been pruned...
[TMP] Stage (9, 15, 3, 2) has been pruned...
[TMP] Stage (10, 13, 2, 0) has been pruned...
[TMP] Stage (10, 13, 2, 1) has been pruned...
[TMP] Stage (10, 13, 2, 2) has been pruned...
[TMP] Stage (10, 14, 2, 0) has been pruned...
[TMP] Stage (10, 14, 2, 1) has been pruned...
[TMP] Stage (10, 14, 2, 2) has been pruned...
[TMP] Stage (11, 14, 2, 0) has been pruned...
[TMP] Stage (11, 14, 2, 1) has been pruned...
[TMP] Stage (11, 14, 2, 2) has been pruned...
[TMP] Stage (11, 15, 2, 0) has been pruned...
[TMP] Stage (11, 15, 2, 1) has been pruned...
[TMP] Stage (11, 15, 2, 2) has been pruned...
[TMP] Stage (12, 15, 2, 0) has been pruned...
[TMP] Stage (12, 15, 2, 1) has been pruned...
[TMP] Stage (12, 15, 2, 2) has been pruned...
Result forward_stage_layer_ids: [[0, 1], [2, 3], [4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14, 15]]
Result mesh_shapes: [(1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2), (1, 2)]
Result logical_mesh_shapes: [(2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1), (2, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}, {}, {'force_batch_dim_to_mesh_dim': 0}, {'force_batch_dim_to_mesh_dim': 0}, {}, {}, {}, {'force_batch_dim_to_mesh_dim': 0}]
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO comm 0x35c1bb0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO comm 0x423d110 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO comm 0x696b240 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO comm 0x66291f0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO comm 0xbabad20 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO comm 0x490f7c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO comm 0x6956fd0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO comm 0xbbe1f80 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO comm 0x34f22c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO comm 0xa6e4520 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO comm 0xa9418d0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO comm 0x5803a40 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO comm 0xabf0150 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO comm 0x380d2d0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO comm 0xaa9bd20 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO comm 0x8c19050 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO comm 0x9adf770 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO comm 0x47be9a0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO comm 0x6b0b470 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO comm 0x9b488b0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO comm 0xbed7b80 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO comm 0x5450570 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO comm 0xbee2c00 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO comm 0x7614e00 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO NCCL_NSOCKS_PERTHREAD set by environment to 8.
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO NCCL_SOCKET_NTHREADS set by environment to 8.
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [0] NCCL INFO comm 0x38f00c0 rank 1 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 00 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 01 : 1[31000] -> 0[31000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 00 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Channel 01 : 0[31000] -> 1[31000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO comm 0xb465c10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] 0/-1/-1->1->-1
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] -1/-1/-1->0->1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [receive] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO NET/Socket: Using 8 threads and 8 sockets per thread
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO Channel 00 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 00 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO Channel 01 : 1[98000] -> 0[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Channel 01 : 0[98000] -> 1[98000] [send] via NET/Socket/0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO comm 0xb3fcad0 rank 0 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO 2 coll channels, 2 p2p channels, 1 p2p channels per peer
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514901 [1] NCCL INFO comm 0x57a6050 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
 - Compile (driver): 111.08 s
compilation time breakdown: {'stage-construction': '59.22', 'stage-construction-dp': '1.48', 'stage-construction-compilation': '11.01', 'stage-construction-profiling': '12.38'}
 - Compile (worker): 4.75 s
[I] Training process warmup (2 rounds) with dummy input batch...
    - Warmup iteration: 1/2
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006009 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:737881 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392614 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550328 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313851 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194688 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [0] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686201 [1] NCCL INFO Launch mode Parallel
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.16<0>
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=737881)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.30<0>
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.17<0>
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.38<0>
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.31<0>
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m 
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO comm 0x7f0a358b7270 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514977 [1] NCCL INFO comm 0x7f0a2ccdf6b0 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1514901, ip=192.168.0.32)[0m gpu17:1514901:1514975 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m 
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO comm 0x7f699160d360 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686265 [1] NCCL INFO comm 0x7f6988ebb670 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3686201, ip=192.168.0.39)[0m gpu24:3686201:3686263 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m 
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO comm 0x7ee8a35e1480 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194836 [1] NCCL INFO comm 0x7efd7f234160 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2194688, ip=192.168.0.18)[0m gpu3:2194688:2194834 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m 
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO comm 0x7f6566826a10 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313957 [1] NCCL INFO comm 0x7f6569701c00 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=3313851, ip=192.168.0.31)[0m gpu16:3313851:3313955 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.37<0>
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m 
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO comm 0x7f8df4f954a0 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550476 [1] NCCL INFO comm 0x7f8def48b920 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.26<0>
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m 
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392720 [1] NCCL INFO comm 0x7f9cd6d42600 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO comm 0x7f9cdfc15610 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=737881)[0m 
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO comm 0x7f5c401c5830 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738003 [1] NCCL INFO comm 0x7f5c4cd2b930 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=737881)[0m gpu2:737881:738001 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO Bootstrap : Using ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] misc/ibvwrap.cc:63 NCCL WARN Failed to open libibverbs.so[.1]
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO NET/Socket : Using [0]ib0.8068:192.168.1.25<0>
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO Using network Socket
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m NCCL version 2.8.4+cuda11.2
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] misc/nvmlwrap.cc:47 NCCL WARN Failed to open libnvidia-ml.so.1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m 
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] graph/xml.cc:595 NCCL WARN No NVML device handle. Skipping nvlink detection.
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Channel 00/02 :    0   1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Channel 01/02 :    0   1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Setting affinity for GPU 0 to ffffff
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Channel 00 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Channel 01 : 0[31000] -> 1[98000] via direct shared memory
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO Channel 00 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO Channel 01 : 1[98000] -> 0[31000] via direct shared memory
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Connected all rings
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Connected all trees
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/64
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO 2 coll channels, 2 p2p channels, 2 p2p channels per peer
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006116 [1] NCCL INFO comm 0x7faa611d5120 rank 1 nranks 2 cudaDev 1 busId 98000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO comm 0x7faa68443210 rank 0 nranks 2 cudaDev 0 busId 31000 - Init COMPLETE
[2m[36m(MeshHostWorker pid=2550328, ip=192.168.0.38)[0m gpu23:2550328:2550474 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=2392614, ip=192.168.0.27)[0m gpu12:2392614:2392718 [0] NCCL INFO Launch mode Group/CGMD
[2m[36m(MeshHostWorker pid=1006009, ip=192.168.0.26)[0m gpu11:1006009:1006114 [0] NCCL INFO Launch mode Group/CGMD
    - Warmup iteration: 2/2
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 5 is performed...
    - Iteration 2 / 5 is performed...
    - Iteration 3 / 5 is performed...
    - Iteration 4 / 5 is performed...
 - Benchmark: 386.08 s

[112.66239809989929, 43.42034292221069, 43.415480613708496, 42.85937976837158, 42.558560848236084, 43.50281381607056, 42.40514397621155]


[I] Performance metrics:
 - Iteration count: 5.
 - Total e2e training time : 222.388 s.
 - Average e2e iteration time: 44.47800064086914 s.
 - Total local training time: 214.7410125732422 s.
 - Average local iteration time: 42.948001861572266 s.
 - Max allocated memory among devices: 23.509 GB.
 - Compilation times:  {'stage-construction': 59.21543526649475, 'stage-construction-dp': 1.483534812927246, 'stage-construction-compilation': 11.014733791351318, 'stage-construction-profiling': 12.3751220703125}
 - Metadata:  []
 - Is need save result:  True

[TMP] Current profiling results of key `8_a40_8_n_2_d`: 42.94827651977539
[TMP] Updated profiling results stored in `./jaxpr/prof_log/optimal/bert_6.7B_512.pkl`...
