
[I] Loading Crius kernel-level profiler...
[TMP] Profiling results not found in `None`, creating it...

[I] Loading model and generating sharded HLO module...
[I] Initializing XLA devices takes 0.5825400352478027 s.
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing general train step func...
[I] General train step func construction is completed.
[I] Model has been loaded, begin Jaxpr transformation...


Layer 0:
Eqn 2 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 24.244239807128906 | Memory access (GB): 0.3441784381866455 | Attainable performance: 37400 | Computation load: 0.0006482417060729654
Eqn 27 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 29 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 18.7578125 | Memory access (GB): 0.1682281494140625 | Attainable performance: 37400 | Computation load: 0.0005015457887700534
Eqn 54 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 55 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 80 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 89 in layer 0: Primitive: remat2 | FLOPs (GB): 1093.8148345947266 | Memory access (GB): 3.6618828773498535 | Attainable performance: 37400 | Computation load: 0.029246385951730656
Layer computation load: 0.04257918890638221

Layer 1:
Eqn 1 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 52 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 53 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 78 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 79 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 104 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 105 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 143 in layer 1: Primitive: remat2 | FLOPs (GB): 2339.6572875976562 | Memory access (GB): 8.719451904296875 | Attainable performance: 37400 | Computation load: 0.06255768148656835
Layer computation load: 0.08677779587685816

Layer 2:
Eqn 1 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 2 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 28 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 53 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 54 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 89 in layer 2: Primitive: remat2 | FLOPs (GB): 1664.3760375976562 | Memory access (GB): 5.929107666015625 | Attainable performance: 37400 | Computation load: 0.044502033090846424
Layer computation load: 0.064628660875091

Layer 3:
Eqn 1 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 3: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 27 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.3050537109375 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 52 in layer 3: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 53 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5263671875 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 3: Primitive: remat2 | FLOPs (GB): 2339.5735473632812 | Memory access (GB): 5.849853515625 | Attainable performance: 37400 | Computation load: 0.06255544244286848
Layer computation load: 0.0848504939739629

Layer 4:
Eqn 1 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 2 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 28 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 53 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 54 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 88 in layer 4: Primitive: remat2 | FLOPs (GB): 1640.6162414550781 | Memory access (GB): 3.2000732421875 | Attainable performance: 37400 | Computation load: 0.04386674442393257
Layer computation load: 0.06017276951262694

Layer 5:
Eqn 1 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 2 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 28 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 53 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 54 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 89 in layer 5: Primitive: remat2 | FLOPs (GB): 1640.6162414550781 | Memory access (GB): 3.2000732421875 | Attainable performance: 37400 | Computation load: 0.04386674442393257
Layer computation load: 0.06113530094222464

Layer 6:
Eqn 1 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 27 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 52 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 53 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 88 in layer 6: Primitive: remat2 | FLOPs (GB): 1640.5743713378906 | Memory access (GB): 2.5301513671875 | Attainable performance: 37400 | Computation load: 0.04386562490208264
Layer computation load: 0.06017164999077701

Layer 7:
Eqn 1 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 7: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 27 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.3409423828125 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 52 in layer 7: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.346923828125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 7: Primitive: remat2 | FLOPs (GB): 2315.8556213378906 | Memory access (GB): 4.055419921875 | Attainable performance: 37400 | Computation load: 0.061921273297804565
Layer computation load: 0.08328331642214187

Layer 8:
Eqn 1 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 2 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 28 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 53 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 54 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 88 in layer 8: Primitive: remat2 | FLOPs (GB): 1594.067886352539 | Memory access (GB): 2.5421142652630806 | Attainable performance: 37400 | Computation load: 0.04262213599873099
Layer computation load: 0.05767140314937127

Layer 9:
Eqn 1 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 2 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 28 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 53 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 54 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 89 in layer 9: Primitive: remat2 | FLOPs (GB): 1594.067886352539 | Memory access (GB): 2.5421142652630806 | Attainable performance: 37400 | Computation load: 0.04262213599873099
Layer computation load: 0.058152668864170115

Layer 10:
Eqn 1 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 10: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 11:
Eqn 1 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 11: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 12:
Eqn 1 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 12: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 13:
Eqn 1 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 13: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 27 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.92413330078125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 13: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 53 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 13: Primitive: remat2 | FLOPs (GB): 2269.3282012939453 | Memory access (GB): 6.549682624638081 | Attainable performance: 37400 | Computation load: 0.06067722463352795
Layer computation load: 0.08126377553461002

Layer 14:
Eqn 1 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 2 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.243743896484375 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.00261688232421875 | Memory access (GB): 0.041870128363370895 | Attainable performance: 0 | Computation load: 6.015823040714209e-05
Eqn 28 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 276.390625 | Memory access (GB): 0.892730712890625 | Attainable performance: 37400 | Computation load: 0.007390123663101604
Eqn 53 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 54 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 89 in layer 14: Primitive: remat2 | FLOPs (GB): 1504.4714431762695 | Memory access (GB): 5.0393676944077015 | Attainable performance: 37400 | Computation load: 0.04022650917583608
Layer computation load: 0.05429692268878889

Layer 15:
Eqn 1 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.243743896484375 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.00261688232421875 | Memory access (GB): 0.041870128363370895 | Attainable performance: 0 | Computation load: 6.015823040714209e-05
Eqn 27 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 276.390625 | Memory access (GB): 0.892730712890625 | Attainable performance: 37400 | Computation load: 0.007390123663101604
Eqn 52 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 53 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 82 in layer 15: Primitive: dot_general | FLOPs (GB): 0.4375 | Memory access (GB): 0.0556030347943306 | Attainable performance: 5476.319793088838 | Computation load: 7.988941780794626e-05
Eqn 148 in layer 15: Primitive: remat2 | FLOPs (GB): 1505.7839431762695 | Memory access (GB): 5.206176798790693 | Attainable performance: 37400 | Computation load: 0.040261602758723786
Layer computation load: 0.054171272832085116
Layer memory access: [2.7641637325286865, 8.4996337890625, 4.7791748046875, 6.4599609375, 2.70361328125, 2.70361328125, 2.70361328125, 4.7373046875, 2.60791015625, 2.60791015625, 2.60791015625, 2.60791015625, 2.60791015625, 8.39794921875, 6.328369140625, 6.550781279802322]
[TMP] Optimization status of GPU fraction: optimal

Layer GPU fractions:
[0.159, 0.372, 0.314, 0.459, 0.359, 0.399, 0.427, 0.637, 0.473, 0.51, 0.538, 0.57, 0.603, 0.895, 0.628, 0.657]

Layer GPU fractions:
[0.159, 0.372, 0.314, 0.459, 0.359, 0.399, 0.427, 0.637, 0.473, 0.51, 0.538, 0.57, 0.603, 0.895, 0.628, 0.657]

[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11], [12, 13], [14, 15]]
507140984.0
1.0494693897394052
([4, 2, 1, 1], 1.0494693897394052)

[I] Pipeline partition mode: auto | Layer-to-stage partition: [[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11], [12, 13, 14, 15]]
[I] Pipeline partition mode: auto | Cell-generated physical mesh shapes: [(1, 2), (1, 2), (1, 2), (1, 2)]
[I] Loading model and transform to Jaxpr stages takes 19.205990076065063 s.
[I] The GPU sharding of pipeline stages is: [2, 2, 2, 2]
[I] Pipeline partition mode: auto | Parallel enum mode: auto | Parallel plans: [[StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None)]]
[TMP] Plans in the non-dominated set:
0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None)]
0.3492477630565442
28997713920.0

0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.3492477630565442
13674801840.0

0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.44598878909676654
13674801840.0

0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.5472239029867026
13674801840.0

0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.6545273103545792
13674801840.0

0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.7067121054573782
13674801840.0

0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.8078873683874503
13674801840.0

0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10], [11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.883138720700208
13674801840.0

0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9283953899066926
13674801840.0

0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9363108458199125
13674801840.0

0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9404860445535593
13674801840.0

0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9570653060267099
13674801840.0

0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9885474191964693
13674801840.0

0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10], [11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9901585731588655
13674801840.0

0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0115868721963526
13674801840.0

0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0149561566885537
13674801840.0

0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0307269279493962
13674801840.0

0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0547331416050227
13674801840.0

0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0665223860754165
13674801840.0

0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0913047237137752
13674801840.0

0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.117301212744352
13674801840.0

0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.1233797220886623
13674801840.0

0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.153453076635543
13674801840.0

0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.180667607754189
13674801840.0

0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.1894561782596278
13674801840.0

0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.2252795599372412
13674801840.0

0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11], [12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.2378747917297612
13674801840.0

0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3119596030366178
13674801840.0

0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3379962630740045
13674801840.0

0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3387583799924465
13674801840.0

0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2], [3, 4, 5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3881721795224107
13674801840.0

0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2], [3, 4, 5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.408467962006946
13674801840.0

[TMP] Existed tuning database in `/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tuning_database/tuning_database.pkl`, updating/rewriting it...
{'plan_set': {'wide_resnet__1B::256__16::1::a40__2__2': ['0_15::4_1'], 'wide_resnet__1B::256__16::2::a40__2__2': ['0_9__10_15::1_2__1_2', '0_9__10_15::1_2__2_1', '0_9__10_15::2_1__1_2', '0_9__10_15::2_1__2_1', '0_8__9_15::2_1__1_2', '0_8__9_15::2_1__2_1', '0_10__11_15::2_1__1_2', '0_10__11_15::2_1__2_1', '0_7__8_15::2_1__1_2', '0_7__8_15::2_1__2_1', '0_11__12_15::2_1__1_2', '0_11__12_15::2_1__2_1'], 'wide_resnet__1B::256__16::3::a40__2__2': ['0_5__6_9__10_15::2_1__1_1__1_1', '0_6__7_9__10_15::2_1__1_1__1_1', '0_5__6_10__11_15::2_1__1_1__1_1', '0_6__7_10__11_15::2_1__1_1__1_1', '0_3__4_11__12_15::1_1__1_2__1_1', '0_3__4_11__12_15::1_1__2_1__1_1', '0_5__6_11__12_15::2_1__1_1__1_1', '0_6__7_11__12_15::2_1__1_1__1_1', '0_7__8_11__12_15::2_1__1_1__1_1', '0_7__8_12__13_15::2_1__1_1__1_1'], 'wide_resnet__1B::256__16::4::a40__2__2': ['0_4__5_8__9_10__11_15::1_1__1_1__1_1__1_1', '0_3__4_7__8_11__12_15::1_1__1_1__1_1__1_1', '0_4__5_7__8_11__12_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_11__12_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_11__12_15::1_1__1_1__1_1__1_1', '0_3__4_7__8_12__13_15::1_1__1_1__1_1__1_1', '0_3__4_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_12__13_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_12__13_15::1_1__1_1__1_1__1_1', '0_3__4_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_7__8_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_6__7_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_8__9_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_7__8_12__13_14__15_15::1_1__1_1__1_1__1_1', '0_10__11_13__14_14__15_15::1_1__1_1__1_1__1_1', '0_12__13_13__14_14__15_15::1_1__1_1__1_1__1_1'], 'wide_resnet__500M::256__16::2::a40__2__2': ['0_7__8_15::2_1__2_1', '0_8__9_15::2_1__2_1'], 'wide_resnet__500M::512__16::2::a40__2__2': ['0_7__8_15::2_1__2_1', '0_8__9_15::2_1__2_1'], 'wide_resnet__2B::512__16::4::a40__4__2': ['0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2', '0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1'], 'wide_resnet__2B::256__16::4::a40__4__2': ['0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2', '0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1']}, 'selected_cell_num_stages': {'wide_resnet__1B::256__16::a40__2__2': 2}}
[TMP] Storing tuning database to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tuning_database/tuning_database.pkl'...

[I] (Parallel plan idx: 0) Sharding stages...
[I] Sharding stages takes 7.902573585510254 s.
[I] (Parallel plan idx: 0) SPMD partitioning and compiling (kernel fusing) HLO stages...
[I] SPMD partitioning and pre-compiling HLO modules concurrently takes 8.598237991333008 s.

[I] All parallel plans in the cell have been sharded and compiled.
[TMP] Writing only-sharded HLO text to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_sharded_stages.pkl'...
[TMP] Writing optimized HLO text to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_optimized_stages.pkl'...
[TMP] Writing inter-stages communicated vars to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_inter_stages_comm_vars.pkl'

[I] Parallel plan idx: 0 | Parallelism: [(1, 2), (1, 2), (1, 2), (1, 2)]

[I] (Parallel plan idx: 0) Parsing HLO texts and profiling XLA operators...
[I] Pipeline partition mode: auto | Preset physical mesh shapes: [(1, 2), (1, 2), (1, 2), (1, 2)]
[TMP] Loading offline profiled communication data: `1_a40_1_n_2_d_ib.pkl`...
[TMP] Loading offline profiled communication data: `2_a40_2_n_1_d_ib.pkl`...
\Time of module 0 (stage 0):
Intra-stage comm time: 0.11482265865232683
Grad sync time: 0

[I] Optimized module 0 for stage 0, comm time has been statically analyzed.
\Time of module 1 (stage 1):
Intra-stage comm time: 0.014319016225636005
Grad sync time: 0

[I] Optimized module 1 for stage 1, comm time has been statically analyzed.
\Time of module 2 (stage 2):
Intra-stage comm time: 0.008933287905529141
Grad sync time: 0

[I] Optimized module 2 for stage 2, comm time has been statically analyzed.
\Time of module 3 (stage 3):
Intra-stage comm time: 0.015907426876947284
Grad sync time: 0

[I] Optimized module 3 for stage 3, comm time has been statically analyzed.
\Time of module 4 (stage 3):
Intra-stage comm time: 0.030212004861732568
Grad sync time: 0

[I] Optimized module 4 for stage 3, comm time has been statically analyzed.
\Time of module 5 (stage 2):
Intra-stage comm time: 0.019475458702072502
Grad sync time: 0

[I] Optimized module 5 for stage 2, comm time has been statically analyzed.
\Time of module 6 (stage 1):
Intra-stage comm time: 0.03798078987747431
Grad sync time: 0

[I] Optimized module 6 for stage 1, comm time has been statically analyzed.
\Time of module 7 (stage 0):
Intra-stage comm time: 0.2121075068685141
Grad sync time: 0

[I] Optimized module 7 for stage 0, comm time has been statically analyzed.
\Time of module 8 (stage 0):
Intra-stage comm time: 0.0007787391543388368
Grad sync time: 0

[I] Optimized module 8 for stage 0, comm time has been statically analyzed.
\Time of module 9 (stage 1):
Intra-stage comm time: 0.00019618347287178042
Grad sync time: 0

[I] Optimized module 9 for stage 1, comm time has been statically analyzed.
\Time of module 10 (stage 2):
Intra-stage comm time: 0.00013078898191452027
Grad sync time: 0

[I] Optimized module 10 for stage 2, comm time has been statically analyzed.
\Time of module 11 (stage 3):
Intra-stage comm time: 0.000263860821723938
Grad sync time: 0

[I] Optimized module 11 for stage 3, comm time has been statically analyzed.
[TMP] Memory footprint of the stage 0 is 2.40174138918519 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 1 is 3.9078230895102024 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 2 is 2.4056758917868137 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 3 is 15.319579277187586 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[I] Only-sharded module 0 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.14473960700000002 s.
    - Iteration 1/5: Kernel execution time: 0.144017452 s.
    - Iteration 2/5: Kernel execution time: 0.1448225090000001 s.
    - Iteration 3/5: Kernel execution time: 0.14659804999999998 s.
    - Iteration 4/5: Kernel execution time: 0.146935205 s.
[I] Only-sharded module 1 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.067854255 s.
    - Iteration 1/5: Kernel execution time: 0.06750094299999998 s.
    - Iteration 2/5: Kernel execution time: 0.06779221100000002 s.
    - Iteration 3/5: Kernel execution time: 0.06824197500000001 s.
    - Iteration 4/5: Kernel execution time: 0.06772808200000002 s.
[I] Only-sharded module 2 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.04019846500000001 s.
    - Iteration 1/5: Kernel execution time: 0.040530274000000005 s.
    - Iteration 2/5: Kernel execution time: 0.04046982600000001 s.
    - Iteration 3/5: Kernel execution time: 0.04043110200000001 s.
    - Iteration 4/5: Kernel execution time: 0.040584931000000005 s.
[I] Only-sharded module 3 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.14370180999999999 s.
    - Iteration 1/5: Kernel execution time: 0.14368807999999997 s.
    - Iteration 2/5: Kernel execution time: 0.14360018 s.
    - Iteration 3/5: Kernel execution time: 0.143699699 s.
    - Iteration 4/5: Kernel execution time: 0.144052982 s.
[I] Only-sharded module 4 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.6401007829999996 s.
    - Iteration 1/5: Kernel execution time: 0.639644081 s.
    - Iteration 2/5: Kernel execution time: 0.6406849779999999 s.
    - Iteration 3/5: Kernel execution time: 0.640160106 s.
    - Iteration 4/5: Kernel execution time: 0.6399939050000002 s.
[I] Only-sharded module 5 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.13460470799999996 s.
    - Iteration 1/5: Kernel execution time: 0.13568602300000002 s.
    - Iteration 2/5: Kernel execution time: 0.13465120200000003 s.
    - Iteration 3/5: Kernel execution time: 0.13332859900000002 s.
    - Iteration 4/5: Kernel execution time: 0.13398083100000002 s.
[I] Only-sharded module 6 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.24316506599999993 s.
    - Iteration 1/5: Kernel execution time: 0.24293245299999985 s.
    - Iteration 2/5: Kernel execution time: 0.24204205300000003 s.
    - Iteration 3/5: Kernel execution time: 0.24147894300000006 s.
    - Iteration 4/5: Kernel execution time: 0.24239123600000004 s.
[I] Only-sharded module 7 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.45523485500000005 s.
    - Iteration 1/5: Kernel execution time: 0.4549332099999998 s.
    - Iteration 2/5: Kernel execution time: 0.4562375460000001 s.
    - Iteration 3/5: Kernel execution time: 0.457020553 s.
    - Iteration 4/5: Kernel execution time: 0.4560572350000002 s.
[I] Only-sharded module 8 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.002351347 s.
    - Iteration 1/5: Kernel execution time: 0.002357079 s.
    - Iteration 2/5: Kernel execution time: 0.002352467 s.
    - Iteration 3/5: Kernel execution time: 0.002359318 s.
    - Iteration 4/5: Kernel execution time: 0.002353524 s.
[I] Only-sharded module 9 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.006015989 s.
    - Iteration 1/5: Kernel execution time: 0.0060217510000000005 s.
    - Iteration 2/5: Kernel execution time: 0.006025398 s.
    - Iteration 3/5: Kernel execution time: 0.006016566000000001 s.
    - Iteration 4/5: Kernel execution time: 0.006025878 s.
[I] Only-sharded module 10 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.003744065 s.
    - Iteration 1/5: Kernel execution time: 0.003743457 s.
    - Iteration 2/5: Kernel execution time: 0.0037425290000000005 s.
    - Iteration 3/5: Kernel execution time: 0.0037468180000000003 s.
    - Iteration 4/5: Kernel execution time: 0.003746305 s.
[I] Only-sharded module 11 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.026254569000000002 s.
    - Iteration 1/5: Kernel execution time: 0.026261677 s.
    - Iteration 2/5: Kernel execution time: 0.026262509000000003 s.
    - Iteration 3/5: Kernel execution time: 0.026250061 s.
    - Iteration 4/5: Kernel execution time: 0.026270638000000002 s.
[I] Constructing single-device HLO modules concurrently takes 0.03395962715148926 s.
[I] Compile single-device HLO modules concurrently takes 1.6762244701385498 s.
[I] Profile all compiled sequentially on one GPU takes 23.200747966766357 s.
[I] Estimated e2e pipeline iteration time is: 18.61828695656982 s.
    - Kernel computation time is: -1.0 s.
    - Communication time is: -1.0 s.
    - Cross-stages communication time is: -1.0 s.
