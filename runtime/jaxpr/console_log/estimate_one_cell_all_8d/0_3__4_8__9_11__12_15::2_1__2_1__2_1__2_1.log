
[I] Loading Crius kernel-level profiler...
[TMP] Profiling results not found in `None`, creating it...

[I] Loading model and generating sharded HLO module...
[I] Initializing XLA devices takes 0.6448853015899658 s.
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing general train step func...
[I] General train step func construction is completed.
[I] Model has been loaded, begin Jaxpr transformation...


Layer 0:
Eqn 2 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 24.244239807128906 | Memory access (GB): 0.3441784381866455 | Attainable performance: 37400 | Computation load: 0.0006482417060729654
Eqn 27 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 29 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 18.7578125 | Memory access (GB): 0.1682281494140625 | Attainable performance: 37400 | Computation load: 0.0005015457887700534
Eqn 54 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 55 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 80 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 89 in layer 0: Primitive: remat2 | FLOPs (GB): 1093.8148345947266 | Memory access (GB): 3.6618828773498535 | Attainable performance: 37400 | Computation load: 0.029246385951730656
Layer computation load: 0.04257918890638221

Layer 1:
Eqn 1 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 52 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 53 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 78 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 79 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 104 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 105 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 143 in layer 1: Primitive: remat2 | FLOPs (GB): 2339.6572875976562 | Memory access (GB): 8.719451904296875 | Attainable performance: 37400 | Computation load: 0.06255768148656835
Layer computation load: 0.08677779587685816

Layer 2:
Eqn 1 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 2 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 28 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 53 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 54 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 89 in layer 2: Primitive: remat2 | FLOPs (GB): 1664.3760375976562 | Memory access (GB): 5.929107666015625 | Attainable performance: 37400 | Computation load: 0.044502033090846424
Layer computation load: 0.064628660875091

Layer 3:
Eqn 1 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 3: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 27 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.3050537109375 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 52 in layer 3: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 53 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5263671875 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 3: Primitive: remat2 | FLOPs (GB): 2339.5735473632812 | Memory access (GB): 5.849853515625 | Attainable performance: 37400 | Computation load: 0.06255544244286848
Layer computation load: 0.0848504939739629

Layer 4:
Eqn 1 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 2 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 28 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 53 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 54 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 88 in layer 4: Primitive: remat2 | FLOPs (GB): 1640.6162414550781 | Memory access (GB): 3.2000732421875 | Attainable performance: 37400 | Computation load: 0.04386674442393257
Layer computation load: 0.06017276951262694

Layer 5:
Eqn 1 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 2 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 28 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 53 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 54 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 89 in layer 5: Primitive: remat2 | FLOPs (GB): 1640.6162414550781 | Memory access (GB): 3.2000732421875 | Attainable performance: 37400 | Computation load: 0.04386674442393257
Layer computation load: 0.06113530094222464

Layer 6:
Eqn 1 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 27 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 52 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 53 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 88 in layer 6: Primitive: remat2 | FLOPs (GB): 1640.5743713378906 | Memory access (GB): 2.5301513671875 | Attainable performance: 37400 | Computation load: 0.04386562490208264
Layer computation load: 0.06017164999077701

Layer 7:
Eqn 1 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 7: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 27 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.3409423828125 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 52 in layer 7: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.346923828125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 7: Primitive: remat2 | FLOPs (GB): 2315.8556213378906 | Memory access (GB): 4.055419921875 | Attainable performance: 37400 | Computation load: 0.061921273297804565
Layer computation load: 0.08328331642214187

Layer 8:
Eqn 1 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 2 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 28 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 53 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 54 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 88 in layer 8: Primitive: remat2 | FLOPs (GB): 1594.067886352539 | Memory access (GB): 2.5421142652630806 | Attainable performance: 37400 | Computation load: 0.04262213599873099
Layer computation load: 0.05767140314937127

Layer 9:
Eqn 1 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 2 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 28 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 53 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 54 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 89 in layer 9: Primitive: remat2 | FLOPs (GB): 1594.067886352539 | Memory access (GB): 2.5421142652630806 | Attainable performance: 37400 | Computation load: 0.04262213599873099
Layer computation load: 0.058152668864170115

Layer 10:
Eqn 1 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 10: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 11:
Eqn 1 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 11: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 12:
Eqn 1 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 12: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 13:
Eqn 1 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 13: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 27 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.92413330078125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 13: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 53 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 13: Primitive: remat2 | FLOPs (GB): 2269.3282012939453 | Memory access (GB): 6.549682624638081 | Attainable performance: 37400 | Computation load: 0.06067722463352795
Layer computation load: 0.08126377553461002

Layer 14:
Eqn 1 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 2 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.243743896484375 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.00261688232421875 | Memory access (GB): 0.041870128363370895 | Attainable performance: 0 | Computation load: 6.015823040714209e-05
Eqn 28 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 276.390625 | Memory access (GB): 0.892730712890625 | Attainable performance: 37400 | Computation load: 0.007390123663101604
Eqn 53 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 54 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 89 in layer 14: Primitive: remat2 | FLOPs (GB): 1504.4714431762695 | Memory access (GB): 5.0393676944077015 | Attainable performance: 37400 | Computation load: 0.04022650917583608
Layer computation load: 0.05429692268878889

Layer 15:
Eqn 1 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.243743896484375 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.00261688232421875 | Memory access (GB): 0.041870128363370895 | Attainable performance: 0 | Computation load: 6.015823040714209e-05
Eqn 27 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 276.390625 | Memory access (GB): 0.892730712890625 | Attainable performance: 37400 | Computation load: 0.007390123663101604
Eqn 52 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 53 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 82 in layer 15: Primitive: dot_general | FLOPs (GB): 0.4375 | Memory access (GB): 0.0556030347943306 | Attainable performance: 5476.319793088838 | Computation load: 7.988941780794626e-05
Eqn 148 in layer 15: Primitive: remat2 | FLOPs (GB): 1505.7839431762695 | Memory access (GB): 5.206176798790693 | Attainable performance: 37400 | Computation load: 0.040261602758723786
Layer computation load: 0.054171272832085116
Layer memory access: [2.7641637325286865, 8.4996337890625, 4.7791748046875, 6.4599609375, 2.70361328125, 2.70361328125, 2.70361328125, 4.7373046875, 2.60791015625, 2.60791015625, 2.60791015625, 2.60791015625, 2.60791015625, 8.39794921875, 6.328369140625, 6.550781279802322]
[TMP] Optimization status of GPU fraction: optimal

Layer GPU fractions:
[0.159, 0.372, 0.314, 0.459, 0.359, 0.399, 0.427, 0.637, 0.473, 0.51, 0.538, 0.57, 0.603, 0.895, 0.628, 0.657]

Layer GPU fractions:
[0.159, 0.372, 0.314, 0.459, 0.359, 0.399, 0.427, 0.637, 0.473, 0.51, 0.538, 0.57, 0.603, 0.895, 0.628, 0.657]

[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11], [12, 13], [14, 15]]
507140984.0
1.0494693897394052
([4, 2, 1, 1], 1.0494693897394052)

[I] Pipeline partition mode: auto | Layer-to-stage partition: [[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[I] Pipeline partition mode: auto | Cell-generated physical mesh shapes: [(1, 2), (1, 2), (1, 2), (1, 2)]
[I] Loading model and transform to Jaxpr stages takes 18.500755786895752 s.
[I] The GPU sharding of pipeline stages is: [2, 2, 2, 2]
[I] Pipeline partition mode: auto | Parallel enum mode: auto | Parallel plans: [[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]]
[TMP] Plans in the non-dominated set:
0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None)]
0.3492477630565442
28997713920.0

0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.3492477630565442
13674801840.0

0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.44598878909676654
13674801840.0

0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.5472239029867026
13674801840.0

0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.6545273103545792
13674801840.0

0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.7067121054573782
13674801840.0

0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.8078873683874503
13674801840.0

0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10], [11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.883138720700208
13674801840.0

0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9283953899066926
13674801840.0

0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9363108458199125
13674801840.0

0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9404860445535593
13674801840.0

0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9570653060267099
13674801840.0

0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9885474191964693
13674801840.0

0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10], [11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9901585731588655
13674801840.0

0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0115868721963526
13674801840.0

0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0149561566885537
13674801840.0

0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0307269279493962
13674801840.0

0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0547331416050227
13674801840.0

0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0665223860754165
13674801840.0

0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0913047237137752
13674801840.0

0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.117301212744352
13674801840.0

0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.1233797220886623
13674801840.0

0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.153453076635543
13674801840.0

0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.180667607754189
13674801840.0

0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.1894561782596278
13674801840.0

0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.2252795599372412
13674801840.0

0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11], [12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.2378747917297612
13674801840.0

0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3119596030366178
13674801840.0

0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3379962630740045
13674801840.0

0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3387583799924465
13674801840.0

0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2], [3, 4, 5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3881721795224107
13674801840.0

0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2], [3, 4, 5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.408467962006946
13674801840.0

[TMP] Existed tuning database in `/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tuning_database/tuning_database.pkl`, updating/rewriting it...
{'plan_set': {'wide_resnet__1B::256__16::1::a40__2__2': ['0_15::4_1'], 'wide_resnet__1B::256__16::2::a40__2__2': ['0_9__10_15::1_2__1_2', '0_9__10_15::1_2__2_1', '0_9__10_15::2_1__1_2', '0_9__10_15::2_1__2_1', '0_8__9_15::2_1__1_2', '0_8__9_15::2_1__2_1', '0_10__11_15::2_1__1_2', '0_10__11_15::2_1__2_1', '0_7__8_15::2_1__1_2', '0_7__8_15::2_1__2_1', '0_11__12_15::2_1__1_2', '0_11__12_15::2_1__2_1'], 'wide_resnet__1B::256__16::3::a40__2__2': ['0_5__6_9__10_15::2_1__1_1__1_1', '0_6__7_9__10_15::2_1__1_1__1_1', '0_5__6_10__11_15::2_1__1_1__1_1', '0_6__7_10__11_15::2_1__1_1__1_1', '0_3__4_11__12_15::1_1__1_2__1_1', '0_3__4_11__12_15::1_1__2_1__1_1', '0_5__6_11__12_15::2_1__1_1__1_1', '0_6__7_11__12_15::2_1__1_1__1_1', '0_7__8_11__12_15::2_1__1_1__1_1', '0_7__8_12__13_15::2_1__1_1__1_1'], 'wide_resnet__1B::256__16::4::a40__2__2': ['0_4__5_8__9_10__11_15::1_1__1_1__1_1__1_1', '0_3__4_7__8_11__12_15::1_1__1_1__1_1__1_1', '0_4__5_7__8_11__12_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_11__12_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_11__12_15::1_1__1_1__1_1__1_1', '0_3__4_7__8_12__13_15::1_1__1_1__1_1__1_1', '0_3__4_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_12__13_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_12__13_15::1_1__1_1__1_1__1_1', '0_3__4_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_7__8_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_6__7_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_8__9_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_7__8_12__13_14__15_15::1_1__1_1__1_1__1_1', '0_10__11_13__14_14__15_15::1_1__1_1__1_1__1_1', '0_12__13_13__14_14__15_15::1_1__1_1__1_1__1_1'], 'wide_resnet__500M::256__16::2::a40__2__2': ['0_7__8_15::2_1__2_1', '0_8__9_15::2_1__2_1'], 'wide_resnet__500M::512__16::2::a40__2__2': ['0_7__8_15::2_1__2_1', '0_8__9_15::2_1__2_1'], 'wide_resnet__2B::512__16::4::a40__4__2': ['0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2', '0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1'], 'wide_resnet__2B::256__16::4::a40__4__2': ['0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2', '0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1']}, 'selected_cell_num_stages': {'wide_resnet__1B::256__16::a40__2__2': 2}}
[TMP] Storing tuning database to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tuning_database/tuning_database.pkl'...

[I] (Parallel plan idx: 0) Sharding stages...
[I] Sharding stages takes 5.873436212539673 s.
[I] (Parallel plan idx: 0) SPMD partitioning and compiling (kernel fusing) HLO stages...
[I] SPMD partitioning and pre-compiling HLO modules concurrently takes 4.3341310024261475 s.

[I] All parallel plans in the cell have been sharded and compiled.
[TMP] Writing only-sharded HLO text to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_sharded_stages.pkl'...
[TMP] Writing optimized HLO text to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_optimized_stages.pkl'...
[TMP] Writing inter-stages communicated vars to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_inter_stages_comm_vars.pkl'

[I] Parallel plan idx: 0 | Parallelism: [(2, 1), (2, 1), (2, 1), (2, 1)]

[I] (Parallel plan idx: 0) Parsing HLO texts and profiling XLA operators...
[I] Pipeline partition mode: auto | Preset physical mesh shapes: [(1, 2), (1, 2), (1, 2), (1, 2)]
[TMP] Loading offline profiled communication data: `1_a40_1_n_2_d_ib.pkl`...
[TMP] Loading offline profiled communication data: `2_a40_2_n_1_d_ib.pkl`...
\Time of module 0 (stage 0):
Intra-stage comm time: 0.0014068055897951126
Grad sync time: 0

[I] Optimized module 0 for stage 0, comm time has been statically analyzed.
\Time of module 1 (stage 1):
Intra-stage comm time: 0.0016680724918842315
Grad sync time: 0

[I] Optimized module 1 for stage 1, comm time has been statically analyzed.
\Time of module 2 (stage 2):
Intra-stage comm time: 0.0010199837386608123
Grad sync time: 0

[I] Optimized module 2 for stage 2, comm time has been statically analyzed.
\Time of module 3 (stage 3):
Intra-stage comm time: 0.0015505291521549225
Grad sync time: 0

[I] Optimized module 3 for stage 3, comm time has been statically analyzed.
[WARN] Communication size of 'all-reduce' excceeds the maximum profiled size. Linear scaling is applied.
       - Communication size of the operator: 1344320000 | Estimated comm time: 0.666415641275134
       - Max profiled communication size: 267386880 | Comm time: 0.08228549003601074
\Time of module 4 (stage 3):
Intra-stage comm time: 0.002812859229743503
Grad sync time: 0.666415641275134

[I] Optimized module 4 for stage 3, comm time has been statically analyzed.
[WARN] Communication size of 'all-reduce' excceeds the maximum profiled size. Linear scaling is applied.
       - Communication size of the operator: 289089024 | Estimated comm time: 0.09877472304191906
       - Max profiled communication size: 267386880 | Comm time: 0.08228549003601074
\Time of module 5 (stage 2):
Intra-stage comm time: 0.00199982821941376
Grad sync time: 0.09877472304191906

[I] Optimized module 5 for stage 2, comm time has been statically analyzed.
[WARN] Communication size of 'all-reduce' excceeds the maximum profiled size. Linear scaling is applied.
       - Communication size of the operator: 284284672 | Estimated comm time: 0.0972237901204644
       - Max profiled communication size: 267386880 | Comm time: 0.08228549003601074
\Time of module 6 (stage 1):
Intra-stage comm time: 0.003295209538191554
Grad sync time: 0.0972237901204644

[I] Optimized module 6 for stage 1, comm time has been statically analyzed.
\Time of module 7 (stage 0):
Intra-stage comm time: 0.002785365935415028
Grad sync time: 0.01485027360758977

[I] Optimized module 7 for stage 0, comm time has been statically analyzed.
\Time of module 8 (stage 0):
Intra-stage comm time: 0.009255168773233891
Grad sync time: 0

[I] Optimized module 8 for stage 0, comm time has been statically analyzed.
\Time of module 9 (stage 1):
Intra-stage comm time: 0.050099343061447144
Grad sync time: 0

[I] Optimized module 9 for stage 1, comm time has been statically analyzed.
\Time of module 10 (stage 2):
Intra-stage comm time: 0.05057522021234036
Grad sync time: 0

[I] Optimized module 10 for stage 2, comm time has been statically analyzed.
\Time of module 11 (stage 3):
Intra-stage comm time: 0.21919832326471803
Grad sync time: 0

[I] Optimized module 11 for stage 3, comm time has been statically analyzed.
[TMP] Memory footprint of the stage 0 is 1.5223612822592258 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 1 is 6.328260663896799 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 2 is 6.133154395967722 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 3 is 27.713629338890314 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[I] Only-sharded module 0 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.08751764599999998 s.
    - Iteration 1/5: Kernel execution time: 0.08764958199999999 s.
    - Iteration 2/5: Kernel execution time: 0.08867963499999999 s.
    - Iteration 3/5: Kernel execution time: 0.088114286 s.
    - Iteration 4/5: Kernel execution time: 0.08885356200000001 s.
[I] Only-sharded module 1 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.122533461 s.
    - Iteration 1/5: Kernel execution time: 0.121881332 s.
    - Iteration 2/5: Kernel execution time: 0.12245093100000004 s.
    - Iteration 3/5: Kernel execution time: 0.12214033900000003 s.
    - Iteration 4/5: Kernel execution time: 0.12219016499999999 s.
[I] Only-sharded module 2 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.08700557599999999 s.
    - Iteration 1/5: Kernel execution time: 0.08706560799999997 s.
    - Iteration 2/5: Kernel execution time: 0.08723684099999997 s.
    - Iteration 3/5: Kernel execution time: 0.08730993100000001 s.
    - Iteration 4/5: Kernel execution time: 0.08742353300000003 s.
[I] Only-sharded module 3 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.2875285010000001 s.
    - Iteration 1/5: Kernel execution time: 0.2884943020000001 s.
    - Iteration 2/5: Kernel execution time: 0.287721754 s.
    - Iteration 3/5: Kernel execution time: 0.28769238299999994 s.
    - Iteration 4/5: Kernel execution time: 0.287871482 s.
[I] Only-sharded module 4 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.8542444370000003 s.
    - Iteration 1/5: Kernel execution time: 0.8556372420000001 s.
    - Iteration 2/5: Kernel execution time: 0.8547818420000002 s.
    - Iteration 3/5: Kernel execution time: 0.855951096 s.
    - Iteration 4/5: Kernel execution time: 0.8554802069999998 s.
[I] Only-sharded module 5 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.26337295600000005 s.
    - Iteration 1/5: Kernel execution time: 0.26314117 s.
    - Iteration 2/5: Kernel execution time: 0.26254417399999996 s.
    - Iteration 3/5: Kernel execution time: 0.26440515000000003 s.
    - Iteration 4/5: Kernel execution time: 0.262140781 s.
[I] Only-sharded module 6 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.3908697300000002 s.
    - Iteration 1/5: Kernel execution time: 0.390909377 s.
    - Iteration 2/5: Kernel execution time: 0.3898015860000001 s.
    - Iteration 3/5: Kernel execution time: 0.3910527349999999 s.
    - Iteration 4/5: Kernel execution time: 0.3903227669999999 s.
[I] Only-sharded module 7 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.279317426 s.
    - Iteration 1/5: Kernel execution time: 0.2760582940000001 s.
    - Iteration 2/5: Kernel execution time: 0.2766331190000001 s.
    - Iteration 3/5: Kernel execution time: 0.27756143700000013 s.
    - Iteration 4/5: Kernel execution time: 0.276543358 s.
[I] Only-sharded module 8 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.0014200740000000001 s.
    - Iteration 1/5: Kernel execution time: 0.0014247160000000001 s.
    - Iteration 2/5: Kernel execution time: 0.00142193 s.
    - Iteration 3/5: Kernel execution time: 0.001419342 s.
    - Iteration 4/5: Kernel execution time: 0.0014290040000000002 s.
[I] Only-sharded module 9 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.007943303 s.
    - Iteration 1/5: Kernel execution time: 0.007946693000000001 s.
    - Iteration 2/5: Kernel execution time: 0.007942149000000003 s.
    - Iteration 3/5: Kernel execution time: 0.007933859000000001 s.
    - Iteration 4/5: Kernel execution time: 0.007933765 s.
[I] Only-sharded module 10 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.008035685 s.
    - Iteration 1/5: Kernel execution time: 0.008031847 s.
    - Iteration 2/5: Kernel execution time: 0.008033256 s.
    - Iteration 3/5: Kernel execution time: 0.008030404000000001 s.
    - Iteration 4/5: Kernel execution time: 0.008030086 s.
[I] Only-sharded module 11 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.037071653 s.
    - Iteration 1/5: Kernel execution time: 0.037074790999999996 s.
    - Iteration 2/5: Kernel execution time: 0.037066274 s.
    - Iteration 3/5: Kernel execution time: 0.037086179999999996 s.
    - Iteration 4/5: Kernel execution time: 0.037196067 s.
[I] Constructing single-device HLO modules concurrently takes 0.019982337951660156 s.
[I] Compile single-device HLO modules concurrently takes 4.199291706085205 s.
[I] Profile all compiled sequentially on one GPU takes 31.208704233169556 s.
[I] Estimated e2e pipeline iteration time is: 22.248531361186227 s.
    - Kernel computation time is: -1.0 s.
    - Communication time is: -1.0 s.
    - Cross-stages communication time is: -1.0 s.
