
[I] Loading Crius kernel-level profiler...
[TMP] Profiling results not found in `None`, creating it...

[I] Loading model and generating sharded HLO module...
[I] Initializing XLA devices takes 0.579887866973877 s.
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 448
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing general train step func...
[I] General train step func construction is completed.
[I] Model has been loaded, begin Jaxpr transformation...


Layer 0:
Eqn 2 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 24.244239807128906 | Memory access (GB): 0.3441784381866455 | Attainable performance: 37400 | Computation load: 0.0006482417060729654
Eqn 27 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 29 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 18.7578125 | Memory access (GB): 0.1682281494140625 | Attainable performance: 37400 | Computation load: 0.0005015457887700534
Eqn 54 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 55 in layer 0: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 80 in layer 0: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 89 in layer 0: Primitive: remat2 | FLOPs (GB): 1093.8148345947266 | Memory access (GB): 3.6618828773498535 | Attainable performance: 37400 | Computation load: 0.029246385951730656
Layer computation load: 0.04257918890638221

Layer 1:
Eqn 1 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 52 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 53 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 78 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 79 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 104 in layer 1: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 105 in layer 1: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 143 in layer 1: Primitive: remat2 | FLOPs (GB): 2339.6572875976562 | Memory access (GB): 8.719451904296875 | Attainable performance: 37400 | Computation load: 0.06255768148656835
Layer computation load: 0.08677779587685816

Layer 2:
Eqn 1 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 2 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.42169189453125 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 28 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.264678955078125 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 53 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 54 in layer 2: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 2: Primitive: custom_jvp_call | FLOPs (GB): 0.083740234375 | Memory access (GB): 1.33984375 | Attainable performance: 43.5 | Computation load: 0.0019250628591954023
Eqn 89 in layer 2: Primitive: remat2 | FLOPs (GB): 1664.3760375976562 | Memory access (GB): 5.929107666015625 | Attainable performance: 37400 | Computation load: 0.044502033090846424
Layer computation load: 0.064628660875091

Layer 3:
Eqn 1 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 3: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 27 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 329.6494140625 | Memory access (GB): 0.3050537109375 | Attainable performance: 37400 | Computation load: 0.008814155456216577
Eqn 52 in layer 3: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 53 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 3: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5263671875 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 3: Primitive: remat2 | FLOPs (GB): 2339.5735473632812 | Memory access (GB): 5.849853515625 | Attainable performance: 37400 | Computation load: 0.06255544244286848
Layer computation load: 0.0848504939739629

Layer 4:
Eqn 1 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 2 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 28 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 53 in layer 4: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 54 in layer 4: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 88 in layer 4: Primitive: remat2 | FLOPs (GB): 1640.6162414550781 | Memory access (GB): 3.2000732421875 | Attainable performance: 37400 | Computation load: 0.04386674442393257
Layer computation load: 0.06017276951262694

Layer 5:
Eqn 1 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 2 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 28 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 53 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 54 in layer 5: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 5: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 89 in layer 5: Primitive: remat2 | FLOPs (GB): 1640.6162414550781 | Memory access (GB): 3.2000732421875 | Attainable performance: 37400 | Computation load: 0.04386674442393257
Layer computation load: 0.06113530094222464

Layer 6:
Eqn 1 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 27 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.179443359375 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 52 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 53 in layer 6: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 6: Primitive: custom_jvp_call | FLOPs (GB): 0.0418701171875 | Memory access (GB): 0.669921875 | Attainable performance: 43.5 | Computation load: 0.0009625314295977011
Eqn 88 in layer 6: Primitive: remat2 | FLOPs (GB): 1640.5743713378906 | Memory access (GB): 2.5301513671875 | Attainable performance: 37400 | Computation load: 0.04386562490208264
Layer computation load: 0.06017164999077701

Layer 7:
Eqn 1 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.275146484375 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 7: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 27 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 321.75390625 | Memory access (GB): 0.3409423828125 | Attainable performance: 37400 | Computation load: 0.008603045621657753
Eqn 52 in layer 7: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 7: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.346923828125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 7: Primitive: remat2 | FLOPs (GB): 2315.8556213378906 | Memory access (GB): 4.055419921875 | Attainable performance: 37400 | Computation load: 0.061921273297804565
Layer computation load: 0.08328331642214187

Layer 8:
Eqn 1 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 2 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 28 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 53 in layer 8: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 54 in layer 8: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 88 in layer 8: Primitive: remat2 | FLOPs (GB): 1594.067886352539 | Memory access (GB): 2.5421142652630806 | Attainable performance: 37400 | Computation load: 0.04262213599873099
Layer computation load: 0.05767140314937127

Layer 9:
Eqn 1 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 2 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 28 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 53 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 54 in layer 9: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 9: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 89 in layer 9: Primitive: remat2 | FLOPs (GB): 1594.067886352539 | Memory access (GB): 2.5421142652630806 | Attainable performance: 37400 | Computation load: 0.04262213599873099
Layer computation load: 0.058152668864170115

Layer 10:
Eqn 1 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 10: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 10: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 10: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 11:
Eqn 1 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 11: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 11: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 11: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 12:
Eqn 1 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.15252685546875 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 27 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.27813720703125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 53 in layer 12: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 12: Primitive: custom_jvp_call | FLOPs (GB): 0.02093505859375 | Memory access (GB): 0.3349609375 | Attainable performance: 43.5 | Computation load: 0.00048126571479885057
Eqn 88 in layer 12: Primitive: remat2 | FLOPs (GB): 1594.0469512939453 | Memory access (GB): 2.2071533277630806 | Attainable performance: 37400 | Computation load: 0.042621576237806026
Layer computation load: 0.0576708433884463

Layer 13:
Eqn 1 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.2213134765625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 26 in layer 13: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 27 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 306.25 | Memory access (GB): 0.92413330078125 | Attainable performance: 37400 | Computation load: 0.008188502673796791
Eqn 52 in layer 13: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 53 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 78 in layer 13: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.5084228515625 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 114 in layer 13: Primitive: remat2 | FLOPs (GB): 2269.3282012939453 | Memory access (GB): 6.549682624638081 | Attainable performance: 37400 | Computation load: 0.06067722463352795
Layer computation load: 0.08126377553461002

Layer 14:
Eqn 1 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 2 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.243743896484375 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 27 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.00261688232421875 | Memory access (GB): 0.041870128363370895 | Attainable performance: 0 | Computation load: 6.015823040714209e-05
Eqn 28 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 276.390625 | Memory access (GB): 0.892730712890625 | Attainable performance: 37400 | Computation load: 0.007390123663101604
Eqn 53 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 54 in layer 14: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 80 in layer 14: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 89 in layer 14: Primitive: remat2 | FLOPs (GB): 1504.4714431762695 | Memory access (GB): 5.0393676944077015 | Attainable performance: 37400 | Computation load: 0.04022650917583608
Layer computation load: 0.05429692268878889

Layer 15:
Eqn 1 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 75.03125 | Memory access (GB): 0.243743896484375 | Attainable performance: 37400 | Computation load: 0.002006183155080214
Eqn 26 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.00261688232421875 | Memory access (GB): 0.041870128363370895 | Attainable performance: 0 | Computation load: 6.015823040714209e-05
Eqn 27 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 276.390625 | Memory access (GB): 0.892730712890625 | Attainable performance: 37400 | Computation load: 0.007390123663101604
Eqn 52 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.0052337646484375 | Memory access (GB): 0.0837402418255806 | Attainable performance: 0 | Computation load: 0.00012031643940456982
Eqn 53 in layer 15: Primitive: conv_general_dilated | FLOPs (GB): 150.0625 | Memory access (GB): 0.44561767578125 | Attainable performance: 37400 | Computation load: 0.004012366310160428
Eqn 79 in layer 15: Primitive: custom_jvp_call | FLOPs (GB): 0.010467529296875 | Memory access (GB): 0.16748046875 | Attainable performance: 43.5 | Computation load: 0.00024063285739942528
Eqn 82 in layer 15: Primitive: dot_general | FLOPs (GB): 0.4375 | Memory access (GB): 0.0556030347943306 | Attainable performance: 5476.319793088838 | Computation load: 7.988941780794626e-05
Eqn 148 in layer 15: Primitive: remat2 | FLOPs (GB): 1505.7839431762695 | Memory access (GB): 5.206176798790693 | Attainable performance: 37400 | Computation load: 0.040261602758723786
Layer computation load: 0.054171272832085116
Layer memory access: [2.7641637325286865, 8.4996337890625, 4.7791748046875, 6.4599609375, 2.70361328125, 2.70361328125, 2.70361328125, 4.7373046875, 2.60791015625, 2.60791015625, 2.60791015625, 2.60791015625, 2.60791015625, 8.39794921875, 6.328369140625, 6.550781279802322]
[TMP] Optimization status of GPU fraction: optimal

Layer GPU fractions:
[0.159, 0.372, 0.314, 0.459, 0.359, 0.399, 0.427, 0.637, 0.473, 0.51, 0.538, 0.57, 0.603, 0.895, 0.628, 0.657]

Layer GPU fractions:
[0.159, 0.372, 0.314, 0.459, 0.359, 0.399, 0.427, 0.637, 0.473, 0.51, 0.538, 0.57, 0.603, 0.895, 0.628, 0.657]

[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11], [12, 13], [14, 15]]
507140984.0
1.0494693897394052
([4, 2, 1, 1], 1.0494693897394052)

[I] Pipeline partition mode: auto | Layer-to-stage partition: [[0, 1], [2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[I] Pipeline partition mode: auto | Cell-generated physical mesh shapes: [(1, 2), (1, 2), (1, 2), (1, 2)]
[I] Loading model and transform to Jaxpr stages takes 18.822721004486084 s.
[I] The GPU sharding of pipeline stages is: [2, 2, 2, 2]
[I] Pipeline partition mode: auto | Parallel enum mode: auto | Parallel plans: [[StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None)]]
[TMP] Plans in the non-dominated set:
0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None), StageShape(type='single', stage_shape=(1, 2), layer_shape=None)]
0.3492477630565442
28997713920.0

0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.3492477630565442
13674801840.0

0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.44598878909676654
13674801840.0

0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.5472239029867026
13674801840.0

0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.6545273103545792
13674801840.0

0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.7067121054573782
13674801840.0

0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.8078873683874503
13674801840.0

0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10], [11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.883138720700208
13674801840.0

0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9283953899066926
13674801840.0

0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9363108458199125
13674801840.0

0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9404860445535593
13674801840.0

0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9570653060267099
13674801840.0

0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9885474191964693
13674801840.0

0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10], [11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
0.9901585731588655
13674801840.0

0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0115868721963526
13674801840.0

0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0149561566885537
13674801840.0

0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0307269279493962
13674801840.0

0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0547331416050227
13674801840.0

0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0665223860754165
13674801840.0

0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.0913047237137752
13674801840.0

0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8, 9], [10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.117301212744352
13674801840.0

0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11, 12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.1233797220886623
13674801840.0

0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3], [4, 5, 6, 7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.153453076635543
13674801840.0

0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.180667607754189
13674801840.0

0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.1894561782596278
13674801840.0

0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5], [6, 7], [8, 9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.2252795599372412
13674801840.0

0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9, 10, 11], [12, 13], [14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.2378747917297612
13674801840.0

0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4], [5, 6, 7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3119596030366178
13674801840.0

0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8], [9, 10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3379962630740045
13674801840.0

0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1
[[0, 1, 2, 3, 4, 5, 6], [7, 8, 9], [10, 11], [12, 13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3387583799924465
13674801840.0

0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2], [3, 4, 5, 6, 7], [8, 9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.3881721795224107
13674801840.0

0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1
[[0, 1, 2], [3, 4, 5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15]]
[StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None), StageShape(type='single', stage_shape=(2, 1), layer_shape=None)]
1.408467962006946
13674801840.0

[TMP] Existed tuning database in `/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tuning_database/tuning_database.pkl`, updating/rewriting it...
{'plan_set': {'wide_resnet__1B::256__16::1::a40__2__2': ['0_15::4_1'], 'wide_resnet__1B::256__16::2::a40__2__2': ['0_9__10_15::1_2__1_2', '0_9__10_15::1_2__2_1', '0_9__10_15::2_1__1_2', '0_9__10_15::2_1__2_1', '0_8__9_15::2_1__1_2', '0_8__9_15::2_1__2_1', '0_10__11_15::2_1__1_2', '0_10__11_15::2_1__2_1', '0_7__8_15::2_1__1_2', '0_7__8_15::2_1__2_1', '0_11__12_15::2_1__1_2', '0_11__12_15::2_1__2_1'], 'wide_resnet__1B::256__16::3::a40__2__2': ['0_5__6_9__10_15::2_1__1_1__1_1', '0_6__7_9__10_15::2_1__1_1__1_1', '0_5__6_10__11_15::2_1__1_1__1_1', '0_6__7_10__11_15::2_1__1_1__1_1', '0_3__4_11__12_15::1_1__1_2__1_1', '0_3__4_11__12_15::1_1__2_1__1_1', '0_5__6_11__12_15::2_1__1_1__1_1', '0_6__7_11__12_15::2_1__1_1__1_1', '0_7__8_11__12_15::2_1__1_1__1_1', '0_7__8_12__13_15::2_1__1_1__1_1'], 'wide_resnet__1B::256__16::4::a40__2__2': ['0_4__5_8__9_10__11_15::1_1__1_1__1_1__1_1', '0_3__4_7__8_11__12_15::1_1__1_1__1_1__1_1', '0_4__5_7__8_11__12_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_11__12_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_11__12_15::1_1__1_1__1_1__1_1', '0_3__4_7__8_12__13_15::1_1__1_1__1_1__1_1', '0_3__4_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_12__13_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_12__13_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_12__13_15::1_1__1_1__1_1__1_1', '0_3__4_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_5__6_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_7__8_10__11_13__14_15::1_1__1_1__1_1__1_1', '0_4__5_8__9_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_8__9_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_9__10_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_9__10_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_6__7_10__11_14__15_15::1_1__1_1__1_1__1_1', '0_4__5_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_5__6_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_8__9_11__12_14__15_15::1_1__1_1__1_1__1_1', '0_7__8_12__13_14__15_15::1_1__1_1__1_1__1_1', '0_10__11_13__14_14__15_15::1_1__1_1__1_1__1_1', '0_12__13_13__14_14__15_15::1_1__1_1__1_1__1_1'], 'wide_resnet__500M::256__16::2::a40__2__2': ['0_7__8_15::2_1__2_1', '0_8__9_15::2_1__2_1'], 'wide_resnet__500M::512__16::2::a40__2__2': ['0_7__8_15::2_1__2_1', '0_8__9_15::2_1__2_1'], 'wide_resnet__2B::512__16::4::a40__4__2': ['0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2', '0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1'], 'wide_resnet__2B::256__16::4::a40__4__2': ['0_5__6_9__10_12__13_15::1_2__1_2__1_2__1_2', '0_5__6_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_4__5_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_10__11_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_12__13_15::2_1__2_1__2_1__2_1', '0_3__4_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_9__10_12__13_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_13__14_15::2_1__2_1__2_1__2_1', '0_3__4_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_5__6_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_5__6_7__8_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_11__12_13__14_15::2_1__2_1__2_1__2_1', '0_4__5_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_8__9_11__12_15::2_1__2_1__2_1__2_1', '0_6__7_9__10_11__12_15::2_1__2_1__2_1__2_1', '0_2__3_7__8_12__13_15::2_1__2_1__2_1__2_1', '0_2__3_8__9_12__13_15::2_1__2_1__2_1__2_1']}, 'selected_cell_num_stages': {'wide_resnet__1B::256__16::a40__2__2': 2}}
[TMP] Storing tuning database to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tuning_database/tuning_database.pkl'...

[I] (Parallel plan idx: 0) Sharding stages...
[I] Sharding stages takes 7.672144651412964 s.
[I] (Parallel plan idx: 0) SPMD partitioning and compiling (kernel fusing) HLO stages...
[I] SPMD partitioning and pre-compiling HLO modules concurrently takes 7.06699013710022 s.

[I] All parallel plans in the cell have been sharded and compiled.
[TMP] Writing only-sharded HLO text to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_sharded_stages.pkl'...
[TMP] Writing optimized HLO text to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_optimized_stages.pkl'...
[TMP] Writing inter-stages communicated vars to '/home/cyxue/Projects/crius/Crius/runtime/jaxpr/tmp/wide_resnet_2B_256_inter_stages_comm_vars.pkl'

[I] Parallel plan idx: 0 | Parallelism: [(1, 2), (1, 2), (1, 2), (1, 2)]

[I] (Parallel plan idx: 0) Parsing HLO texts and profiling XLA operators...
[I] Pipeline partition mode: auto | Preset physical mesh shapes: [(1, 2), (1, 2), (1, 2), (1, 2)]
[TMP] Loading offline profiled communication data: `1_a40_1_n_2_d_ib.pkl`...
[TMP] Loading offline profiled communication data: `2_a40_2_n_1_d_ib.pkl`...
\Time of module 0 (stage 0):
Intra-stage comm time: 0.034703241521492595
Grad sync time: 0

[I] Optimized module 0 for stage 0, comm time has been statically analyzed.
\Time of module 1 (stage 1):
Intra-stage comm time: 0.08506852705172743
Grad sync time: 0

[I] Optimized module 1 for stage 1, comm time has been statically analyzed.
\Time of module 2 (stage 2):
Intra-stage comm time: 0.025090472865849735
Grad sync time: 0

[I] Optimized module 2 for stage 2, comm time has been statically analyzed.
\Time of module 3 (stage 3):
Intra-stage comm time: 0.007613656716421246
Grad sync time: 0

[I] Optimized module 3 for stage 3, comm time has been statically analyzed.
\Time of module 4 (stage 3):
Intra-stage comm time: 0.020770953276132785
Grad sync time: 0

[I] Optimized module 4 for stage 3, comm time has been statically analyzed.
\Time of module 5 (stage 2):
Intra-stage comm time: 0.04957868005149067
Grad sync time: 0

[I] Optimized module 5 for stage 2, comm time has been statically analyzed.
\Time of module 6 (stage 1):
Intra-stage comm time: 0.14286349080502989
Grad sync time: 0

[I] Optimized module 6 for stage 1, comm time has been statically analyzed.
\Time of module 7 (stage 0):
Intra-stage comm time: 0.08074344775893472
Grad sync time: 0

[I] Optimized module 7 for stage 0, comm time has been statically analyzed.
\Time of module 8 (stage 0):
Intra-stage comm time: 0.00019242540001869203
Grad sync time: 0

[I] Optimized module 8 for stage 0, comm time has been statically analyzed.
\Time of module 9 (stage 1):
Intra-stage comm time: 0.0007214285433292388
Grad sync time: 0

[I] Optimized module 9 for stage 1, comm time has been statically analyzed.
\Time of module 10 (stage 2):
Intra-stage comm time: 0.00032697245478630066
Grad sync time: 0

[I] Optimized module 10 for stage 2, comm time has been statically analyzed.
\Time of module 11 (stage 3):
Intra-stage comm time: 0.0001981079578399658
Grad sync time: 0

[I] Optimized module 11 for stage 3, comm time has been statically analyzed.
[TMP] Memory footprint of the stage 0 is 0.5326832570135593 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 1 is 3.957438711076975 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 2 is 5.762985710054636 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[TMP] Memory footprint of the stage 3 is 14.15859142318368 GB, while the available memory of the XLA device is 35.33237304631621 GB.
[I] Only-sharded module 0 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.042372849000000004 s.
    - Iteration 1/5: Kernel execution time: 0.04273314 s.
    - Iteration 2/5: Kernel execution time: 0.042958969 s.
    - Iteration 3/5: Kernel execution time: 0.043428382 s.
    - Iteration 4/5: Kernel execution time: 0.043580731000000004 s.
[I] Only-sharded module 1 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.13272244699999997 s.
    - Iteration 1/5: Kernel execution time: 0.13329592899999998 s.
    - Iteration 2/5: Kernel execution time: 0.13305307499999997 s.
    - Iteration 3/5: Kernel execution time: 0.13330865899999997 s.
    - Iteration 4/5: Kernel execution time: 0.132535915 s.
[I] Only-sharded module 2 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.10116117600000002 s.
    - Iteration 1/5: Kernel execution time: 0.100794707 s.
    - Iteration 2/5: Kernel execution time: 0.101386556 s.
    - Iteration 3/5: Kernel execution time: 0.10211763600000004 s.
    - Iteration 4/5: Kernel execution time: 0.10090489000000001 s.
[I] Only-sharded module 3 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.118695982 s.
    - Iteration 1/5: Kernel execution time: 0.11883048600000005 s.
    - Iteration 2/5: Kernel execution time: 0.11884776500000008 s.
    - Iteration 3/5: Kernel execution time: 0.11874561699999998 s.
    - Iteration 4/5: Kernel execution time: 0.11871716900000001 s.
[I] Only-sharded module 4 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.5739981310000001 s.
    - Iteration 1/5: Kernel execution time: 0.5742240850000001 s.
    - Iteration 2/5: Kernel execution time: 0.573325641 s.
    - Iteration 3/5: Kernel execution time: 0.5733520730000001 s.
    - Iteration 4/5: Kernel execution time: 0.574950618 s.
[I] Only-sharded module 5 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.3382064559999998 s.
    - Iteration 1/5: Kernel execution time: 0.335505424 s.
    - Iteration 2/5: Kernel execution time: 0.33654716899999987 s.
    - Iteration 3/5: Kernel execution time: 0.3370803560000001 s.
    - Iteration 4/5: Kernel execution time: 0.335659219 s.
[I] Only-sharded module 6 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.436896305 s.
    - Iteration 1/5: Kernel execution time: 0.4365035980000001 s.
    - Iteration 2/5: Kernel execution time: 0.4373285090000001 s.
    - Iteration 3/5: Kernel execution time: 0.4371545780000001 s.
    - Iteration 4/5: Kernel execution time: 0.43610487599999975 s.
[I] Only-sharded module 7 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.13026096800000006 s.
    - Iteration 1/5: Kernel execution time: 0.128583743 s.
    - Iteration 2/5: Kernel execution time: 0.128164787 s.
    - Iteration 3/5: Kernel execution time: 0.12981717799999992 s.
    - Iteration 4/5: Kernel execution time: 0.128946937 s.
[I] Only-sharded module 8 for stage 0, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.000260514 s.
    - Iteration 1/5: Kernel execution time: 0.00026109 s.
    - Iteration 2/5: Kernel execution time: 0.000261347 s.
    - Iteration 3/5: Kernel execution time: 0.000260484 s.
    - Iteration 4/5: Kernel execution time: 0.000260002 s.
[I] Only-sharded module 9 for stage 1, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.004325672 s.
    - Iteration 1/5: Kernel execution time: 0.004327749 s.
    - Iteration 2/5: Kernel execution time: 0.004327588 s.
    - Iteration 3/5: Kernel execution time: 0.0043282319999999996 s.
    - Iteration 4/5: Kernel execution time: 0.004329605 s.
[I] Only-sharded module 10 for stage 2, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.009429971 s.
    - Iteration 1/5: Kernel execution time: 0.009435540000000001 s.
    - Iteration 2/5: Kernel execution time: 0.009436182000000001 s.
    - Iteration 3/5: Kernel execution time: 0.009438708 s.
    - Iteration 4/5: Kernel execution time: 0.009427509 s.
[I] Only-sharded module 11 for stage 3, constructing entry XLA computation...
[I] Compiling HLO module...
[I] Profiling HLO modules...
    - Iteration 0/5: Kernel execution time: 0.024458812 s.
    - Iteration 1/5: Kernel execution time: 0.024446848 s.
    - Iteration 2/5: Kernel execution time: 0.024447869 s.
    - Iteration 3/5: Kernel execution time: 0.024436092000000003 s.
    - Iteration 4/5: Kernel execution time: 0.024459038000000002 s.
[I] Constructing single-device HLO modules concurrently takes 0.026960134506225586 s.
[I] Compile single-device HLO modules concurrently takes 1.2344934940338135 s.
[I] Profile all compiled sequentially on one GPU takes 22.314361333847046 s.
[I] Estimated e2e pipeline iteration time is: 18.787144294312338 s.
    - Kernel computation time is: -1.0 s.
    - Communication time is: -1.0 s.
    - Cross-stages communication time is: -1.0 s.
