[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_760M_128.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f636b32cc70>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 43.98 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 19.74 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 58.57 s

[46.9134726524353, 2.9003376960754395]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 2.95 s.
 - Average e2e iteration time: 2.950000047683716 s.
 - Total local training time: 2.9000000953674316 s.
 - Average local iteration time: 2.9000000953674316 s.
 - Max allocated memory among devices: 6.902 GB.
 - Compilation times:  {'stage-construction': 0.01779317855834961, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 127.39405679702759 (GPU time = 1019.1524543762207) s.

[I] The e2e profiling overhead with estimation is 128.12475609779358 s (GPU time = 1024.9980487823486 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 2.9003376960754395)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_760M_128.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_1.3B_128.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fb67804c2b0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 42.24 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 18.53 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 22.45 s

[7.297495365142822, 4.778598070144653]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 4.824 s.
 - Average e2e iteration time: 4.824000358581543 s.
 - Total local training time: 4.779000282287598 s.
 - Average local iteration time: 4.779000282287598 s.
 - Max allocated memory among devices: 10.882 GB.
 - Compilation times:  {'stage-construction': 0.01655411720275879, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 89.22516775131226 (GPU time = 713.801342010498) s.

[I] The e2e profiling overhead with estimation is 89.96502470970154 s (GPU time = 719.7201976776123 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 4.778597831726074)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_1.3B_128.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_2.6B_128.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fcb8bb7bcd0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 62.13 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 24.16 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 35.58 s

[10.504088401794434, 8.100921154022217]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 8.149 s.
 - Average e2e iteration time: 8.14900016784668 s.
 - Total local training time: 8.101000785827637 s.
 - Average local iteration time: 8.101000785827637 s.
 - Max allocated memory among devices: 20.645 GB.
 - Compilation times:  {'stage-construction': 0.024553537368774414, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 129.55278968811035 (GPU time = 1036.4223175048828) s.

[I] The e2e profiling overhead with estimation is 130.300776720047 s (GPU time = 1042.406213760376 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 8.100921630859375)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_2.6B_128.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_760M_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7fe5e126cd00>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 40.95 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 19.05 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 20.64 s

[7.014316082000732, 4.540262460708618]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 4.583 s.
 - Average e2e iteration time: 4.583000183105469 s.
 - Total local training time: 4.5400004386901855 s.
 - Average local iteration time: 4.5400004386901855 s.
 - Max allocated memory among devices: 7.934 GB.
 - Compilation times:  {'stage-construction': 0.017911672592163086, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 86.20166397094727 (GPU time = 689.6133117675781) s.

[I] The e2e profiling overhead with estimation is 86.9444968700409 s (GPU time = 695.5559749603271 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 4.540262222290039)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_760M_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_1.3B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f3a6bfa4d00>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 42.78 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 18.80 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 28.36 s

[10.121009588241577, 7.691628932952881]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 7.743 s.
 - Average e2e iteration time: 7.743000507354736 s.
 - Total local training time: 7.692000389099121 s.
 - Average local iteration time: 7.692000389099121 s.
 - Max allocated memory among devices: 11.82 GB.
 - Compilation times:  {'stage-construction': 0.0178070068359375, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 95.41108870506287 (GPU time = 763.2887096405029) s.

[I] The e2e profiling overhead with estimation is 96.1581974029541 s (GPU time = 769.2655792236328 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 7.691628932952881)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_1.3B_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_2.6B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f5b7c5a7820>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 62.91 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 25.35 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 44.03 s

[15.314900636672974, 12.881393432617188]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 12.933 s.
 - Average e2e iteration time: 12.933000564575195 s.
 - Total local training time: 12.881000518798828 s.
 - Average local iteration time: 12.881000518798828 s.
 - Max allocated memory among devices: 21.817 GB.
 - Compilation times:  {'stage-construction': 0.024974346160888672, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 140.61099696159363 (GPU time = 1124.887975692749) s.

[I] The e2e profiling overhead with estimation is 141.36968541145325 s (GPU time = 1130.957483291626 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 12.881393432617188)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_2.6B_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_760M_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f2bab0e4970>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 43.22 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 18.93 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 27.39 s

[10.381577014923096, 7.929229259490967]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 7.977 s.
 - Average e2e iteration time: 7.9770002365112305 s.
 - Total local training time: 7.929000377655029 s.
 - Average local iteration time: 7.929000377655029 s.
 - Max allocated memory among devices: 9.997 GB.
 - Compilation times:  {'stage-construction': 0.0172579288482666, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 94.7621169090271 (GPU time = 758.0969352722168) s.

[I] The e2e profiling overhead with estimation is 95.50977325439453 s (GPU time = 764.0781860351562 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 7.929229259490967)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_760M_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_1.3B_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f018f2b0ac0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 44.14 s
compilation time breakdown: {'stage-construction': '0.02', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 19.53 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 40.36 s

[16.05571699142456, 13.610286712646484]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 14.13 s.
 - Average e2e iteration time: 14.130001068115234 s.
 - Total local training time: 13.610000610351562 s.
 - Average local iteration time: 13.610000610351562 s.
 - Max allocated memory among devices: 13.695 GB.
 - Compilation times:  {'stage-construction': 0.016692399978637695, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 109.52906060218811 (GPU time = 876.2324848175049) s.

[I] The e2e profiling overhead with estimation is 110.28637337684631 s (GPU time = 882.2909870147705 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 13.610286712646484)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_1.3B_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/bert_2.6B_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'bert' loads sucessfully. Model Info: 

FlaxBertForMaskedLMModule(
    # attributes
    config = <model.bert_model.BertConfig object at 0x7f1ff4f44cd0>
    dtype = float16
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 62.64 s
compilation time breakdown: {'stage-construction': '0.03', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 26.19 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 65.86 s

[25.937764167785645, 23.468645095825195]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 23.527 s.
 - Average e2e iteration time: 23.527000427246094 s.
 - Total local training time: 23.46900177001953 s.
 - Average local iteration time: 23.46900177001953 s.
 - Max allocated memory among devices: 24.161 GB.
 - Compilation times:  {'stage-construction': 0.025075435638427734, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 162.88611817359924 (GPU time = 1303.088945388794) s.

[I] The e2e profiling overhead with estimation is 163.62339210510254 s (GPU time = 1308.9871368408203 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 23.468645095825195)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/bert_2.6B_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_690M_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f81c8f82a30>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 23.08 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 15.59 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 9.67 s

[4.277182340621948, 1.9207942485809326]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 1.953 s.
 - Average e2e iteration time: 1.9530000686645508 s.
 - Total local training time: 1.9210001230239868 s.
 - Average local iteration time: 1.9210001230239868 s.
 - Max allocated memory among devices: 6.201 GB.
 - Compilation times:  {'stage-construction': 0.006921291351318359, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 51.63680148124695 (GPU time = 413.0944118499756) s.

[I] The e2e profiling overhead with estimation is 52.30163025856018 s (GPU time = 418.41304206848145 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 1.9207942485809326)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_690M_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_1.3B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7fa1d587ccd0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 43.13 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.42 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 15.87 s

[6.086019515991211, 3.6611194610595703]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 3.709 s.
 - Average e2e iteration time: 3.7090001106262207 s.
 - Total local training time: 3.6610002517700195 s.
 - Average local iteration time: 3.6610002517700195 s.
 - Max allocated memory among devices: 9.357 GB.
 - Compilation times:  {'stage-construction': 0.01169896125793457, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 94.30780076980591 (GPU time = 754.4624061584473) s.

[I] The e2e profiling overhead with estimation is 95.06952285766602 s (GPU time = 760.5561828613281 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 3.6611194610595703)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_1.3B_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_2.4B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f4878c3e370>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 44.53 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 29.25 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 21.56 s

[7.772405624389648, 5.40918493270874]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 5.452 s.
 - Average e2e iteration time: 5.452000141143799 s.
 - Total local training time: 5.409000396728516 s.
 - Average local iteration time: 5.409000396728516 s.
 - Max allocated memory among devices: 16.072 GB.
 - Compilation times:  {'stage-construction': 0.011963129043579102, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 100.62327790260315 (GPU time = 804.9862232208252) s.

[I] The e2e profiling overhead with estimation is 101.36758351325989 s (GPU time = 810.9406681060791 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 5.40918493270874)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_2.4B_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_10B_256.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f203b8b5bb0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 44.73 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.35 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 175.61 s

[]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 0.072 s.
 - Average e2e iteration time: 0.07200000435113907 s.
 - Total local training time: 0.0 s.
 - Average local iteration time: nan s.
 - Max allocated memory among devices: 35.068 GB.
 - Compilation times:  {'stage-construction': 0.011639118194580078, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 260.6088559627533 (GPU time = 2084.8708477020264) s.

[I] The e2e profiling overhead with estimation is 261.3621644973755 s (GPU time = 2090.897315979004 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), nan)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_10B_256.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_690M_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f7390cd1c70>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 22.80 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 15.79 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 12.04 s

[5.299357652664185, 2.925347328186035]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 2.972 s.
 - Average e2e iteration time: 2.9720001220703125 s.
 - Total local training time: 2.9250001907348633 s.
 - Average local iteration time: 2.9250001907348633 s.
 - Max allocated memory among devices: 8.095 GB.
 - Compilation times:  {'stage-construction': 0.0069539546966552734, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 53.797760009765625 (GPU time = 430.382080078125) s.

[I] The e2e profiling overhead with estimation is 54.493207693099976 s (GPU time = 435.9456615447998 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 2.925347328186035)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_690M_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_1.3B_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f123232ac10>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 45.25 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.31 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 19.11 s

[8.024785280227661, 5.592933177947998]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 5.629 s.
 - Average e2e iteration time: 5.629000186920166 s.
 - Total local training time: 5.593000411987305 s.
 - Average local iteration time: 5.593000411987305 s.
 - Max allocated memory among devices: 10.405 GB.
 - Compilation times:  {'stage-construction': 0.011693239212036133, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 100.13392972946167 (GPU time = 801.0714378356934) s.

[I] The e2e profiling overhead with estimation is 100.91072845458984 s (GPU time = 807.2858276367188 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 5.592933177947998)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_1.3B_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_2.4B_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f9d17c9af10>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 44.13 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.11 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 27.42 s

[10.499234676361084, 8.063841581344604]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 8.108 s.
 - Average e2e iteration time: 8.108000755310059 s.
 - Total local training time: 8.064000129699707 s.
 - Average local iteration time: 8.064000129699707 s.
 - Max allocated memory among devices: 17.51 GB.
 - Compilation times:  {'stage-construction': 0.012095451354980469, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 106.8957736492157 (GPU time = 855.1661891937256) s.

[I] The e2e profiling overhead with estimation is 107.66613292694092 s (GPU time = 861.3290634155273 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 8.063841819763184)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_2.4B_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_10B_512.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7fc58b42ad00>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 44.50 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.71 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 136.63 s

[]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 0.074 s.
 - Average e2e iteration time: 0.07400000095367432 s.
 - Total local training time: 0.0 s.
 - Average local iteration time: nan s.
 - Max allocated memory among devices: 35.069 GB.
 - Compilation times:  {'stage-construction': 0.01190328598022461, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 221.8348207473755 (GPU time = 1774.678565979004) s.

[I] The e2e profiling overhead with estimation is 222.5933859348297 s (GPU time = 1780.7470874786377 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), nan)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_10B_512.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_690M_1024.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7fa7da181be0>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 23.87 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 15.96 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 15.90 s

[7.329142332077026, 4.962899684906006]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 5.006 s.
 - Average e2e iteration time: 5.00600004196167 s.
 - Total local training time: 4.963000297546387 s.
 - Average local iteration time: 4.963000297546387 s.
 - Max allocated memory among devices: 11.962 GB.
 - Compilation times:  {'stage-construction': 0.0070934295654296875, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 59.085556507110596 (GPU time = 472.68445205688477) s.

[I] The e2e profiling overhead with estimation is 59.86046504974365 s (GPU time = 478.8837203979492 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 4.962899684906006)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_690M_1024.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_1.3B_1024.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f1bf1759640>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 43.69 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.00 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 26.79 s

[11.915166854858398, 9.433870792388916]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 9.48 s.
 - Average e2e iteration time: 9.480000495910645 s.
 - Total local training time: 9.434000015258789 s.
 - Average local iteration time: 9.434000015258789 s.
 - Max allocated memory among devices: 12.499 GB.
 - Compilation times:  {'stage-construction': 0.011731147766113281, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 105.60619378089905 (GPU time = 844.8495502471924) s.

[I] The e2e profiling overhead with estimation is 106.32127332687378 s (GPU time = 850.5701866149902 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 9.433870315551758)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_1.3B_1024.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_2.4B_1024.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f09746fac70>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 46.41 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 29.64 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 36.72 s

[15.740918159484863, 13.295413255691528]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 13.352 s.
 - Average e2e iteration time: 13.35200023651123 s.
 - Total local training time: 13.295001029968262 s.
 - Average local iteration time: 13.295001029968262 s.
 - Max allocated memory among devices: 20.386 GB.
 - Compilation times:  {'stage-construction': 0.011413097381591797, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 118.66158199310303 (GPU time = 949.2926559448242) s.

[I] The e2e profiling overhead with estimation is 119.43475580215454 s (GPU time = 955.4780464172363 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), 13.29541301727295)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_2.4B_1024.pkl`...
[TMP] Existed profiling results in `./jaxpr/prof_log/ground_truth/moe_10B_1024.pkl`, updating/rewriting it...
[I] Setting sync timer in Alpa workers to perform accurate execution time.
[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 2
    - Nodes num: 4
[I] Loading flax model....
[I] Model 'moe' loads sucessfully. Model Info: 

FlaxMoEForLMModule(
    # attributes
    config = <model.moe_model.MoEConfig object at 0x7f9efd04ba60>
    dtype = float16
    bias_init = zeros
) 

[I] Manually constructing dummy batch...
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Manually slice pipeline and sharding, the related specs are as follows:
    - 'forward_stage_layer_id': [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
    - 'submesh_physical_shapes': [(4, 2)]
    - 'submesh_logical_shapes': [(8, 1)]
    - 'submesh_autosharding_option_dicts': [{'force_batch_dim_to_mesh_dim': 0}]
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
 - Compile (driver): 46.00 s
compilation time breakdown: {'stage-construction': '0.01', 'stage-construction-dp': '0.00', 'stage-construction-compilation': '0.00', 'stage-construction-profiling': '0.00'}
 - Compile (worker): 30.00 s
[I] Training process warmup (1 rounds) with dummy input batch...
    - Warmup iteration: 1/1
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
 - Benchmark: 133.97 s

[]


[I] Performance metrics:
 - Iteration count: 1.
 - Total e2e training time : 0.063 s.
 - Average e2e iteration time: 0.06300000101327896 s.
 - Total local training time: 0.0 s.
 - Average local iteration time: nan s.
 - Max allocated memory among devices: 35.071 GB.
 - Compilation times:  {'stage-construction': 0.0114288330078125, 'stage-construction-dp': 0.0, 'stage-construction-compilation': 0.0, 'stage-construction-profiling': 0.0}
 - Metadata:  []
 - Is need save result:  False

[I] Measuring one parallelzing configuration takes 220.75439310073853 (GPU time = 1766.0351448059082) s.

[I] The e2e profiling overhead with estimation is 221.57262802124023 s (GPU time = 1772.5810241699219 s)
[TMP] Current profiling results of key `4_a40_4_n_2_d`: [((1, 8, 1), nan)]
[TMP] Updated profiling results stored in `./jaxpr/prof_log/ground_truth/moe_10B_1024.pkl`...
