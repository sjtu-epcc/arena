
########################################
#     Re-constructing Ray Cluster      #
########################################

[I][SHELL] The path of the configuration file is: 


[I][SHELL] The Ray cluster re-construction is completed. 
[I][SHELL] To manually shut down the Ray cluster, use:
 - (on head node) ray down [FILE_PATH] -y
 - (on worker node) ray stop --force

########################################
#       Executing Alpa Profiling       #
########################################

########################################
Ray Cluster Info:
 - Ray address: auto
 - Devices num: 1_a40
 - Nodes num: 1
 - Devices num per node: 1
########################################
DL Job Info:
 - Job UUID: default
 - Model name: wide_resnet
 - Param num: 500M
 - Dataset name: none
 - Batch size: 256
 - Pipeline layer num: 16
 - Miro-batches num: 16
 - Iteration num: 2
 - Is DP only: false
 - Is PP only: false
 - Is MP only: false
 - Is dummy test: false
 - Is manual config test: false
 - Output log path: /home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/profile_result/1_nodes_1_devices_per_node/1_a40/wide_resnet_none/bs_256_nmb_16_pln_16_param_500M_try_1.log
########################################

[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Dump debug file path: ./profile_result/debug/1_nodes_1_devices_per_node/1_a40/wide_resnet_none/bs_256_nmb_16_pln_16_param_num_500M
[I] Manually construct dummy batch with the shape of (256, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1),)
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.892 GB, invar_size=1.855 GB, outvar_size=1.089 GB, temp_buffer_size=0.948 GB, available_memory=35.130 GB)
result[(0, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=3.892 GB, invar_size=1.855 GB, outvar_size=1.089 GB, temp_buffer_size=0.948 GB, available_memory=35.130 GB)
result[(0, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.209 GB, invar_size=4.786 GB, outvar_size=1.844 GB, temp_buffer_size=1.423 GB, available_memory=35.130 GB)
result[(0, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=6.209 GB, invar_size=4.786 GB, outvar_size=1.844 GB, temp_buffer_size=1.423 GB, available_memory=35.130 GB)
Profiling for submesh 0 (1, 1) takes 20.84 seconds
--------------------------------------------------
Profile result saved to: ./profile_result/profile-results-2023-10-30-23-05-24.npy
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 1)]
Result logical_mesh_shapes: [(1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 38.09 s
compilation time breakdown: {'stage-construction': '22.61', 'stage-construction-dp': '1.06', 'stage-construction-compilation': '3.26', 'stage-construction-profiling': '14.21'}
 - Compile (worker): 13.42 s
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 2 is performed...
 - Benchmark: 35.76 s

[I] Performance metrics:
 - Iteration count: 2.
 - Total e2e training time : 35.752 s.
 - Average e2e iteration time: 17.876001358032227 s.
 - Total local training time: 32.34000015258789 s.
 - Average local iteration time: 16.170000076293945 s.
 - Max allocated memory among devices: 8.189 GB.
 - Compilation times:  {'stage-construction': 22.613354444503784, 'stage-construction-dp': 1.0629327297210693, 'stage-construction-compilation': 3.2596962451934814, 'stage-construction-profiling': 14.208915710449219}
 - Metadata:  []
 - Is need save result:  True


########################################
#     Re-constructing Ray Cluster      #
########################################

[I][SHELL] The path of the configuration file is: 


[I][SHELL] The Ray cluster re-construction is completed. 
[I][SHELL] To manually shut down the Ray cluster, use:
 - (on head node) ray down [FILE_PATH] -y
 - (on worker node) ray stop --force

########################################
#       Executing Alpa Profiling       #
########################################

########################################
Ray Cluster Info:
 - Ray address: auto
 - Devices num: 1_a40
 - Nodes num: 1
 - Devices num per node: 1
########################################
DL Job Info:
 - Job UUID: default
 - Model name: wide_resnet
 - Param num: 500M
 - Dataset name: none
 - Batch size: 512
 - Pipeline layer num: 16
 - Miro-batches num: 16
 - Iteration num: 2
 - Is DP only: false
 - Is PP only: false
 - Is MP only: false
 - Is dummy test: false
 - Is manual config test: false
 - Output log path: /home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/profile_result/1_nodes_1_devices_per_node/1_a40/wide_resnet_none/bs_512_nmb_16_pln_16_param_500M_try_1.log
########################################

[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Dump debug file path: ./profile_result/debug/1_nodes_1_devices_per_node/1_a40/wide_resnet_none/bs_512_nmb_16_pln_16_param_num_500M
[I] Manually construct dummy batch with the shape of (512, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1),)
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.652 GB, invar_size=5.883 GB, outvar_size=1.844 GB, temp_buffer_size=2.768 GB, available_memory=35.130 GB)
result[(0, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.465 GB, invar_size=1.864 GB, outvar_size=2.178 GB, temp_buffer_size=1.424 GB, available_memory=35.130 GB)
result[(0, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=8.652 GB, invar_size=5.883 GB, outvar_size=1.844 GB, temp_buffer_size=2.768 GB, available_memory=35.130 GB)
result[(0, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=5.465 GB, invar_size=1.864 GB, outvar_size=2.178 GB, temp_buffer_size=1.424 GB, available_memory=35.130 GB)
Profiling for submesh 0 (1, 1) takes 30.46 seconds
--------------------------------------------------
Profile result saved to: ./profile_result/profile-results-2023-10-30-23-07-41.npy
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 1)]
Result logical_mesh_shapes: [(1, 1)]
Result autosharding_option_dicts: [{'force_batch_dim_to_mesh_dim': 0}]
 - Compile (driver): 47.57 s
compilation time breakdown: {'stage-construction': '32.25', 'stage-construction-dp': '1.06', 'stage-construction-compilation': '3.40', 'stage-construction-profiling': '23.79'}
 - Compile (worker): 24.59 s
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 2 is performed...
 - Benchmark: 61.25 s

[I] Performance metrics:
 - Iteration count: 2.
 - Total e2e training time : 61.242 s.
 - Average e2e iteration time: 30.621002197265625 s.
 - Total local training time: 56.90700149536133 s.
 - Average local iteration time: 28.453001022338867 s.
 - Max allocated memory among devices: 10.767 GB.
 - Compilation times:  {'stage-construction': 32.249287605285645, 'stage-construction-dp': 1.064319372177124, 'stage-construction-compilation': 3.395123243331909, 'stage-construction-profiling': 23.792552947998047}
 - Metadata:  []
 - Is need save result:  True


########################################
#     Re-constructing Ray Cluster      #
########################################

[I][SHELL] The path of the configuration file is: 


[I][SHELL] The Ray cluster re-construction is completed. 
[I][SHELL] To manually shut down the Ray cluster, use:
 - (on head node) ray down [FILE_PATH] -y
 - (on worker node) ray stop --force

########################################
#       Executing Alpa Profiling       #
########################################

########################################
Ray Cluster Info:
 - Ray address: auto
 - Devices num: 1_a40
 - Nodes num: 1
 - Devices num per node: 1
########################################
DL Job Info:
 - Job UUID: default
 - Model name: wide_resnet
 - Param num: 500M
 - Dataset name: none
 - Batch size: 1024
 - Pipeline layer num: 16
 - Miro-batches num: 16
 - Iteration num: 2
 - Is DP only: false
 - Is PP only: false
 - Is MP only: false
 - Is dummy test: false
 - Is manual config test: false
 - Output log path: /home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/profile_result/1_nodes_1_devices_per_node/1_a40/wide_resnet_none/bs_1024_nmb_16_pln_16_param_500M_try_1.log
########################################

[I] Ray Cluster & Alpa backend initialization is completed.
[I] Device Info:
    - Devices num per node: 1
    - Nodes num: 1
[I] Loading flax model....
[I] Model 'wide_resnet' loads sucessfully. Model Info: 

WideResNet(
    # attributes
    stage_sizes = [3, 4, 6, 3]
    block_cls = BottleneckResNetBlock
    num_classes = 1024
    num_filters = 224
    width_factor = 2
    dtype = float32
    act = relu
) 

[I] Dump debug file path: ./profile_result/debug/1_nodes_1_devices_per_node/1_a40/wide_resnet_none/bs_1024_nmb_16_pln_16_param_num_500M
[I] Manually construct dummy batch with the shape of (1024, 224, 224, 3).
[I] Initialize training state...
[I] Training state initialization is completed.
[I] Constructing Alpa parallelized train step func...
[I] Alpa parallelized train step func construction is completed.
-------------------- Automatic stage clustering --------------------
submesh_choices: ((1, 1),)
- Profiling for submesh 0 (1, 1):
- Generate all stage infos (Jaxpr -> HLO)
- Compile all stages
- Profile all stages
result[(0, 15, 0, 0), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=13.611 GB, invar_size=8.079 GB, outvar_size=1.844 GB, temp_buffer_size=5.533 GB, available_memory=35.130 GB)
result[(0, 15, 0, 0), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.200 GB, invar_size=1.882 GB, outvar_size=4.355 GB, temp_buffer_size=2.964 GB, available_memory=35.130 GB)
result[(0, 15, 0, 1), 1] = ModuleProfileResult(compute_cost=0.000, peak_memory=13.611 GB, invar_size=8.079 GB, outvar_size=1.844 GB, temp_buffer_size=5.533 GB, available_memory=35.130 GB)
result[(0, 15, 0, 1), 0] = ModuleProfileResult(compute_cost=0.000, peak_memory=9.200 GB, invar_size=1.882 GB, outvar_size=4.355 GB, temp_buffer_size=2.964 GB, available_memory=35.130 GB)
Profiling for submesh 0 (1, 1) takes 30.88 seconds
--------------------------------------------------
Profile result saved to: ./profile_result/profile-results-2023-10-30-23-10-36.npy
----------------------------------------------------------------------
Result forward_stage_layer_ids: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]]
Result mesh_shapes: [(1, 1)]
Result logical_mesh_shapes: [(1, 1)]
Result autosharding_option_dicts: [{}]
 - Compile (driver): 48.02 s
compilation time breakdown: {'stage-construction': '32.69', 'stage-construction-dp': '1.09', 'stage-construction-compilation': '3.39', 'stage-construction-profiling': '24.06'}
 - Compile (worker): 24.31 s
[I] Ready to perform training process.
[I] Benchmark the training process with entire dataset and profile with driver overhead...
    - Iteration 1 / 2 is performed...
 - Benchmark: 115.90 s

[I] Performance metrics:
 - Iteration count: 2.
 - Total e2e training time : 115.892 s.
 - Average e2e iteration time: 57.94600296020508 s.
 - Total local training time: 109.19000244140625 s.
 - Average local iteration time: 54.595001220703125 s.
 - Max allocated memory among devices: 15.996 GB.
 - Compilation times:  {'stage-construction': 32.69305419921875, 'stage-construction-dp': 1.0880098342895508, 'stage-construction-compilation': 3.3949553966522217, 'stage-construction-profiling': 24.064604997634888}
 - Metadata:  []
 - Is need save result:  True

