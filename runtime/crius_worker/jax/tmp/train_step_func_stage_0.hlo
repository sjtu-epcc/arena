HloModule train_step_func_pipeshard_parallel_mesh_0-0, input_output_alias={ {0}: (0, {}, may-alias), {1}: (1, {}, may-alias), {2}: (2, {}, may-alias), {3}: (3, {}, may-alias), {4}: (4, {}, may-alias), {5}: (5, {}, may-alias), {6}: (6, {}, may-alias), {7}: (7, {}, may-alias), {8}: (8, {}, may-alias), {9}: (9, {}, may-alias), {10}: (10, {}, may-alias), {11}: (11, {}, may-alias), {12}: (12, {}, may-alias), {13}: (13, {}, may-alias), {14}: (14, {}, may-alias), {15}: (15, {}, may-alias), {16}: (16, {}, may-alias), {17}: (17, {}, may-alias), {18}: (18, {}, may-alias), {19}: (19, {}, may-alias), {20}: (20, {}, may-alias), {21}: (21, {}, may-alias), {22}: (22, {}, may-alias), {23}: (23, {}, may-alias), {24}: (24, {}, may-alias), {25}: (25, {}, may-alias), {26}: (26, {}, may-alias), {27}: (27, {}, may-alias), {28}: (28, {}, may-alias), {29}: (29, {}, may-alias), {30}: (30, {}, may-alias), {31}: (31, {}, may-alias), {32}: (32, {}, may-alias), {33}: (33, {}, may-alias), {34}: (34, {}, may-alias), {35}: (35, {}, may-alias), {36}: (36, {}, may-alias), {37}: (37, {}, may-alias), {38}: (38, {}, may-alias), {39}: (39, {}, may-alias), {40}: (40, {}, may-alias), {41}: (41, {}, may-alias), {42}: (42, {}, may-alias), {43}: (43, {}, may-alias), {44}: (44, {}, may-alias), {45}: (45, {}, may-alias), {46}: (46, {}, may-alias), {47}: (47, {}, may-alias), {48}: (48, {}, may-alias), {49}: (49, {}, may-alias), {50}: (50, {}, may-alias), {51}: (51, {}, may-alias), {52}: (52, {}, may-alias), {53}: (53, {}, may-alias), {54}: (54, {}, may-alias), {55}: (55, {}, may-alias) }, entry_computation_layout={(f32[224]{0},f32[224]{0},f32[224]{0},f32[224]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[896]{0},f32[896]{0},f32[224]{0},f32[224]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[224]{0},f32[224]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[1792]{0},f32[1792]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[3584]{0},f32[3584]{0},f32[3584]{0},f32[3584]{0},f32[7,7,3,224]{3,2,1,0},f32[224]{0},f32[224]{0},f32[1,1,224,224]{3,2,1,0},f32[224]{0},f32[224]{0},f32[3,3,224,448]{3,2,1,0},f32[448]{0},f32[448]{0},s32[16,224,224,3]{3,2,1,0},f32[224]{0},f32[224]{0},f32[224]{0},f32[224]{0},f32[448]{0},f32[448]{0},f32[1,1,448,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[1,1,224,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[1,1,896,224]{3,2,1,0},f32[224]{0},f32[224]{0},f32[3,3,224,448]{3,2,1,0},f32[448]{0},f32[448]{0},f32[1,1,448,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[896]{0},f32[896]{0},f32[896]{0},f32[896]{0},f32[224]{0},f32[224]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1,1,896,224]{3,2,1,0},f32[224]{0},f32[224]{0},f32[3,3,224,448]{3,2,1,0},f32[448]{0},f32[448]{0},f32[1,1,448,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[224]{0},f32[224]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1,1,896,448]{3,2,1,0},f32[448]{0},f32[448]{0},f32[3,3,448,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[1,1,896,1792]{3,2,1,0},f32[1792]{0},f32[1792]{0},f32[1,1,896,1792]{3,2,1,0},f32[1792]{0},f32[1792]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[1792]{0},f32[1792]{0},f32[1,1,1792,448]{3,2,1,0},f32[448]{0},f32[448]{0},f32[3,3,448,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[1,1,896,1792]{3,2,1,0},f32[1792]{0},f32[1792]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[1,1,1792,448]{3,2,1,0},f32[448]{0},f32[448]{0},f32[3,3,448,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[1,1,896,1792]{3,2,1,0},f32[1792]{0},f32[1792]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[1,1,1792,448]{3,2,1,0},f32[448]{0},f32[448]{0},f32[3,3,448,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[1,1,896,1792]{3,2,1,0},f32[1792]{0},f32[1792]{0},f32[448]{0},f32[448]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[1,1,1792,896]{3,2,1,0},f32[896]{0},f32[896]{0},f32[3,3,896,1792]{3,2,1,0},f32[1792]{0},f32[1792]{0},f32[1,1,1792,3584]{3,2,1,0},f32[3584]{0},f32[3584]{0},f32[1,1,1792,3584]{3,2,1,0},f32[3584]{0},f32[3584]{0},f32[896]{0},f32[896]{0},f32[1792]{0},f32[1792]{0},f32[3584]{0},f32[3584]{0},f32[3584]{0},f32[3584]{0})->(f32[224]{0}, f32[224]{0}, f32[224]{0}, f32[224]{0}, f32[448]{0}, /*index=5*/f32[448]{0}, f32[896]{0}, f32[896]{0}, f32[896]{0}, f32[896]{0}, /*index=10*/f32[224]{0}, f32[224]{0}, f32[448]{0}, f32[448]{0}, f32[896]{0}, /*index=15*/f32[896]{0}, f32[224]{0}, f32[224]{0}, f32[448]{0}, f32[448]{0}, /*index=20*/f32[896]{0}, f32[896]{0}, f32[448]{0}, f32[448]{0}, f32[896]{0}, /*index=25*/f32[896]{0}, f32[1792]{0}, f32[1792]{0}, f32[1792]{0}, f32[1792]{0}, /*index=30*/f32[448]{0}, f32[448]{0}, f32[896]{0}, f32[896]{0}, f32[1792]{0}, /*index=35*/f32[1792]{0}, f32[448]{0}, f32[448]{0}, f32[896]{0}, f32[896]{0}, /*index=40*/f32[1792]{0}, f32[1792]{0}, f32[448]{0}, f32[448]{0}, f32[896]{0}, /*index=45*/f32[896]{0}, f32[1792]{0}, f32[1792]{0}, f32[896]{0}, f32[896]{0}, /*index=50*/f32[1792]{0}, f32[1792]{0}, f32[3584]{0}, f32[3584]{0}, f32[3584]{0}, /*index=55*/f32[3584]{0}, f32[16,56,56,448]{3,2,1,0}, f32[16,56,56,224]{3,2,1,0}, f32[16,56,56,896]{3,2,1,0}, f32[16,56,56,896]{3,2,1,0}, /*index=60*/f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{3,2,1,0}, f32[16,14,14,3584]{3,2,1,0})}

%region_0.1246.0 (Arg_0.1247: f32[], Arg_1.1248: f32[]) -> f32[] {
  %Arg_0.1247 = f32[] parameter(0)
  %Arg_1.1248 = f32[] parameter(1)
  ROOT %add.1249 = f32[] add(f32[] %Arg_0.1247, f32[] %Arg_1.1248), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
}

%fused_computation (param_0.3: f32[3584], param_1.4: f32[3584], param_2.5: f32[3584], param_3.7: f32[16,14,14,3584], param_4.394: f32[3584], param_5.183: f32[3584], param_6.132: f32[3584], param_7.52: f32[16,14,14,3584], param_8.40: f32[3584], param_9.42: f32[3584]) -> f32[16,14,14,3584] {
  %param_7.52 = f32[16,14,14,3584]{2,1,3,0} parameter(7)
  %param_9.42 = f32[3584]{0} parameter(9)
  %constant_727 = f32[] constant(0.000318877544)
  %broadcast.710 = f32[3584]{0} broadcast(f32[] %constant_727), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.577 = f32[3584]{0} multiply(f32[3584]{0} %param_9.42, f32[3584]{0} %broadcast.710), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.202 = f32[16,14,14,3584]{2,1,3,0} broadcast(f32[3584]{0} %multiply.577), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.1 = f32[16,14,14,3584]{2,1,3,0} subtract(f32[16,14,14,3584]{2,1,3,0} %param_7.52, f32[16,14,14,3584]{2,1,3,0} %broadcast.202), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.132 = f32[3584]{0} parameter(6)
  %constant_88 = f32[] constant(1e-05)
  %broadcast.201 = f32[3584]{0} broadcast(f32[] %constant_88), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.32 = f32[3584]{0} add(f32[3584]{0} %param_6.132, f32[3584]{0} %broadcast.201), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.29 = f32[3584]{0} rsqrt(f32[3584]{0} %add.32), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_5.183 = f32[3584]{0} parameter(5)
  %multiply.143 = f32[3584]{0} multiply(f32[3584]{0} %rsqrt.29, f32[3584]{0} %param_5.183), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.200 = f32[16,14,14,3584]{2,1,3,0} broadcast(f32[3584]{0} %multiply.143), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.142 = f32[16,14,14,3584]{2,1,3,0} multiply(f32[16,14,14,3584]{2,1,3,0} %subtract.1, f32[16,14,14,3584]{2,1,3,0} %broadcast.200), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_4.394 = f32[3584]{0} parameter(4)
  %broadcast.199 = f32[16,14,14,3584]{2,1,3,0} broadcast(f32[3584]{0} %param_4.394), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.31 = f32[16,14,14,3584]{2,1,3,0} add(f32[16,14,14,3584]{2,1,3,0} %multiply.142, f32[16,14,14,3584]{2,1,3,0} %broadcast.199), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %param_3.7 = f32[16,14,14,3584]{2,1,3,0} parameter(3)
  %param_8.40 = f32[3584]{0} parameter(8)
  %multiply.573 = f32[3584]{0} multiply(f32[3584]{0} %param_8.40, f32[3584]{0} %broadcast.710), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.198 = f32[16,14,14,3584]{2,1,3,0} broadcast(f32[3584]{0} %multiply.573), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.0 = f32[16,14,14,3584]{2,1,3,0} subtract(f32[16,14,14,3584]{2,1,3,0} %param_3.7, f32[16,14,14,3584]{2,1,3,0} %broadcast.198), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.5 = f32[3584]{0} parameter(2)
  %add.30 = f32[3584]{0} add(f32[3584]{0} %param_2.5, f32[3584]{0} %broadcast.201), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.28 = f32[3584]{0} rsqrt(f32[3584]{0} %add.30), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.4 = f32[3584]{0} parameter(1)
  %multiply.141 = f32[3584]{0} multiply(f32[3584]{0} %rsqrt.28, f32[3584]{0} %param_1.4), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.197 = f32[16,14,14,3584]{2,1,3,0} broadcast(f32[3584]{0} %multiply.141), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.140 = f32[16,14,14,3584]{2,1,3,0} multiply(f32[16,14,14,3584]{2,1,3,0} %subtract.0, f32[16,14,14,3584]{2,1,3,0} %broadcast.197), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.3 = f32[3584]{0} parameter(0)
  %broadcast.196 = f32[16,14,14,3584]{2,1,3,0} broadcast(f32[3584]{0} %param_0.3), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.29 = f32[16,14,14,3584]{2,1,3,0} add(f32[16,14,14,3584]{2,1,3,0} %multiply.140, f32[16,14,14,3584]{2,1,3,0} %broadcast.196), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.28 = f32[16,14,14,3584]{2,1,3,0} add(f32[16,14,14,3584]{2,1,3,0} %add.31, f32[16,14,14,3584]{2,1,3,0} %add.29), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %copy.39 = f32[16,14,14,3584]{3,2,1,0} copy(f32[16,14,14,3584]{2,1,3,0} %add.28), metadata={op_name="tuple.66"}
}

%fused_computation.1 (param_0.4: f32[3584], param_1.663: f32[3584], param_2.598: f32[3584], param_3.574: f32[16,3584], param_4.400: f32[3584], param_5.190: f32[3584]) -> (f32[3584], f32[3584], f32[3584]) {
  %param_0.4 = f32[3584]{0} parameter(0)
  %param_1.663 = f32[3584]{0} parameter(1)
  %constant_90 = f32[] constant(0.9)
  %broadcast.204 = f32[3584]{0} broadcast(f32[] %constant_90), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.145 = f32[3584]{0} multiply(f32[3584]{0} %param_1.663, f32[3584]{0} %broadcast.204), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.574 = f32[16,3584]{1,0} parameter(3)
  %constant_92_clone_1 = f32[] constant(0)
  %reduce.280.clone.1 = f32[3584]{0} reduce(f32[16,3584]{1,0} %param_3.574, f32[] %constant_92_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_736_clone_1 = f32[] constant(0.000318877544)
  %broadcast.206.clone.1 = f32[3584]{0} broadcast(f32[] %constant_736_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.147.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %reduce.280.clone.1, f32[3584]{0} %broadcast.206.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.598 = f32[3584]{0} parameter(2)
  %multiply.579.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %param_2.598, f32[3584]{0} %broadcast.206.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.146.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %multiply.579.clone.1, f32[3584]{0} %multiply.579.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.2.clone.1 = f32[3584]{0} subtract(f32[3584]{0} %multiply.147.clone.1, f32[3584]{0} %multiply.146.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.205.clone.1 = f32[3584]{0} broadcast(f32[] %constant_92_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.0.clone.1 = f32[3584]{0} maximum(f32[3584]{0} %subtract.2.clone.1, f32[3584]{0} %broadcast.205.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_89 = f32[] constant(0.1)
  %broadcast.203 = f32[3584]{0} broadcast(f32[] %constant_89), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.144 = f32[3584]{0} multiply(f32[3584]{0} %maximum.0.clone.1, f32[3584]{0} %broadcast.203), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.34 = f32[3584]{0} add(f32[3584]{0} %multiply.145, f32[3584]{0} %multiply.144), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.33 = f32[3584]{0} add(f32[3584]{0} %param_0.4, f32[3584]{0} %add.34), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  %param_4.400 = f32[3584]{0} parameter(4)
  %param_5.190 = f32[3584]{0} parameter(5)
  %multiply.151.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %param_5.190, f32[3584]{0} %broadcast.204), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_95_clone_1 = f32[] constant(3.18877537e-05)
  %broadcast.208.clone.1 = f32[3584]{0} broadcast(f32[] %constant_95_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.150.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %param_2.598, f32[3584]{0} %broadcast.208.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.36.clone.1 = f32[3584]{0} add(f32[3584]{0} %multiply.151.clone.1, f32[3584]{0} %multiply.150.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.35.clone.1 = f32[3584]{0} add(f32[3584]{0} %param_4.400, f32[3584]{0} %add.36.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  ROOT %tuple.2 = (f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) tuple(f32[3584]{0} %add.33, f32[3584]{0} %maximum.0.clone.1, f32[3584]{0} %add.35.clone.1)
}

%fused_computation.7 (param_0.14: f32[3584], param_1.666: f32[3584], param_2.602: f32[3584], param_3.579: f32[16,3584], param_4.406: f32[3584], param_5.197: f32[3584]) -> (f32[3584], f32[3584], f32[3584]) {
  %param_0.14 = f32[3584]{0} parameter(0)
  %param_1.666 = f32[3584]{0} parameter(1)
  %constant_99 = f32[] constant(0.9)
  %broadcast.211 = f32[3584]{0} broadcast(f32[] %constant_99), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.153 = f32[3584]{0} multiply(f32[3584]{0} %param_1.666, f32[3584]{0} %broadcast.211), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.579 = f32[16,3584]{1,0} parameter(3)
  %constant_101_clone_1 = f32[] constant(0)
  %reduce.283.clone.1 = f32[3584]{0} reduce(f32[16,3584]{1,0} %param_3.579, f32[] %constant_101_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_730_clone_1 = f32[] constant(0.000318877544)
  %broadcast.213.clone.1 = f32[3584]{0} broadcast(f32[] %constant_730_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.155.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %reduce.283.clone.1, f32[3584]{0} %broadcast.213.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.602 = f32[3584]{0} parameter(2)
  %multiply.575.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %param_2.602, f32[3584]{0} %broadcast.213.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.154.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %multiply.575.clone.1, f32[3584]{0} %multiply.575.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.3.clone.1 = f32[3584]{0} subtract(f32[3584]{0} %multiply.155.clone.1, f32[3584]{0} %multiply.154.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.212.clone.1 = f32[3584]{0} broadcast(f32[] %constant_101_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.1.clone.1 = f32[3584]{0} maximum(f32[3584]{0} %subtract.3.clone.1, f32[3584]{0} %broadcast.212.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_98 = f32[] constant(0.1)
  %broadcast.210 = f32[3584]{0} broadcast(f32[] %constant_98), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.152 = f32[3584]{0} multiply(f32[3584]{0} %maximum.1.clone.1, f32[3584]{0} %broadcast.210), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.38 = f32[3584]{0} add(f32[3584]{0} %multiply.153, f32[3584]{0} %multiply.152), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.37 = f32[3584]{0} add(f32[3584]{0} %param_0.14, f32[3584]{0} %add.38), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  %param_4.406 = f32[3584]{0} parameter(4)
  %param_5.197 = f32[3584]{0} parameter(5)
  %multiply.159.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %param_5.197, f32[3584]{0} %broadcast.211), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_104_clone_1 = f32[] constant(3.18877537e-05)
  %broadcast.215.clone.1 = f32[3584]{0} broadcast(f32[] %constant_104_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.158.clone.1 = f32[3584]{0} multiply(f32[3584]{0} %param_2.602, f32[3584]{0} %broadcast.215.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.40.clone.1 = f32[3584]{0} add(f32[3584]{0} %multiply.159.clone.1, f32[3584]{0} %multiply.158.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.39.clone.1 = f32[3584]{0} add(f32[3584]{0} %param_4.406, f32[3584]{0} %add.40.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  ROOT %tuple.5 = (f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) tuple(f32[3584]{0} %add.37, f32[3584]{0} %maximum.1.clone.1, f32[3584]{0} %add.39.clone.1)
}

%fused_computation.13 (param_0.349: f32[1792], param_1.592: f32[1792], param_2.480: f32[1792], param_3.470: f32[16,14,14,1792], param_4.393: f32[1792]) -> f32[16,14,14,1792] {
  %param_3.470 = f32[16,14,14,1792]{2,1,3,0} parameter(3)
  %param_4.393 = f32[1792]{0} parameter(4)
  %constant_721 = f32[] constant(0.000318877544)
  %broadcast.702 = f32[1792]{0} broadcast(f32[] %constant_721), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.569 = f32[1792]{0} multiply(f32[1792]{0} %param_4.393, f32[1792]{0} %broadcast.702), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.220 = f32[16,14,14,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.569), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.4 = f32[16,14,14,1792]{2,1,3,0} subtract(f32[16,14,14,1792]{2,1,3,0} %param_3.470, f32[16,14,14,1792]{2,1,3,0} %broadcast.220), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.480 = f32[1792]{0} parameter(2)
  %constant_107 = f32[] constant(1e-05)
  %broadcast.221 = f32[1792]{0} broadcast(f32[] %constant_107), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.42 = f32[1792]{0} add(f32[1792]{0} %param_2.480, f32[1792]{0} %broadcast.221), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.30 = f32[1792]{0} rsqrt(f32[1792]{0} %add.42), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.592 = f32[1792]{0} parameter(1)
  %multiply.161 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.30, f32[1792]{0} %param_1.592), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.219 = f32[16,14,14,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.161), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.160 = f32[16,14,14,1792]{2,1,3,0} multiply(f32[16,14,14,1792]{2,1,3,0} %subtract.4, f32[16,14,14,1792]{2,1,3,0} %broadcast.219), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.349 = f32[1792]{0} parameter(0)
  %broadcast.218 = f32[16,14,14,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_0.349), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.41 = f32[16,14,14,1792]{2,1,3,0} add(f32[16,14,14,1792]{2,1,3,0} %multiply.160, f32[16,14,14,1792]{2,1,3,0} %broadcast.218), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_108 = f32[] constant(0)
  %broadcast.217 = f32[16,14,14,1792]{2,1,3,0} broadcast(f32[] %constant_108), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  ROOT %maximum.2 = f32[16,14,14,1792]{2,1,3,0} maximum(f32[16,14,14,1792]{2,1,3,0} %add.41, f32[16,14,14,1792]{2,1,3,0} %broadcast.217), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
}

%fused_computation.14 (param_0.25: f32[1792], param_1.669: f32[1792], param_2.606: f32[1792], param_3.584: f32[16,1792], param_4.412: f32[1792], param_5.204: f32[1792]) -> (f32[1792], f32[1792], f32[1792]) {
  %param_0.25 = f32[1792]{0} parameter(0)
  %param_1.669 = f32[1792]{0} parameter(1)
  %constant_110 = f32[] constant(0.9)
  %broadcast.223 = f32[1792]{0} broadcast(f32[] %constant_110), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.163 = f32[1792]{0} multiply(f32[1792]{0} %param_1.669, f32[1792]{0} %broadcast.223), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.584 = f32[16,1792]{1,0} parameter(3)
  %constant_112_clone_1 = f32[] constant(0)
  %reduce.286.clone.1 = f32[1792]{0} reduce(f32[16,1792]{1,0} %param_3.584, f32[] %constant_112_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_724_clone_1 = f32[] constant(0.000318877544)
  %broadcast.224.clone.1 = f32[1792]{0} broadcast(f32[] %constant_724_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.165.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %reduce.286.clone.1, f32[1792]{0} %broadcast.224.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.606 = f32[1792]{0} parameter(2)
  %multiply.571.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.606, f32[1792]{0} %broadcast.224.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.164.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %multiply.571.clone.1, f32[1792]{0} %multiply.571.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.5.clone.1 = f32[1792]{0} subtract(f32[1792]{0} %multiply.165.clone.1, f32[1792]{0} %multiply.164.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.225.clone.1 = f32[1792]{0} broadcast(f32[] %constant_112_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.3.clone.1 = f32[1792]{0} maximum(f32[1792]{0} %subtract.5.clone.1, f32[1792]{0} %broadcast.225.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_109 = f32[] constant(0.1)
  %broadcast.222 = f32[1792]{0} broadcast(f32[] %constant_109), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.162 = f32[1792]{0} multiply(f32[1792]{0} %maximum.3.clone.1, f32[1792]{0} %broadcast.222), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.44 = f32[1792]{0} add(f32[1792]{0} %multiply.163, f32[1792]{0} %multiply.162), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.43 = f32[1792]{0} add(f32[1792]{0} %param_0.25, f32[1792]{0} %add.44), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  %param_4.412 = f32[1792]{0} parameter(4)
  %param_5.204 = f32[1792]{0} parameter(5)
  %multiply.169.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_5.204, f32[1792]{0} %broadcast.223), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_115_clone_1 = f32[] constant(3.18877537e-05)
  %broadcast.227.clone.1 = f32[1792]{0} broadcast(f32[] %constant_115_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.168.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.606, f32[1792]{0} %broadcast.227.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.46.clone.1 = f32[1792]{0} add(f32[1792]{0} %multiply.169.clone.1, f32[1792]{0} %multiply.168.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.45.clone.1 = f32[1792]{0} add(f32[1792]{0} %param_4.412, f32[1792]{0} %add.46.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  ROOT %tuple.8 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) tuple(f32[1792]{0} %add.43, f32[1792]{0} %maximum.3.clone.1, f32[1792]{0} %add.45.clone.1)
}

%fused_computation.20 (param_0.352: f32[896], param_1.594: f32[896], param_2.483: f32[896], param_3.473: f32[16,28,28,896], param_4.392: f32[896]) -> f32[16,29,29,896] {
  %param_3.473 = f32[16,28,28,896]{2,1,3,0} parameter(3)
  %param_4.392 = f32[896]{0} parameter(4)
  %constant_715 = f32[] constant(7.97193861e-05)
  %broadcast.698 = f32[896]{0} broadcast(f32[] %constant_715), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.565 = f32[896]{0} multiply(f32[896]{0} %param_4.392, f32[896]{0} %broadcast.698), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.231 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.565), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.6 = f32[16,28,28,896]{2,1,3,0} subtract(f32[16,28,28,896]{2,1,3,0} %param_3.473, f32[16,28,28,896]{2,1,3,0} %broadcast.231), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.483 = f32[896]{0} parameter(2)
  %constant_118 = f32[] constant(1e-05)
  %broadcast.233 = f32[896]{0} broadcast(f32[] %constant_118), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.48 = f32[896]{0} add(f32[896]{0} %param_2.483, f32[896]{0} %broadcast.233), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.31 = f32[896]{0} rsqrt(f32[896]{0} %add.48), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.594 = f32[896]{0} parameter(1)
  %multiply.171 = f32[896]{0} multiply(f32[896]{0} %rsqrt.31, f32[896]{0} %param_1.594), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.230 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.171), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.170 = f32[16,28,28,896]{2,1,3,0} multiply(f32[16,28,28,896]{2,1,3,0} %subtract.6, f32[16,28,28,896]{2,1,3,0} %broadcast.230), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.352 = f32[896]{0} parameter(0)
  %broadcast.229 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %param_0.352), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.47 = f32[16,28,28,896]{2,1,3,0} add(f32[16,28,28,896]{2,1,3,0} %multiply.170, f32[16,28,28,896]{2,1,3,0} %broadcast.229), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_119 = f32[] constant(0)
  %broadcast.232 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[] %constant_119), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %maximum.4 = f32[16,28,28,896]{2,1,3,0} maximum(f32[16,28,28,896]{2,1,3,0} %add.47, f32[16,28,28,896]{2,1,3,0} %broadcast.232), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  ROOT %pad.2 = f32[16,29,29,896]{2,1,3,0} pad(f32[16,28,28,896]{2,1,3,0} %maximum.4, f32[] %constant_119), padding=0_0x0_1x0_1x0_0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
}

%fused_computation.21 (param_0.37: f32[896], param_1.672: f32[896], param_2.610: f32[896], param_3.589: f32[16,896], param_4.418: f32[896], param_5.211: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.37 = f32[896]{0} parameter(0)
  %param_1.672 = f32[896]{0} parameter(1)
  %constant_121 = f32[] constant(0.9)
  %broadcast.235 = f32[896]{0} broadcast(f32[] %constant_121), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.173 = f32[896]{0} multiply(f32[896]{0} %param_1.672, f32[896]{0} %broadcast.235), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.589 = f32[16,896]{1,0} parameter(3)
  %constant_123_clone_1 = f32[] constant(0)
  %reduce.289.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.589, f32[] %constant_123_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_718_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.236.clone.1 = f32[896]{0} broadcast(f32[] %constant_718_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.175.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.289.clone.1, f32[896]{0} %broadcast.236.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.610 = f32[896]{0} parameter(2)
  %multiply.567.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.610, f32[896]{0} %broadcast.236.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.174.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.567.clone.1, f32[896]{0} %multiply.567.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.7.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.175.clone.1, f32[896]{0} %multiply.174.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.237.clone.1 = f32[896]{0} broadcast(f32[] %constant_123_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.5.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.7.clone.1, f32[896]{0} %broadcast.237.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_120 = f32[] constant(0.1)
  %broadcast.234 = f32[896]{0} broadcast(f32[] %constant_120), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.172 = f32[896]{0} multiply(f32[896]{0} %maximum.5.clone.1, f32[896]{0} %broadcast.234), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.50 = f32[896]{0} add(f32[896]{0} %multiply.173, f32[896]{0} %multiply.172), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.49 = f32[896]{0} add(f32[896]{0} %param_0.37, f32[896]{0} %add.50), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  %param_4.418 = f32[896]{0} parameter(4)
  %param_5.211 = f32[896]{0} parameter(5)
  %multiply.179.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.211, f32[896]{0} %broadcast.235), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_126_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.239.clone.1 = f32[896]{0} broadcast(f32[] %constant_126_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.178.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.610, f32[896]{0} %broadcast.239.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.52.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.179.clone.1, f32[896]{0} %multiply.178.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.51.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.418, f32[896]{0} %add.52.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  ROOT %tuple.11 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.49, f32[896]{0} %maximum.5.clone.1, f32[896]{0} %add.51.clone.1)
}

%fused_computation.28 (param_0.50: f32[1792], param_1.680: f32[1792], param_2.618: f32[1792], param_3.599: f32[16,1792], param_4.428: f32[1792], param_5.223: f32[1792]) -> (f32[1792], f32[1792], f32[1792]) {
  %param_0.50 = f32[1792]{0} parameter(0)
  %param_1.680 = f32[1792]{0} parameter(1)
  %constant_132 = f32[] constant(0.9)
  %broadcast.247 = f32[1792]{0} broadcast(f32[] %constant_132), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.183 = f32[1792]{0} multiply(f32[1792]{0} %param_1.680, f32[1792]{0} %broadcast.247), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.599 = f32[16,1792]{1,0} parameter(3)
  %constant_134_clone_1 = f32[] constant(0)
  %reduce.292.clone.1 = f32[1792]{0} reduce(f32[16,1792]{1,0} %param_3.599, f32[] %constant_134_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_712_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.249.clone.1 = f32[1792]{0} broadcast(f32[] %constant_712_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.185.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %reduce.292.clone.1, f32[1792]{0} %broadcast.249.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.618 = f32[1792]{0} parameter(2)
  %multiply.563.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.618, f32[1792]{0} %broadcast.249.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.184.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %multiply.563.clone.1, f32[1792]{0} %multiply.563.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.9.clone.1 = f32[1792]{0} subtract(f32[1792]{0} %multiply.185.clone.1, f32[1792]{0} %multiply.184.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.248.clone.1 = f32[1792]{0} broadcast(f32[] %constant_134_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.7.clone.1 = f32[1792]{0} maximum(f32[1792]{0} %subtract.9.clone.1, f32[1792]{0} %broadcast.248.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_131 = f32[] constant(0.1)
  %broadcast.246 = f32[1792]{0} broadcast(f32[] %constant_131), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.182 = f32[1792]{0} multiply(f32[1792]{0} %maximum.7.clone.1, f32[1792]{0} %broadcast.246), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.57 = f32[1792]{0} add(f32[1792]{0} %multiply.183, f32[1792]{0} %multiply.182), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.56 = f32[1792]{0} add(f32[1792]{0} %param_0.50, f32[1792]{0} %add.57), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  %param_4.428 = f32[1792]{0} parameter(4)
  %param_5.223 = f32[1792]{0} parameter(5)
  %multiply.189.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_5.223, f32[1792]{0} %broadcast.247), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_137_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.251.clone.1 = f32[1792]{0} broadcast(f32[] %constant_137_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.188.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.618, f32[1792]{0} %broadcast.251.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.59.clone.1 = f32[1792]{0} add(f32[1792]{0} %multiply.189.clone.1, f32[1792]{0} %multiply.188.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.58.clone.1 = f32[1792]{0} add(f32[1792]{0} %param_4.428, f32[1792]{0} %add.59.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  ROOT %tuple.15 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) tuple(f32[1792]{0} %add.56, f32[1792]{0} %maximum.7.clone.1, f32[1792]{0} %add.58.clone.1)
}

%fused_computation.34 (param_0.62: f32[896], param_1.106: f32[896], param_2.86: f32[896], param_3.85: f32[16,28,28,896], param_4.391: f32[896]) -> f32[16,28,28,896] {
  %param_3.85 = f32[16,28,28,896]{2,1,3,0} parameter(3)
  %param_4.391 = f32[896]{0} parameter(4)
  %constant_703 = f32[] constant(7.97193861e-05)
  %broadcast.690 = f32[896]{0} broadcast(f32[] %constant_703), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.557 = f32[896]{0} multiply(f32[896]{0} %param_4.391, f32[896]{0} %broadcast.690), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.255 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.557), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.10 = f32[16,28,28,896]{2,1,3,0} subtract(f32[16,28,28,896]{2,1,3,0} %param_3.85, f32[16,28,28,896]{2,1,3,0} %broadcast.255), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.86 = f32[896]{0} parameter(2)
  %constant_140 = f32[] constant(1e-05)
  %broadcast.257 = f32[896]{0} broadcast(f32[] %constant_140), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.61 = f32[896]{0} add(f32[896]{0} %param_2.86, f32[896]{0} %broadcast.257), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.33 = f32[896]{0} rsqrt(f32[896]{0} %add.61), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.106 = f32[896]{0} parameter(1)
  %multiply.191 = f32[896]{0} multiply(f32[896]{0} %rsqrt.33, f32[896]{0} %param_1.106), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.254 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.191), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.190 = f32[16,28,28,896]{2,1,3,0} multiply(f32[16,28,28,896]{2,1,3,0} %subtract.10, f32[16,28,28,896]{2,1,3,0} %broadcast.254), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.62 = f32[896]{0} parameter(0)
  %broadcast.253 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %param_0.62), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.60 = f32[16,28,28,896]{2,1,3,0} add(f32[16,28,28,896]{2,1,3,0} %multiply.190, f32[16,28,28,896]{2,1,3,0} %broadcast.253), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_141 = f32[] constant(0)
  %broadcast.256 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[] %constant_141), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  ROOT %maximum.8 = f32[16,28,28,896]{2,1,3,0} maximum(f32[16,28,28,896]{2,1,3,0} %add.60, f32[16,28,28,896]{2,1,3,0} %broadcast.256), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
}

%fused_computation.35 (param_0.63: f32[896], param_1.683: f32[896], param_2.622: f32[896], param_3.604: f32[16,896], param_4.434: f32[896], param_5.230: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.63 = f32[896]{0} parameter(0)
  %param_1.683 = f32[896]{0} parameter(1)
  %constant_143 = f32[] constant(0.9)
  %broadcast.259 = f32[896]{0} broadcast(f32[] %constant_143), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.193 = f32[896]{0} multiply(f32[896]{0} %param_1.683, f32[896]{0} %broadcast.259), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.604 = f32[16,896]{1,0} parameter(3)
  %constant_145_clone_1 = f32[] constant(0)
  %reduce.295.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.604, f32[] %constant_145_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_706_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.260.clone.1 = f32[896]{0} broadcast(f32[] %constant_706_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.195.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.295.clone.1, f32[896]{0} %broadcast.260.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.622 = f32[896]{0} parameter(2)
  %multiply.559.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.622, f32[896]{0} %broadcast.260.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.194.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.559.clone.1, f32[896]{0} %multiply.559.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.11.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.195.clone.1, f32[896]{0} %multiply.194.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.261.clone.1 = f32[896]{0} broadcast(f32[] %constant_145_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.9.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.11.clone.1, f32[896]{0} %broadcast.261.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_142 = f32[] constant(0.1)
  %broadcast.258 = f32[896]{0} broadcast(f32[] %constant_142), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.192 = f32[896]{0} multiply(f32[896]{0} %maximum.9.clone.1, f32[896]{0} %broadcast.258), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.63 = f32[896]{0} add(f32[896]{0} %multiply.193, f32[896]{0} %multiply.192), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.62 = f32[896]{0} add(f32[896]{0} %param_0.63, f32[896]{0} %add.63), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  %param_4.434 = f32[896]{0} parameter(4)
  %param_5.230 = f32[896]{0} parameter(5)
  %multiply.199.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.230, f32[896]{0} %broadcast.259), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_148_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.263.clone.1 = f32[896]{0} broadcast(f32[] %constant_148_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.198.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.622, f32[896]{0} %broadcast.263.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.65.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.199.clone.1, f32[896]{0} %multiply.198.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.64.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.434, f32[896]{0} %add.65.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  ROOT %tuple.18 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.62, f32[896]{0} %maximum.9.clone.1, f32[896]{0} %add.64.clone.1)
}

%fused_computation.41 (param_0.75: f32[448], param_1.127: f32[448], param_2.105: f32[448], param_3.104: f32[16,28,28,448], param_4.390: f32[448]) -> f32[16,28,28,448] {
  %param_3.104 = f32[16,28,28,448]{2,1,3,0} parameter(3)
  %param_4.390 = f32[448]{0} parameter(4)
  %constant_697 = f32[] constant(7.97193861e-05)
  %broadcast.686 = f32[448]{0} broadcast(f32[] %constant_697), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.553 = f32[448]{0} multiply(f32[448]{0} %param_4.390, f32[448]{0} %broadcast.686), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.267 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.553), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.12 = f32[16,28,28,448]{2,1,3,0} subtract(f32[16,28,28,448]{2,1,3,0} %param_3.104, f32[16,28,28,448]{2,1,3,0} %broadcast.267), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.105 = f32[448]{0} parameter(2)
  %constant_151 = f32[] constant(1e-05)
  %broadcast.269 = f32[448]{0} broadcast(f32[] %constant_151), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.67 = f32[448]{0} add(f32[448]{0} %param_2.105, f32[448]{0} %broadcast.269), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.34 = f32[448]{0} rsqrt(f32[448]{0} %add.67), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.127 = f32[448]{0} parameter(1)
  %multiply.201 = f32[448]{0} multiply(f32[448]{0} %rsqrt.34, f32[448]{0} %param_1.127), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.266 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.201), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.200 = f32[16,28,28,448]{2,1,3,0} multiply(f32[16,28,28,448]{2,1,3,0} %subtract.12, f32[16,28,28,448]{2,1,3,0} %broadcast.266), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.75 = f32[448]{0} parameter(0)
  %broadcast.265 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %param_0.75), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.66 = f32[16,28,28,448]{2,1,3,0} add(f32[16,28,28,448]{2,1,3,0} %multiply.200, f32[16,28,28,448]{2,1,3,0} %broadcast.265), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_152 = f32[] constant(0)
  %broadcast.268 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[] %constant_152), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  ROOT %maximum.10 = f32[16,28,28,448]{2,1,3,0} maximum(f32[16,28,28,448]{2,1,3,0} %add.66, f32[16,28,28,448]{2,1,3,0} %broadcast.268), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
}

%fused_computation.42 (param_0.76: f32[448], param_1.686: f32[448], param_2.626: f32[448], param_3.609: f32[16,448], param_4.440: f32[448], param_5.237: f32[448]) -> (f32[448], f32[448], f32[448]) {
  %param_0.76 = f32[448]{0} parameter(0)
  %param_1.686 = f32[448]{0} parameter(1)
  %constant_154 = f32[] constant(0.9)
  %broadcast.271 = f32[448]{0} broadcast(f32[] %constant_154), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.203 = f32[448]{0} multiply(f32[448]{0} %param_1.686, f32[448]{0} %broadcast.271), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.609 = f32[16,448]{1,0} parameter(3)
  %constant_156_clone_1 = f32[] constant(0)
  %reduce.298.clone.1 = f32[448]{0} reduce(f32[16,448]{1,0} %param_3.609, f32[] %constant_156_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_700_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.272.clone.1 = f32[448]{0} broadcast(f32[] %constant_700_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.205.clone.1 = f32[448]{0} multiply(f32[448]{0} %reduce.298.clone.1, f32[448]{0} %broadcast.272.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.626 = f32[448]{0} parameter(2)
  %multiply.555.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.626, f32[448]{0} %broadcast.272.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.204.clone.1 = f32[448]{0} multiply(f32[448]{0} %multiply.555.clone.1, f32[448]{0} %multiply.555.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.13.clone.1 = f32[448]{0} subtract(f32[448]{0} %multiply.205.clone.1, f32[448]{0} %multiply.204.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.273.clone.1 = f32[448]{0} broadcast(f32[] %constant_156_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.11.clone.1 = f32[448]{0} maximum(f32[448]{0} %subtract.13.clone.1, f32[448]{0} %broadcast.273.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_153 = f32[] constant(0.1)
  %broadcast.270 = f32[448]{0} broadcast(f32[] %constant_153), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.202 = f32[448]{0} multiply(f32[448]{0} %maximum.11.clone.1, f32[448]{0} %broadcast.270), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.69 = f32[448]{0} add(f32[448]{0} %multiply.203, f32[448]{0} %multiply.202), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.68 = f32[448]{0} add(f32[448]{0} %param_0.76, f32[448]{0} %add.69), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  %param_4.440 = f32[448]{0} parameter(4)
  %param_5.237 = f32[448]{0} parameter(5)
  %multiply.209.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_5.237, f32[448]{0} %broadcast.271), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_159_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.275.clone.1 = f32[448]{0} broadcast(f32[] %constant_159_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.208.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.626, f32[448]{0} %broadcast.275.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.71.clone.1 = f32[448]{0} add(f32[448]{0} %multiply.209.clone.1, f32[448]{0} %multiply.208.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.70.clone.1 = f32[448]{0} add(f32[448]{0} %param_4.440, f32[448]{0} %add.71.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  ROOT %tuple.21 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) tuple(f32[448]{0} %add.68, f32[448]{0} %maximum.11.clone.1, f32[448]{0} %add.70.clone.1)
}

%fused_computation.48 (param_0.88: f32[16,28,28,1792], param_1.147: f32[1792], param_2.123: f32[1792], param_3.121: f32[1792], param_4.92: f32[16,28,28,1792], param_5.173: f32[1792]) -> f32[16,28,28,1792] {
  %param_0.88 = f32[16,28,28,1792]{2,1,3,0} parameter(0)
  %param_4.92 = f32[16,28,28,1792]{2,1,3,0} parameter(4)
  %param_5.173 = f32[1792]{0} parameter(5)
  %constant_691 = f32[] constant(7.97193861e-05)
  %broadcast.682 = f32[1792]{0} broadcast(f32[] %constant_691), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.549 = f32[1792]{0} multiply(f32[1792]{0} %param_5.173, f32[1792]{0} %broadcast.682), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.279 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.549), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.14 = f32[16,28,28,1792]{2,1,3,0} subtract(f32[16,28,28,1792]{2,1,3,0} %param_4.92, f32[16,28,28,1792]{2,1,3,0} %broadcast.279), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.121 = f32[1792]{0} parameter(3)
  %constant_162 = f32[] constant(1e-05)
  %broadcast.281 = f32[1792]{0} broadcast(f32[] %constant_162), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.74 = f32[1792]{0} add(f32[1792]{0} %param_3.121, f32[1792]{0} %broadcast.281), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.35 = f32[1792]{0} rsqrt(f32[1792]{0} %add.74), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_2.123 = f32[1792]{0} parameter(2)
  %multiply.211 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.35, f32[1792]{0} %param_2.123), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.278 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.211), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.210 = f32[16,28,28,1792]{2,1,3,0} multiply(f32[16,28,28,1792]{2,1,3,0} %subtract.14, f32[16,28,28,1792]{2,1,3,0} %broadcast.278), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_1.147 = f32[1792]{0} parameter(1)
  %broadcast.277 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_1.147), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.73 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %multiply.210, f32[16,28,28,1792]{2,1,3,0} %broadcast.277), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.72 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %param_0.88, f32[16,28,28,1792]{2,1,3,0} %add.73), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %constant_163 = f32[] constant(0)
  %broadcast.280 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[] %constant_163), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %maximum.12 = f32[16,28,28,1792]{2,1,3,0} maximum(f32[16,28,28,1792]{2,1,3,0} %add.72, f32[16,28,28,1792]{2,1,3,0} %broadcast.280), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
}

%fused_computation.49 (param_0.89: f32[1792], param_1.689: f32[1792], param_2.630: f32[1792], param_3.614: f32[16,1792], param_4.446: f32[1792], param_5.244: f32[1792]) -> (f32[1792], f32[1792], f32[1792]) {
  %param_0.89 = f32[1792]{0} parameter(0)
  %param_1.689 = f32[1792]{0} parameter(1)
  %constant_165 = f32[] constant(0.9)
  %broadcast.283 = f32[1792]{0} broadcast(f32[] %constant_165), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.213 = f32[1792]{0} multiply(f32[1792]{0} %param_1.689, f32[1792]{0} %broadcast.283), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.614 = f32[16,1792]{1,0} parameter(3)
  %constant_167_clone_1 = f32[] constant(0)
  %reduce.301.clone.1 = f32[1792]{0} reduce(f32[16,1792]{1,0} %param_3.614, f32[] %constant_167_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_694_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.285.clone.1 = f32[1792]{0} broadcast(f32[] %constant_694_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.215.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %reduce.301.clone.1, f32[1792]{0} %broadcast.285.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.630 = f32[1792]{0} parameter(2)
  %multiply.551.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.630, f32[1792]{0} %broadcast.285.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.214.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %multiply.551.clone.1, f32[1792]{0} %multiply.551.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.15.clone.1 = f32[1792]{0} subtract(f32[1792]{0} %multiply.215.clone.1, f32[1792]{0} %multiply.214.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.284.clone.1 = f32[1792]{0} broadcast(f32[] %constant_167_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.13.clone.1 = f32[1792]{0} maximum(f32[1792]{0} %subtract.15.clone.1, f32[1792]{0} %broadcast.284.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_164 = f32[] constant(0.1)
  %broadcast.282 = f32[1792]{0} broadcast(f32[] %constant_164), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.212 = f32[1792]{0} multiply(f32[1792]{0} %maximum.13.clone.1, f32[1792]{0} %broadcast.282), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.76 = f32[1792]{0} add(f32[1792]{0} %multiply.213, f32[1792]{0} %multiply.212), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.75 = f32[1792]{0} add(f32[1792]{0} %param_0.89, f32[1792]{0} %add.76), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  %param_4.446 = f32[1792]{0} parameter(4)
  %param_5.244 = f32[1792]{0} parameter(5)
  %multiply.219.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_5.244, f32[1792]{0} %broadcast.283), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_170_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.287.clone.1 = f32[1792]{0} broadcast(f32[] %constant_170_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.218.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.630, f32[1792]{0} %broadcast.287.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.78.clone.1 = f32[1792]{0} add(f32[1792]{0} %multiply.219.clone.1, f32[1792]{0} %multiply.218.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.77.clone.1 = f32[1792]{0} add(f32[1792]{0} %param_4.446, f32[1792]{0} %add.78.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  ROOT %tuple.25 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) tuple(f32[1792]{0} %add.75, f32[1792]{0} %maximum.13.clone.1, f32[1792]{0} %add.77.clone.1)
}

%fused_computation.55 (param_0.101: f32[896], param_1.169: f32[896], param_2.142: f32[896], param_3.140: f32[16,28,28,896], param_4.389: f32[896]) -> f32[16,28,28,896] {
  %param_3.140 = f32[16,28,28,896]{2,1,3,0} parameter(3)
  %param_4.389 = f32[896]{0} parameter(4)
  %constant_685 = f32[] constant(7.97193861e-05)
  %broadcast.678 = f32[896]{0} broadcast(f32[] %constant_685), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.545 = f32[896]{0} multiply(f32[896]{0} %param_4.389, f32[896]{0} %broadcast.678), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.291 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.545), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.16 = f32[16,28,28,896]{2,1,3,0} subtract(f32[16,28,28,896]{2,1,3,0} %param_3.140, f32[16,28,28,896]{2,1,3,0} %broadcast.291), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.142 = f32[896]{0} parameter(2)
  %constant_173 = f32[] constant(1e-05)
  %broadcast.293 = f32[896]{0} broadcast(f32[] %constant_173), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.80 = f32[896]{0} add(f32[896]{0} %param_2.142, f32[896]{0} %broadcast.293), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.36 = f32[896]{0} rsqrt(f32[896]{0} %add.80), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.169 = f32[896]{0} parameter(1)
  %multiply.221 = f32[896]{0} multiply(f32[896]{0} %rsqrt.36, f32[896]{0} %param_1.169), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.290 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.221), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.220 = f32[16,28,28,896]{2,1,3,0} multiply(f32[16,28,28,896]{2,1,3,0} %subtract.16, f32[16,28,28,896]{2,1,3,0} %broadcast.290), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.101 = f32[896]{0} parameter(0)
  %broadcast.289 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %param_0.101), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.79 = f32[16,28,28,896]{2,1,3,0} add(f32[16,28,28,896]{2,1,3,0} %multiply.220, f32[16,28,28,896]{2,1,3,0} %broadcast.289), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_174 = f32[] constant(0)
  %broadcast.292 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[] %constant_174), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  ROOT %maximum.14 = f32[16,28,28,896]{2,1,3,0} maximum(f32[16,28,28,896]{2,1,3,0} %add.79, f32[16,28,28,896]{2,1,3,0} %broadcast.292), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
}

%fused_computation.56 (param_0.102: f32[896], param_1.692: f32[896], param_2.634: f32[896], param_3.619: f32[16,896], param_4.452: f32[896], param_5.251: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.102 = f32[896]{0} parameter(0)
  %param_1.692 = f32[896]{0} parameter(1)
  %constant_176 = f32[] constant(0.9)
  %broadcast.295 = f32[896]{0} broadcast(f32[] %constant_176), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.223 = f32[896]{0} multiply(f32[896]{0} %param_1.692, f32[896]{0} %broadcast.295), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.619 = f32[16,896]{1,0} parameter(3)
  %constant_178_clone_1 = f32[] constant(0)
  %reduce.304.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.619, f32[] %constant_178_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_688_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.296.clone.1 = f32[896]{0} broadcast(f32[] %constant_688_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.225.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.304.clone.1, f32[896]{0} %broadcast.296.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.634 = f32[896]{0} parameter(2)
  %multiply.547.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.634, f32[896]{0} %broadcast.296.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.224.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.547.clone.1, f32[896]{0} %multiply.547.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.17.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.225.clone.1, f32[896]{0} %multiply.224.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.297.clone.1 = f32[896]{0} broadcast(f32[] %constant_178_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.15.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.17.clone.1, f32[896]{0} %broadcast.297.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_175 = f32[] constant(0.1)
  %broadcast.294 = f32[896]{0} broadcast(f32[] %constant_175), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.222 = f32[896]{0} multiply(f32[896]{0} %maximum.15.clone.1, f32[896]{0} %broadcast.294), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.82 = f32[896]{0} add(f32[896]{0} %multiply.223, f32[896]{0} %multiply.222), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.81 = f32[896]{0} add(f32[896]{0} %param_0.102, f32[896]{0} %add.82), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  %param_4.452 = f32[896]{0} parameter(4)
  %param_5.251 = f32[896]{0} parameter(5)
  %multiply.229.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.251, f32[896]{0} %broadcast.295), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_181_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.299.clone.1 = f32[896]{0} broadcast(f32[] %constant_181_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.228.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.634, f32[896]{0} %broadcast.299.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.84.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.229.clone.1, f32[896]{0} %multiply.228.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.83.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.452, f32[896]{0} %add.84.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  ROOT %tuple.28 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.81, f32[896]{0} %maximum.15.clone.1, f32[896]{0} %add.83.clone.1)
}

%fused_computation.62 (param_0.114: f32[448], param_1.190: f32[448], param_2.161: f32[448], param_3.159: f32[16,28,28,448], param_4.388: f32[448]) -> f32[16,28,28,448] {
  %param_3.159 = f32[16,28,28,448]{2,1,3,0} parameter(3)
  %param_4.388 = f32[448]{0} parameter(4)
  %constant_679 = f32[] constant(7.97193861e-05)
  %broadcast.674 = f32[448]{0} broadcast(f32[] %constant_679), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.541 = f32[448]{0} multiply(f32[448]{0} %param_4.388, f32[448]{0} %broadcast.674), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.303 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.541), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.18 = f32[16,28,28,448]{2,1,3,0} subtract(f32[16,28,28,448]{2,1,3,0} %param_3.159, f32[16,28,28,448]{2,1,3,0} %broadcast.303), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.161 = f32[448]{0} parameter(2)
  %constant_184 = f32[] constant(1e-05)
  %broadcast.305 = f32[448]{0} broadcast(f32[] %constant_184), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.86 = f32[448]{0} add(f32[448]{0} %param_2.161, f32[448]{0} %broadcast.305), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.37 = f32[448]{0} rsqrt(f32[448]{0} %add.86), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.190 = f32[448]{0} parameter(1)
  %multiply.231 = f32[448]{0} multiply(f32[448]{0} %rsqrt.37, f32[448]{0} %param_1.190), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.302 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.231), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.230 = f32[16,28,28,448]{2,1,3,0} multiply(f32[16,28,28,448]{2,1,3,0} %subtract.18, f32[16,28,28,448]{2,1,3,0} %broadcast.302), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.114 = f32[448]{0} parameter(0)
  %broadcast.301 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %param_0.114), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.85 = f32[16,28,28,448]{2,1,3,0} add(f32[16,28,28,448]{2,1,3,0} %multiply.230, f32[16,28,28,448]{2,1,3,0} %broadcast.301), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_185 = f32[] constant(0)
  %broadcast.304 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[] %constant_185), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  ROOT %maximum.16 = f32[16,28,28,448]{2,1,3,0} maximum(f32[16,28,28,448]{2,1,3,0} %add.85, f32[16,28,28,448]{2,1,3,0} %broadcast.304), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
}

%fused_computation.63 (param_0.115: f32[448], param_1.695: f32[448], param_2.638: f32[448], param_3.624: f32[16,448], param_4.458: f32[448], param_5.258: f32[448]) -> (f32[448], f32[448], f32[448]) {
  %param_0.115 = f32[448]{0} parameter(0)
  %param_1.695 = f32[448]{0} parameter(1)
  %constant_187 = f32[] constant(0.9)
  %broadcast.307 = f32[448]{0} broadcast(f32[] %constant_187), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.233 = f32[448]{0} multiply(f32[448]{0} %param_1.695, f32[448]{0} %broadcast.307), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.624 = f32[16,448]{1,0} parameter(3)
  %constant_189_clone_1 = f32[] constant(0)
  %reduce.307.clone.1 = f32[448]{0} reduce(f32[16,448]{1,0} %param_3.624, f32[] %constant_189_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_682_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.308.clone.1 = f32[448]{0} broadcast(f32[] %constant_682_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.235.clone.1 = f32[448]{0} multiply(f32[448]{0} %reduce.307.clone.1, f32[448]{0} %broadcast.308.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.638 = f32[448]{0} parameter(2)
  %multiply.543.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.638, f32[448]{0} %broadcast.308.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.234.clone.1 = f32[448]{0} multiply(f32[448]{0} %multiply.543.clone.1, f32[448]{0} %multiply.543.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.19.clone.1 = f32[448]{0} subtract(f32[448]{0} %multiply.235.clone.1, f32[448]{0} %multiply.234.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.309.clone.1 = f32[448]{0} broadcast(f32[] %constant_189_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.17.clone.1 = f32[448]{0} maximum(f32[448]{0} %subtract.19.clone.1, f32[448]{0} %broadcast.309.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_186 = f32[] constant(0.1)
  %broadcast.306 = f32[448]{0} broadcast(f32[] %constant_186), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.232 = f32[448]{0} multiply(f32[448]{0} %maximum.17.clone.1, f32[448]{0} %broadcast.306), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.88 = f32[448]{0} add(f32[448]{0} %multiply.233, f32[448]{0} %multiply.232), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.87 = f32[448]{0} add(f32[448]{0} %param_0.115, f32[448]{0} %add.88), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  %param_4.458 = f32[448]{0} parameter(4)
  %param_5.258 = f32[448]{0} parameter(5)
  %multiply.239.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_5.258, f32[448]{0} %broadcast.307), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_192_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.311.clone.1 = f32[448]{0} broadcast(f32[] %constant_192_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.238.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.638, f32[448]{0} %broadcast.311.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.90.clone.1 = f32[448]{0} add(f32[448]{0} %multiply.239.clone.1, f32[448]{0} %multiply.238.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.89.clone.1 = f32[448]{0} add(f32[448]{0} %param_4.458, f32[448]{0} %add.90.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  ROOT %tuple.42 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) tuple(f32[448]{0} %add.87, f32[448]{0} %maximum.17.clone.1, f32[448]{0} %add.89.clone.1)
}

%fused_computation.69 (param_0.484: f32[16,28,28,1792], param_1.652: f32[1792], param_2.575: f32[1792], param_3.569: f32[1792], param_4.387: f32[16,28,28,1792], param_5.168: f32[1792]) -> f32[16,28,28,1792] {
  %param_0.484 = f32[16,28,28,1792]{2,1,3,0} parameter(0)
  %param_4.387 = f32[16,28,28,1792]{2,1,3,0} parameter(4)
  %param_5.168 = f32[1792]{0} parameter(5)
  %constant_676 = f32[] constant(7.97193861e-05)
  %broadcast.672 = f32[1792]{0} broadcast(f32[] %constant_676), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.539 = f32[1792]{0} multiply(f32[1792]{0} %param_5.168, f32[1792]{0} %broadcast.672), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.671 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.539), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.71 = f32[16,28,28,1792]{2,1,3,0} subtract(f32[16,28,28,1792]{2,1,3,0} %param_4.387, f32[16,28,28,1792]{2,1,3,0} %broadcast.671), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.569 = f32[1792]{0} parameter(3)
  %constant_675 = f32[] constant(1e-05)
  %broadcast.670 = f32[1792]{0} broadcast(f32[] %constant_675), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.247 = f32[1792]{0} add(f32[1792]{0} %param_3.569, f32[1792]{0} %broadcast.670), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.99 = f32[1792]{0} rsqrt(f32[1792]{0} %add.247), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_2.575 = f32[1792]{0} parameter(2)
  %multiply.538 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.99, f32[1792]{0} %param_2.575), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.669 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.538), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.537 = f32[16,28,28,1792]{2,1,3,0} multiply(f32[16,28,28,1792]{2,1,3,0} %subtract.71, f32[16,28,28,1792]{2,1,3,0} %broadcast.669), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_1.652 = f32[1792]{0} parameter(1)
  %broadcast.668 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_1.652), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.246 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %multiply.537, f32[16,28,28,1792]{2,1,3,0} %broadcast.668), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.245 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %param_0.484, f32[16,28,28,1792]{2,1,3,0} %add.246), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %constant_195 = f32[] constant(0)
  %broadcast.313 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[] %constant_195), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %maximum.18 = f32[16,28,28,1792]{2,1,3,0} maximum(f32[16,28,28,1792]{2,1,3,0} %add.245, f32[16,28,28,1792]{2,1,3,0} %broadcast.313), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
}

%fused_computation.71 (param_0.127: f32[1792], param_1.698: f32[1792], param_2.642: f32[1792], param_3.629: f32[16,1792], param_4.464: f32[1792], param_5.265: f32[1792]) -> (f32[1792], f32[1792], f32[1792]) {
  %param_0.127 = f32[1792]{0} parameter(0)
  %param_1.698 = f32[1792]{0} parameter(1)
  %constant_198 = f32[] constant(0.9)
  %broadcast.319 = f32[1792]{0} broadcast(f32[] %constant_198), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.243 = f32[1792]{0} multiply(f32[1792]{0} %param_1.698, f32[1792]{0} %broadcast.319), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.629 = f32[16,1792]{1,0} parameter(3)
  %constant_200_clone_1 = f32[] constant(0)
  %reduce.310.clone.1 = f32[1792]{0} reduce(f32[16,1792]{1,0} %param_3.629, f32[] %constant_200_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_666_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.321.clone.1 = f32[1792]{0} broadcast(f32[] %constant_666_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.245.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %reduce.310.clone.1, f32[1792]{0} %broadcast.321.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.642 = f32[1792]{0} parameter(2)
  %multiply.527.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.642, f32[1792]{0} %broadcast.321.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.244.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %multiply.527.clone.1, f32[1792]{0} %multiply.527.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.21.clone.1 = f32[1792]{0} subtract(f32[1792]{0} %multiply.245.clone.1, f32[1792]{0} %multiply.244.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.320.clone.1 = f32[1792]{0} broadcast(f32[] %constant_200_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.19.clone.1 = f32[1792]{0} maximum(f32[1792]{0} %subtract.21.clone.1, f32[1792]{0} %broadcast.320.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_197 = f32[] constant(0.1)
  %broadcast.318 = f32[1792]{0} broadcast(f32[] %constant_197), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.242 = f32[1792]{0} multiply(f32[1792]{0} %maximum.19.clone.1, f32[1792]{0} %broadcast.318), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.95 = f32[1792]{0} add(f32[1792]{0} %multiply.243, f32[1792]{0} %multiply.242), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.94 = f32[1792]{0} add(f32[1792]{0} %param_0.127, f32[1792]{0} %add.95), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  %param_4.464 = f32[1792]{0} parameter(4)
  %param_5.265 = f32[1792]{0} parameter(5)
  %multiply.249.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_5.265, f32[1792]{0} %broadcast.319), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_203_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.323.clone.1 = f32[1792]{0} broadcast(f32[] %constant_203_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.248.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.642, f32[1792]{0} %broadcast.323.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.97.clone.1 = f32[1792]{0} add(f32[1792]{0} %multiply.249.clone.1, f32[1792]{0} %multiply.248.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.96.clone.1 = f32[1792]{0} add(f32[1792]{0} %param_4.464, f32[1792]{0} %add.97.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  ROOT %tuple.45 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) tuple(f32[1792]{0} %add.94, f32[1792]{0} %maximum.19.clone.1, f32[1792]{0} %add.96.clone.1)
}

%fused_computation.77 (param_0.139: f32[896], param_1.234: f32[896], param_2.198: f32[896], param_3.195: f32[16,28,28,896], param_4.377: f32[896]) -> f32[16,28,28,896] {
  %param_3.195 = f32[16,28,28,896]{2,1,3,0} parameter(3)
  %param_4.377 = f32[896]{0} parameter(4)
  %constant_657 = f32[] constant(7.97193861e-05)
  %broadcast.646 = f32[896]{0} broadcast(f32[] %constant_657), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.521 = f32[896]{0} multiply(f32[896]{0} %param_4.377, f32[896]{0} %broadcast.646), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.327 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.521), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.22 = f32[16,28,28,896]{2,1,3,0} subtract(f32[16,28,28,896]{2,1,3,0} %param_3.195, f32[16,28,28,896]{2,1,3,0} %broadcast.327), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.198 = f32[896]{0} parameter(2)
  %constant_206 = f32[] constant(1e-05)
  %broadcast.329 = f32[896]{0} broadcast(f32[] %constant_206), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.99 = f32[896]{0} add(f32[896]{0} %param_2.198, f32[896]{0} %broadcast.329), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.39 = f32[896]{0} rsqrt(f32[896]{0} %add.99), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.234 = f32[896]{0} parameter(1)
  %multiply.251 = f32[896]{0} multiply(f32[896]{0} %rsqrt.39, f32[896]{0} %param_1.234), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.326 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.251), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.250 = f32[16,28,28,896]{2,1,3,0} multiply(f32[16,28,28,896]{2,1,3,0} %subtract.22, f32[16,28,28,896]{2,1,3,0} %broadcast.326), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.139 = f32[896]{0} parameter(0)
  %broadcast.325 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %param_0.139), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.98 = f32[16,28,28,896]{2,1,3,0} add(f32[16,28,28,896]{2,1,3,0} %multiply.250, f32[16,28,28,896]{2,1,3,0} %broadcast.325), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_207 = f32[] constant(0)
  %broadcast.328 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[] %constant_207), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  ROOT %maximum.20 = f32[16,28,28,896]{2,1,3,0} maximum(f32[16,28,28,896]{2,1,3,0} %add.98, f32[16,28,28,896]{2,1,3,0} %broadcast.328), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
}

%fused_computation.78 (param_0.140: f32[896], param_1.701: f32[896], param_2.646: f32[896], param_3.634: f32[16,896], param_4.470: f32[896], param_5.272: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.140 = f32[896]{0} parameter(0)
  %param_1.701 = f32[896]{0} parameter(1)
  %constant_209 = f32[] constant(0.9)
  %broadcast.331 = f32[896]{0} broadcast(f32[] %constant_209), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.253 = f32[896]{0} multiply(f32[896]{0} %param_1.701, f32[896]{0} %broadcast.331), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.634 = f32[16,896]{1,0} parameter(3)
  %constant_211_clone_1 = f32[] constant(0)
  %reduce.313.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.634, f32[] %constant_211_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_660_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.332.clone.1 = f32[896]{0} broadcast(f32[] %constant_660_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.255.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.313.clone.1, f32[896]{0} %broadcast.332.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.646 = f32[896]{0} parameter(2)
  %multiply.523.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.646, f32[896]{0} %broadcast.332.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.254.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.523.clone.1, f32[896]{0} %multiply.523.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.23.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.255.clone.1, f32[896]{0} %multiply.254.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.333.clone.1 = f32[896]{0} broadcast(f32[] %constant_211_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.21.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.23.clone.1, f32[896]{0} %broadcast.333.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_208 = f32[] constant(0.1)
  %broadcast.330 = f32[896]{0} broadcast(f32[] %constant_208), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.252 = f32[896]{0} multiply(f32[896]{0} %maximum.21.clone.1, f32[896]{0} %broadcast.330), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.101 = f32[896]{0} add(f32[896]{0} %multiply.253, f32[896]{0} %multiply.252), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.100 = f32[896]{0} add(f32[896]{0} %param_0.140, f32[896]{0} %add.101), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  %param_4.470 = f32[896]{0} parameter(4)
  %param_5.272 = f32[896]{0} parameter(5)
  %multiply.259.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.272, f32[896]{0} %broadcast.331), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_214_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.335.clone.1 = f32[896]{0} broadcast(f32[] %constant_214_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.258.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.646, f32[896]{0} %broadcast.335.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.103.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.259.clone.1, f32[896]{0} %multiply.258.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.102.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.470, f32[896]{0} %add.103.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  ROOT %tuple.48 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.100, f32[896]{0} %maximum.21.clone.1, f32[896]{0} %add.102.clone.1)
}

%fused_computation.84 (param_0.371: f32[448], param_1.595: f32[448], param_2.484: f32[448], param_3.492: f32[16,28,28,448], param_4.376: f32[448]) -> f32[16,28,28,448] {
  %param_3.492 = f32[16,28,28,448]{2,1,3,0} parameter(3)
  %param_4.376 = f32[448]{0} parameter(4)
  %constant_651 = f32[] constant(7.97193861e-05)
  %broadcast.642 = f32[448]{0} broadcast(f32[] %constant_651), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.517 = f32[448]{0} multiply(f32[448]{0} %param_4.376, f32[448]{0} %broadcast.642), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.340 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.517), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.24 = f32[16,28,28,448]{2,1,3,0} subtract(f32[16,28,28,448]{2,1,3,0} %param_3.492, f32[16,28,28,448]{2,1,3,0} %broadcast.340), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.484 = f32[448]{0} parameter(2)
  %constant_217 = f32[] constant(1e-05)
  %broadcast.341 = f32[448]{0} broadcast(f32[] %constant_217), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.105 = f32[448]{0} add(f32[448]{0} %param_2.484, f32[448]{0} %broadcast.341), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.40 = f32[448]{0} rsqrt(f32[448]{0} %add.105), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.595 = f32[448]{0} parameter(1)
  %multiply.261 = f32[448]{0} multiply(f32[448]{0} %rsqrt.40, f32[448]{0} %param_1.595), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.339 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.261), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.260 = f32[16,28,28,448]{2,1,3,0} multiply(f32[16,28,28,448]{2,1,3,0} %subtract.24, f32[16,28,28,448]{2,1,3,0} %broadcast.339), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.371 = f32[448]{0} parameter(0)
  %broadcast.338 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[448]{0} %param_0.371), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.104 = f32[16,28,28,448]{2,1,3,0} add(f32[16,28,28,448]{2,1,3,0} %multiply.260, f32[16,28,28,448]{2,1,3,0} %broadcast.338), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_218 = f32[] constant(0)
  %broadcast.337 = f32[16,28,28,448]{2,1,3,0} broadcast(f32[] %constant_218), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  ROOT %maximum.22 = f32[16,28,28,448]{2,1,3,0} maximum(f32[16,28,28,448]{2,1,3,0} %add.104, f32[16,28,28,448]{2,1,3,0} %broadcast.337), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
}

%fused_computation.85 (param_0.152: f32[448], param_1.704: f32[448], param_2.650: f32[448], param_3.639: f32[16,448], param_4.476: f32[448], param_5.279: f32[448]) -> (f32[448], f32[448], f32[448]) {
  %param_0.152 = f32[448]{0} parameter(0)
  %param_1.704 = f32[448]{0} parameter(1)
  %constant_220 = f32[] constant(0.9)
  %broadcast.343 = f32[448]{0} broadcast(f32[] %constant_220), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.263 = f32[448]{0} multiply(f32[448]{0} %param_1.704, f32[448]{0} %broadcast.343), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.639 = f32[16,448]{1,0} parameter(3)
  %constant_222_clone_1 = f32[] constant(0)
  %reduce.316.clone.1 = f32[448]{0} reduce(f32[16,448]{1,0} %param_3.639, f32[] %constant_222_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_654_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.344.clone.1 = f32[448]{0} broadcast(f32[] %constant_654_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.265.clone.1 = f32[448]{0} multiply(f32[448]{0} %reduce.316.clone.1, f32[448]{0} %broadcast.344.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.650 = f32[448]{0} parameter(2)
  %multiply.519.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.650, f32[448]{0} %broadcast.344.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.264.clone.1 = f32[448]{0} multiply(f32[448]{0} %multiply.519.clone.1, f32[448]{0} %multiply.519.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.25.clone.1 = f32[448]{0} subtract(f32[448]{0} %multiply.265.clone.1, f32[448]{0} %multiply.264.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.345.clone.1 = f32[448]{0} broadcast(f32[] %constant_222_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.23.clone.1 = f32[448]{0} maximum(f32[448]{0} %subtract.25.clone.1, f32[448]{0} %broadcast.345.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_219 = f32[] constant(0.1)
  %broadcast.342 = f32[448]{0} broadcast(f32[] %constant_219), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.262 = f32[448]{0} multiply(f32[448]{0} %maximum.23.clone.1, f32[448]{0} %broadcast.342), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.107 = f32[448]{0} add(f32[448]{0} %multiply.263, f32[448]{0} %multiply.262), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.106 = f32[448]{0} add(f32[448]{0} %param_0.152, f32[448]{0} %add.107), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  %param_4.476 = f32[448]{0} parameter(4)
  %param_5.279 = f32[448]{0} parameter(5)
  %multiply.269.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_5.279, f32[448]{0} %broadcast.343), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_225_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.347.clone.1 = f32[448]{0} broadcast(f32[] %constant_225_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.268.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.650, f32[448]{0} %broadcast.347.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.109.clone.1 = f32[448]{0} add(f32[448]{0} %multiply.269.clone.1, f32[448]{0} %multiply.268.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.108.clone.1 = f32[448]{0} add(f32[448]{0} %param_4.476, f32[448]{0} %add.109.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  ROOT %tuple.51 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) tuple(f32[448]{0} %add.106, f32[448]{0} %maximum.23.clone.1, f32[448]{0} %add.108.clone.1)
}

%fused_computation.91 (param_0.470: f32[1792], param_1.641: f32[1792], param_2.559: f32[1792], param_3.557: f32[16,28,28,1792], param_4.375: f32[1792], param_5.151: f32[1792], param_6.121: f32[1792], param_7.51: f32[1792], param_8.38: f32[16,28,28,1792], param_9.40: f32[1792]) -> f32[16,28,28,1792] {
  %param_8.38 = f32[16,28,28,1792]{2,1,3,0} parameter(8)
  %param_9.40 = f32[1792]{0} parameter(9)
  %constant_646 = f32[] constant(7.97193861e-05)
  %broadcast.640 = f32[1792]{0} broadcast(f32[] %constant_646), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.515 = f32[1792]{0} multiply(f32[1792]{0} %param_9.40, f32[1792]{0} %broadcast.640), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.639 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.515), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.67 = f32[16,28,28,1792]{2,1,3,0} subtract(f32[16,28,28,1792]{2,1,3,0} %param_8.38, f32[16,28,28,1792]{2,1,3,0} %broadcast.639), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_7.51 = f32[1792]{0} parameter(7)
  %constant_647 = f32[] constant(1e-05)
  %broadcast.638 = f32[1792]{0} broadcast(f32[] %constant_647), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.235 = f32[1792]{0} add(f32[1792]{0} %param_7.51, f32[1792]{0} %broadcast.638), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.95 = f32[1792]{0} rsqrt(f32[1792]{0} %add.235), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_6.121 = f32[1792]{0} parameter(6)
  %multiply.514 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.95, f32[1792]{0} %param_6.121), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.637 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.514), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.513 = f32[16,28,28,1792]{2,1,3,0} multiply(f32[16,28,28,1792]{2,1,3,0} %subtract.67, f32[16,28,28,1792]{2,1,3,0} %broadcast.637), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_5.151 = f32[1792]{0} parameter(5)
  %broadcast.636 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_5.151), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.234 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %multiply.513, f32[16,28,28,1792]{2,1,3,0} %broadcast.636), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %param_3.557 = f32[16,28,28,1792]{2,1,3,0} parameter(3)
  %param_4.375 = f32[1792]{0} parameter(4)
  %multiply.512 = f32[1792]{0} multiply(f32[1792]{0} %param_4.375, f32[1792]{0} %broadcast.640), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.634 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.512), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.66 = f32[16,28,28,1792]{2,1,3,0} subtract(f32[16,28,28,1792]{2,1,3,0} %param_3.557, f32[16,28,28,1792]{2,1,3,0} %broadcast.634), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.559 = f32[1792]{0} parameter(2)
  %add.233 = f32[1792]{0} add(f32[1792]{0} %param_2.559, f32[1792]{0} %broadcast.638), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.94 = f32[1792]{0} rsqrt(f32[1792]{0} %add.233), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.641 = f32[1792]{0} parameter(1)
  %multiply.511 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.94, f32[1792]{0} %param_1.641), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.633 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.511), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.510 = f32[16,28,28,1792]{2,1,3,0} multiply(f32[16,28,28,1792]{2,1,3,0} %subtract.66, f32[16,28,28,1792]{2,1,3,0} %broadcast.633), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.470 = f32[1792]{0} parameter(0)
  %broadcast.632 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_0.470), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.232 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %multiply.510, f32[16,28,28,1792]{2,1,3,0} %broadcast.632), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.231 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %add.234, f32[16,28,28,1792]{2,1,3,0} %add.232), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %constant_228 = f32[] constant(0)
  %broadcast.349 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[] %constant_228), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %maximum.24 = f32[16,28,28,1792]{2,1,3,0} maximum(f32[16,28,28,1792]{2,1,3,0} %add.231, f32[16,28,28,1792]{2,1,3,0} %broadcast.349), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
}

%fused_computation.93 (param_0.165: f32[1792], param_1.707: f32[1792], param_2.654: f32[1792], param_3.644: f32[16,1792], param_4.482: f32[1792], param_5.286: f32[1792]) -> (f32[1792], f32[1792], f32[1792]) {
  %param_0.165 = f32[1792]{0} parameter(0)
  %param_1.707 = f32[1792]{0} parameter(1)
  %constant_231 = f32[] constant(0.9)
  %broadcast.358 = f32[1792]{0} broadcast(f32[] %constant_231), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.275 = f32[1792]{0} multiply(f32[1792]{0} %param_1.707, f32[1792]{0} %broadcast.358), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.644 = f32[16,1792]{1,0} parameter(3)
  %constant_233_clone_1 = f32[] constant(0)
  %reduce.319.clone.1 = f32[1792]{0} reduce(f32[16,1792]{1,0} %param_3.644, f32[] %constant_233_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_634_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.360.clone.1 = f32[1792]{0} broadcast(f32[] %constant_634_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.277.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %reduce.319.clone.1, f32[1792]{0} %broadcast.360.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.654 = f32[1792]{0} parameter(2)
  %multiply.491.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.654, f32[1792]{0} %broadcast.360.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.276.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %multiply.491.clone.1, f32[1792]{0} %multiply.491.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.28.clone.1 = f32[1792]{0} subtract(f32[1792]{0} %multiply.277.clone.1, f32[1792]{0} %multiply.276.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.359.clone.1 = f32[1792]{0} broadcast(f32[] %constant_233_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.25.clone.1 = f32[1792]{0} maximum(f32[1792]{0} %subtract.28.clone.1, f32[1792]{0} %broadcast.359.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_230 = f32[] constant(0.1)
  %broadcast.357 = f32[1792]{0} broadcast(f32[] %constant_230), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.274 = f32[1792]{0} multiply(f32[1792]{0} %maximum.25.clone.1, f32[1792]{0} %broadcast.357), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.116 = f32[1792]{0} add(f32[1792]{0} %multiply.275, f32[1792]{0} %multiply.274), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.115 = f32[1792]{0} add(f32[1792]{0} %param_0.165, f32[1792]{0} %add.116), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  %param_4.482 = f32[1792]{0} parameter(4)
  %param_5.286 = f32[1792]{0} parameter(5)
  %multiply.281.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_5.286, f32[1792]{0} %broadcast.358), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_236_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.362.clone.1 = f32[1792]{0} broadcast(f32[] %constant_236_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.280.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.654, f32[1792]{0} %broadcast.362.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.118.clone.1 = f32[1792]{0} add(f32[1792]{0} %multiply.281.clone.1, f32[1792]{0} %multiply.280.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.117.clone.1 = f32[1792]{0} add(f32[1792]{0} %param_4.482, f32[1792]{0} %add.118.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  ROOT %tuple.54 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) tuple(f32[1792]{0} %add.115, f32[1792]{0} %maximum.25.clone.1, f32[1792]{0} %add.117.clone.1)
}

%fused_computation.99 (param_0.175: f32[1792], param_1.710: f32[1792], param_2.658: f32[1792], param_3.649: f32[16,1792], param_4.488: f32[1792], param_5.293: f32[1792]) -> (f32[1792], f32[1792], f32[1792]) {
  %param_0.175 = f32[1792]{0} parameter(0)
  %param_1.710 = f32[1792]{0} parameter(1)
  %constant_240 = f32[] constant(0.9)
  %broadcast.365 = f32[1792]{0} broadcast(f32[] %constant_240), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.283 = f32[1792]{0} multiply(f32[1792]{0} %param_1.710, f32[1792]{0} %broadcast.365), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.649 = f32[16,1792]{1,0} parameter(3)
  %constant_242_clone_1 = f32[] constant(0)
  %reduce.322.clone.1 = f32[1792]{0} reduce(f32[16,1792]{1,0} %param_3.649, f32[] %constant_242_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_628_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.367.clone.1 = f32[1792]{0} broadcast(f32[] %constant_628_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.285.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %reduce.322.clone.1, f32[1792]{0} %broadcast.367.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.658 = f32[1792]{0} parameter(2)
  %multiply.487.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.658, f32[1792]{0} %broadcast.367.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.284.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %multiply.487.clone.1, f32[1792]{0} %multiply.487.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.29.clone.1 = f32[1792]{0} subtract(f32[1792]{0} %multiply.285.clone.1, f32[1792]{0} %multiply.284.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.366.clone.1 = f32[1792]{0} broadcast(f32[] %constant_242_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.26.clone.1 = f32[1792]{0} maximum(f32[1792]{0} %subtract.29.clone.1, f32[1792]{0} %broadcast.366.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_239 = f32[] constant(0.1)
  %broadcast.364 = f32[1792]{0} broadcast(f32[] %constant_239), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.282 = f32[1792]{0} multiply(f32[1792]{0} %maximum.26.clone.1, f32[1792]{0} %broadcast.364), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.120 = f32[1792]{0} add(f32[1792]{0} %multiply.283, f32[1792]{0} %multiply.282), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.119 = f32[1792]{0} add(f32[1792]{0} %param_0.175, f32[1792]{0} %add.120), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  %param_4.488 = f32[1792]{0} parameter(4)
  %param_5.293 = f32[1792]{0} parameter(5)
  %multiply.289.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_5.293, f32[1792]{0} %broadcast.365), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_245_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.369.clone.1 = f32[1792]{0} broadcast(f32[] %constant_245_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.288.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_2.658, f32[1792]{0} %broadcast.369.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.122.clone.1 = f32[1792]{0} add(f32[1792]{0} %multiply.289.clone.1, f32[1792]{0} %multiply.288.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.121.clone.1 = f32[1792]{0} add(f32[1792]{0} %param_4.488, f32[1792]{0} %add.122.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  ROOT %tuple.57 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) tuple(f32[1792]{0} %add.119, f32[1792]{0} %maximum.26.clone.1, f32[1792]{0} %add.121.clone.1)
}

%fused_computation.105 (param_0.379: f32[896], param_1.598: f32[896], param_2.490: f32[896], param_3.499: f32[16,28,28,896], param_4.360: f32[896]) -> f32[16,28,28,896] {
  %param_3.499 = f32[16,28,28,896]{2,1,3,0} parameter(3)
  %param_4.360 = f32[896]{0} parameter(4)
  %constant_619 = f32[] constant(7.97193861e-05)
  %broadcast.594 = f32[896]{0} broadcast(f32[] %constant_619), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.481 = f32[896]{0} multiply(f32[896]{0} %param_4.360, f32[896]{0} %broadcast.594), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.374 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.481), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.30 = f32[16,28,28,896]{2,1,3,0} subtract(f32[16,28,28,896]{2,1,3,0} %param_3.499, f32[16,28,28,896]{2,1,3,0} %broadcast.374), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.490 = f32[896]{0} parameter(2)
  %constant_248 = f32[] constant(1e-05)
  %broadcast.375 = f32[896]{0} broadcast(f32[] %constant_248), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.124 = f32[896]{0} add(f32[896]{0} %param_2.490, f32[896]{0} %broadcast.375), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.43 = f32[896]{0} rsqrt(f32[896]{0} %add.124), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.598 = f32[896]{0} parameter(1)
  %multiply.291 = f32[896]{0} multiply(f32[896]{0} %rsqrt.43, f32[896]{0} %param_1.598), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.373 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.291), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.290 = f32[16,28,28,896]{2,1,3,0} multiply(f32[16,28,28,896]{2,1,3,0} %subtract.30, f32[16,28,28,896]{2,1,3,0} %broadcast.373), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.379 = f32[896]{0} parameter(0)
  %broadcast.372 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[896]{0} %param_0.379), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.123 = f32[16,28,28,896]{2,1,3,0} add(f32[16,28,28,896]{2,1,3,0} %multiply.290, f32[16,28,28,896]{2,1,3,0} %broadcast.372), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_249 = f32[] constant(0)
  %broadcast.371 = f32[16,28,28,896]{2,1,3,0} broadcast(f32[] %constant_249), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  ROOT %maximum.27 = f32[16,28,28,896]{2,1,3,0} maximum(f32[16,28,28,896]{2,1,3,0} %add.123, f32[16,28,28,896]{2,1,3,0} %broadcast.371), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
}

%fused_computation.106 (param_0.186: f32[896], param_1.713: f32[896], param_2.662: f32[896], param_3.654: f32[16,896], param_4.494: f32[896], param_5.300: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.186 = f32[896]{0} parameter(0)
  %param_1.713 = f32[896]{0} parameter(1)
  %constant_251 = f32[] constant(0.9)
  %broadcast.377 = f32[896]{0} broadcast(f32[] %constant_251), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.293 = f32[896]{0} multiply(f32[896]{0} %param_1.713, f32[896]{0} %broadcast.377), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.654 = f32[16,896]{1,0} parameter(3)
  %constant_253_clone_1 = f32[] constant(0)
  %reduce.325.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.654, f32[] %constant_253_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_622_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.378.clone.1 = f32[896]{0} broadcast(f32[] %constant_622_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.295.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.325.clone.1, f32[896]{0} %broadcast.378.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.662 = f32[896]{0} parameter(2)
  %multiply.483.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.662, f32[896]{0} %broadcast.378.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.294.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.483.clone.1, f32[896]{0} %multiply.483.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.31.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.295.clone.1, f32[896]{0} %multiply.294.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.379.clone.1 = f32[896]{0} broadcast(f32[] %constant_253_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.28.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.31.clone.1, f32[896]{0} %broadcast.379.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_250 = f32[] constant(0.1)
  %broadcast.376 = f32[896]{0} broadcast(f32[] %constant_250), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.292 = f32[896]{0} multiply(f32[896]{0} %maximum.28.clone.1, f32[896]{0} %broadcast.376), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.126 = f32[896]{0} add(f32[896]{0} %multiply.293, f32[896]{0} %multiply.292), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.125 = f32[896]{0} add(f32[896]{0} %param_0.186, f32[896]{0} %add.126), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  %param_4.494 = f32[896]{0} parameter(4)
  %param_5.300 = f32[896]{0} parameter(5)
  %multiply.299.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.300, f32[896]{0} %broadcast.377), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_256_clone_1 = f32[] constant(7.97193843e-06)
  %broadcast.381.clone.1 = f32[896]{0} broadcast(f32[] %constant_256_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.298.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.662, f32[896]{0} %broadcast.381.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.128.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.299.clone.1, f32[896]{0} %multiply.298.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.127.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.494, f32[896]{0} %add.128.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  ROOT %tuple.60 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.125, f32[896]{0} %maximum.28.clone.1, f32[896]{0} %add.127.clone.1)
}

%fused_computation.112 (param_0.382: f32[448], param_1.600: f32[448], param_2.493: f32[448], param_3.502: f32[16,56,56,448], param_4.359: f32[448]) -> f32[16,57,57,448] {
  %param_3.502 = f32[16,56,56,448]{2,1,3,0} parameter(3)
  %param_4.359 = f32[448]{0} parameter(4)
  %constant_613 = f32[] constant(1.99298465e-05)
  %broadcast.590 = f32[448]{0} broadcast(f32[] %constant_613), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.477 = f32[448]{0} multiply(f32[448]{0} %param_4.359, f32[448]{0} %broadcast.590), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.385 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.477), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.32 = f32[16,56,56,448]{2,1,3,0} subtract(f32[16,56,56,448]{2,1,3,0} %param_3.502, f32[16,56,56,448]{2,1,3,0} %broadcast.385), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.493 = f32[448]{0} parameter(2)
  %constant_259 = f32[] constant(1e-05)
  %broadcast.387 = f32[448]{0} broadcast(f32[] %constant_259), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.130 = f32[448]{0} add(f32[448]{0} %param_2.493, f32[448]{0} %broadcast.387), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.44 = f32[448]{0} rsqrt(f32[448]{0} %add.130), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.600 = f32[448]{0} parameter(1)
  %multiply.301 = f32[448]{0} multiply(f32[448]{0} %rsqrt.44, f32[448]{0} %param_1.600), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.384 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.301), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.300 = f32[16,56,56,448]{2,1,3,0} multiply(f32[16,56,56,448]{2,1,3,0} %subtract.32, f32[16,56,56,448]{2,1,3,0} %broadcast.384), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.382 = f32[448]{0} parameter(0)
  %broadcast.383 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %param_0.382), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.129 = f32[16,56,56,448]{2,1,3,0} add(f32[16,56,56,448]{2,1,3,0} %multiply.300, f32[16,56,56,448]{2,1,3,0} %broadcast.383), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_260 = f32[] constant(0)
  %broadcast.386 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[] %constant_260), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %maximum.29 = f32[16,56,56,448]{2,1,3,0} maximum(f32[16,56,56,448]{2,1,3,0} %add.129, f32[16,56,56,448]{2,1,3,0} %broadcast.386), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  ROOT %pad.3 = f32[16,57,57,448]{2,1,3,0} pad(f32[16,56,56,448]{2,1,3,0} %maximum.29, f32[] %constant_260), padding=0_0x0_1x0_1x0_0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
}

%fused_computation.113 (param_0.198: f32[448], param_1.716: f32[448], param_2.666: f32[448], param_3.659: f32[16,448], param_4.500: f32[448], param_5.307: f32[448]) -> (f32[448], f32[448], f32[448]) {
  %param_0.198 = f32[448]{0} parameter(0)
  %param_1.716 = f32[448]{0} parameter(1)
  %constant_262 = f32[] constant(0.9)
  %broadcast.389 = f32[448]{0} broadcast(f32[] %constant_262), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.303 = f32[448]{0} multiply(f32[448]{0} %param_1.716, f32[448]{0} %broadcast.389), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.659 = f32[16,448]{1,0} parameter(3)
  %constant_264_clone_1 = f32[] constant(0)
  %reduce.328.clone.1 = f32[448]{0} reduce(f32[16,448]{1,0} %param_3.659, f32[] %constant_264_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_616_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.391.clone.1 = f32[448]{0} broadcast(f32[] %constant_616_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.305.clone.1 = f32[448]{0} multiply(f32[448]{0} %reduce.328.clone.1, f32[448]{0} %broadcast.391.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.666 = f32[448]{0} parameter(2)
  %multiply.479.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.666, f32[448]{0} %broadcast.391.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.304.clone.1 = f32[448]{0} multiply(f32[448]{0} %multiply.479.clone.1, f32[448]{0} %multiply.479.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.33.clone.1 = f32[448]{0} subtract(f32[448]{0} %multiply.305.clone.1, f32[448]{0} %multiply.304.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.390.clone.1 = f32[448]{0} broadcast(f32[] %constant_264_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.30.clone.1 = f32[448]{0} maximum(f32[448]{0} %subtract.33.clone.1, f32[448]{0} %broadcast.390.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_261 = f32[] constant(0.1)
  %broadcast.388 = f32[448]{0} broadcast(f32[] %constant_261), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.302 = f32[448]{0} multiply(f32[448]{0} %maximum.30.clone.1, f32[448]{0} %broadcast.388), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.132 = f32[448]{0} add(f32[448]{0} %multiply.303, f32[448]{0} %multiply.302), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.131 = f32[448]{0} add(f32[448]{0} %param_0.198, f32[448]{0} %add.132), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  %param_4.500 = f32[448]{0} parameter(4)
  %param_5.307 = f32[448]{0} parameter(5)
  %multiply.309.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_5.307, f32[448]{0} %broadcast.389), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_267_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.393.clone.1 = f32[448]{0} broadcast(f32[] %constant_267_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.308.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.666, f32[448]{0} %broadcast.393.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.134.clone.1 = f32[448]{0} add(f32[448]{0} %multiply.309.clone.1, f32[448]{0} %multiply.308.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.133.clone.1 = f32[448]{0} add(f32[448]{0} %param_4.500, f32[448]{0} %add.134.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  ROOT %tuple.63 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) tuple(f32[448]{0} %add.131, f32[448]{0} %maximum.30.clone.1, f32[448]{0} %add.133.clone.1)
}

%fused_computation.120 (param_0.211: f32[896], param_1.724: f32[896], param_2.674: f32[896], param_3.669: f32[16,896], param_4.510: f32[896], param_5.319: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.211 = f32[896]{0} parameter(0)
  %param_1.724 = f32[896]{0} parameter(1)
  %constant_273 = f32[] constant(0.9)
  %broadcast.401 = f32[896]{0} broadcast(f32[] %constant_273), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.313 = f32[896]{0} multiply(f32[896]{0} %param_1.724, f32[896]{0} %broadcast.401), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.669 = f32[16,896]{1,0} parameter(3)
  %constant_275_clone_1 = f32[] constant(0)
  %reduce.331.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.669, f32[] %constant_275_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_610_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.403.clone.1 = f32[896]{0} broadcast(f32[] %constant_610_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.315.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.331.clone.1, f32[896]{0} %broadcast.403.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.674 = f32[896]{0} parameter(2)
  %multiply.475.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.674, f32[896]{0} %broadcast.403.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.314.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.475.clone.1, f32[896]{0} %multiply.475.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.35.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.315.clone.1, f32[896]{0} %multiply.314.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.402.clone.1 = f32[896]{0} broadcast(f32[] %constant_275_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.32.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.35.clone.1, f32[896]{0} %broadcast.402.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_272 = f32[] constant(0.1)
  %broadcast.400 = f32[896]{0} broadcast(f32[] %constant_272), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.312 = f32[896]{0} multiply(f32[896]{0} %maximum.32.clone.1, f32[896]{0} %broadcast.400), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.139 = f32[896]{0} add(f32[896]{0} %multiply.313, f32[896]{0} %multiply.312), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.138 = f32[896]{0} add(f32[896]{0} %param_0.211, f32[896]{0} %add.139), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  %param_4.510 = f32[896]{0} parameter(4)
  %param_5.319 = f32[896]{0} parameter(5)
  %multiply.319.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.319, f32[896]{0} %broadcast.401), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_278_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.405.clone.1 = f32[896]{0} broadcast(f32[] %constant_278_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.318.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.674, f32[896]{0} %broadcast.405.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.141.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.319.clone.1, f32[896]{0} %multiply.318.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.140.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.510, f32[896]{0} %add.141.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  ROOT %tuple.67 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.138, f32[896]{0} %maximum.32.clone.1, f32[896]{0} %add.140.clone.1)
}

%fused_computation.126 (param_0.223: f32[448], param_1.381: f32[448], param_2.314: f32[448], param_3.308: f32[16,56,56,448], param_4.358: f32[448]) -> f32[16,56,56,448] {
  %param_3.308 = f32[16,56,56,448]{2,1,3,0} parameter(3)
  %param_4.358 = f32[448]{0} parameter(4)
  %constant_601 = f32[] constant(1.99298465e-05)
  %broadcast.582 = f32[448]{0} broadcast(f32[] %constant_601), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.469 = f32[448]{0} multiply(f32[448]{0} %param_4.358, f32[448]{0} %broadcast.582), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.409 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.469), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.36 = f32[16,56,56,448]{2,1,3,0} subtract(f32[16,56,56,448]{2,1,3,0} %param_3.308, f32[16,56,56,448]{2,1,3,0} %broadcast.409), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.314 = f32[448]{0} parameter(2)
  %constant_281 = f32[] constant(1e-05)
  %broadcast.411 = f32[448]{0} broadcast(f32[] %constant_281), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.143 = f32[448]{0} add(f32[448]{0} %param_2.314, f32[448]{0} %broadcast.411), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.46 = f32[448]{0} rsqrt(f32[448]{0} %add.143), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.381 = f32[448]{0} parameter(1)
  %multiply.321 = f32[448]{0} multiply(f32[448]{0} %rsqrt.46, f32[448]{0} %param_1.381), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.408 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.321), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.320 = f32[16,56,56,448]{2,1,3,0} multiply(f32[16,56,56,448]{2,1,3,0} %subtract.36, f32[16,56,56,448]{2,1,3,0} %broadcast.408), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.223 = f32[448]{0} parameter(0)
  %broadcast.407 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %param_0.223), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.142 = f32[16,56,56,448]{2,1,3,0} add(f32[16,56,56,448]{2,1,3,0} %multiply.320, f32[16,56,56,448]{2,1,3,0} %broadcast.407), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_282 = f32[] constant(0)
  %broadcast.410 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[] %constant_282), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  ROOT %maximum.33 = f32[16,56,56,448]{2,1,3,0} maximum(f32[16,56,56,448]{2,1,3,0} %add.142, f32[16,56,56,448]{2,1,3,0} %broadcast.410), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
}

%fused_computation.127 (param_0.224: f32[448], param_1.727: f32[448], param_2.678: f32[448], param_3.674: f32[16,448], param_4.516: f32[448], param_5.326: f32[448]) -> (f32[448], f32[448], f32[448]) {
  %param_0.224 = f32[448]{0} parameter(0)
  %param_1.727 = f32[448]{0} parameter(1)
  %constant_284 = f32[] constant(0.9)
  %broadcast.413 = f32[448]{0} broadcast(f32[] %constant_284), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.323 = f32[448]{0} multiply(f32[448]{0} %param_1.727, f32[448]{0} %broadcast.413), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.674 = f32[16,448]{1,0} parameter(3)
  %constant_286_clone_1 = f32[] constant(0)
  %reduce.334.clone.1 = f32[448]{0} reduce(f32[16,448]{1,0} %param_3.674, f32[] %constant_286_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_604_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.415.clone.1 = f32[448]{0} broadcast(f32[] %constant_604_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.325.clone.1 = f32[448]{0} multiply(f32[448]{0} %reduce.334.clone.1, f32[448]{0} %broadcast.415.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.678 = f32[448]{0} parameter(2)
  %multiply.471.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.678, f32[448]{0} %broadcast.415.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.324.clone.1 = f32[448]{0} multiply(f32[448]{0} %multiply.471.clone.1, f32[448]{0} %multiply.471.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.37.clone.1 = f32[448]{0} subtract(f32[448]{0} %multiply.325.clone.1, f32[448]{0} %multiply.324.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.414.clone.1 = f32[448]{0} broadcast(f32[] %constant_286_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.34.clone.1 = f32[448]{0} maximum(f32[448]{0} %subtract.37.clone.1, f32[448]{0} %broadcast.414.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_283 = f32[] constant(0.1)
  %broadcast.412 = f32[448]{0} broadcast(f32[] %constant_283), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.322 = f32[448]{0} multiply(f32[448]{0} %maximum.34.clone.1, f32[448]{0} %broadcast.412), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.145 = f32[448]{0} add(f32[448]{0} %multiply.323, f32[448]{0} %multiply.322), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.144 = f32[448]{0} add(f32[448]{0} %param_0.224, f32[448]{0} %add.145), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  %param_4.516 = f32[448]{0} parameter(4)
  %param_5.326 = f32[448]{0} parameter(5)
  %multiply.329.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_5.326, f32[448]{0} %broadcast.413), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_289_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.417.clone.1 = f32[448]{0} broadcast(f32[] %constant_289_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.328.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.678, f32[448]{0} %broadcast.417.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.147.clone.1 = f32[448]{0} add(f32[448]{0} %multiply.329.clone.1, f32[448]{0} %multiply.328.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.146.clone.1 = f32[448]{0} add(f32[448]{0} %param_4.516, f32[448]{0} %add.147.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  ROOT %tuple.70 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) tuple(f32[448]{0} %add.144, f32[448]{0} %maximum.34.clone.1, f32[448]{0} %add.146.clone.1)
}

%fused_computation.133 (param_0.236: f32[224], param_1.403: f32[224], param_2.333: f32[224], param_3.327: f32[16,56,56,224], param_4.357: f32[224]) -> f32[16,56,56,224] {
  %param_3.327 = f32[16,56,56,224]{2,1,3,0} parameter(3)
  %param_4.357 = f32[224]{0} parameter(4)
  %constant_595 = f32[] constant(1.99298465e-05)
  %broadcast.578 = f32[224]{0} broadcast(f32[] %constant_595), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.465 = f32[224]{0} multiply(f32[224]{0} %param_4.357, f32[224]{0} %broadcast.578), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.421 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %multiply.465), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.38 = f32[16,56,56,224]{2,1,3,0} subtract(f32[16,56,56,224]{2,1,3,0} %param_3.327, f32[16,56,56,224]{2,1,3,0} %broadcast.421), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.333 = f32[224]{0} parameter(2)
  %constant_292 = f32[] constant(1e-05)
  %broadcast.423 = f32[224]{0} broadcast(f32[] %constant_292), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.149 = f32[224]{0} add(f32[224]{0} %param_2.333, f32[224]{0} %broadcast.423), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.47 = f32[224]{0} rsqrt(f32[224]{0} %add.149), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.403 = f32[224]{0} parameter(1)
  %multiply.331 = f32[224]{0} multiply(f32[224]{0} %rsqrt.47, f32[224]{0} %param_1.403), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.420 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %multiply.331), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.330 = f32[16,56,56,224]{2,1,3,0} multiply(f32[16,56,56,224]{2,1,3,0} %subtract.38, f32[16,56,56,224]{2,1,3,0} %broadcast.420), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.236 = f32[224]{0} parameter(0)
  %broadcast.419 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %param_0.236), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.148 = f32[16,56,56,224]{2,1,3,0} add(f32[16,56,56,224]{2,1,3,0} %multiply.330, f32[16,56,56,224]{2,1,3,0} %broadcast.419), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_293 = f32[] constant(0)
  %broadcast.422 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[] %constant_293), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  ROOT %maximum.35 = f32[16,56,56,224]{2,1,3,0} maximum(f32[16,56,56,224]{2,1,3,0} %add.148, f32[16,56,56,224]{2,1,3,0} %broadcast.422), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
}

%fused_computation.134 (param_0.237: f32[224], param_1.730: f32[224], param_2.682: f32[224], param_3.679: f32[16,224], param_4.522: f32[224], param_5.333: f32[224]) -> (f32[224], f32[224], f32[224]) {
  %param_0.237 = f32[224]{0} parameter(0)
  %param_1.730 = f32[224]{0} parameter(1)
  %constant_295 = f32[] constant(0.9)
  %broadcast.425 = f32[224]{0} broadcast(f32[] %constant_295), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.333 = f32[224]{0} multiply(f32[224]{0} %param_1.730, f32[224]{0} %broadcast.425), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.679 = f32[16,224]{1,0} parameter(3)
  %constant_297_clone_1 = f32[] constant(0)
  %reduce.337.clone.1 = f32[224]{0} reduce(f32[16,224]{1,0} %param_3.679, f32[] %constant_297_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_598_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.426.clone.1 = f32[224]{0} broadcast(f32[] %constant_598_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.335.clone.1 = f32[224]{0} multiply(f32[224]{0} %reduce.337.clone.1, f32[224]{0} %broadcast.426.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.682 = f32[224]{0} parameter(2)
  %multiply.467.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_2.682, f32[224]{0} %broadcast.426.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.334.clone.1 = f32[224]{0} multiply(f32[224]{0} %multiply.467.clone.1, f32[224]{0} %multiply.467.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.39.clone.1 = f32[224]{0} subtract(f32[224]{0} %multiply.335.clone.1, f32[224]{0} %multiply.334.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.427.clone.1 = f32[224]{0} broadcast(f32[] %constant_297_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.36.clone.1 = f32[224]{0} maximum(f32[224]{0} %subtract.39.clone.1, f32[224]{0} %broadcast.427.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_294 = f32[] constant(0.1)
  %broadcast.424 = f32[224]{0} broadcast(f32[] %constant_294), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.332 = f32[224]{0} multiply(f32[224]{0} %maximum.36.clone.1, f32[224]{0} %broadcast.424), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.151 = f32[224]{0} add(f32[224]{0} %multiply.333, f32[224]{0} %multiply.332), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.150 = f32[224]{0} add(f32[224]{0} %param_0.237, f32[224]{0} %add.151), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  %param_4.522 = f32[224]{0} parameter(4)
  %param_5.333 = f32[224]{0} parameter(5)
  %multiply.339.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_5.333, f32[224]{0} %broadcast.425), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_300_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.429.clone.1 = f32[224]{0} broadcast(f32[] %constant_300_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.338.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_2.682, f32[224]{0} %broadcast.429.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.153.clone.1 = f32[224]{0} add(f32[224]{0} %multiply.339.clone.1, f32[224]{0} %multiply.338.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.152.clone.1 = f32[224]{0} add(f32[224]{0} %param_4.522, f32[224]{0} %add.153.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  ROOT %tuple.73 = (f32[224]{0}, f32[224]{0}, f32[224]{0}) tuple(f32[224]{0} %add.150, f32[224]{0} %maximum.36.clone.1, f32[224]{0} %add.152.clone.1)
}

%fused_computation.140 (param_0.442: f32[16,56,56,896], param_1.624: f32[896], param_2.533: f32[896], param_3.541: f32[896], param_4.356: f32[16,56,56,896], param_5.131: f32[896]) -> f32[16,56,56,896] {
  %param_0.442 = f32[16,56,56,896]{2,1,3,0} parameter(0)
  %param_4.356 = f32[16,56,56,896]{2,1,3,0} parameter(4)
  %param_5.131 = f32[896]{0} parameter(5)
  %constant_592 = f32[] constant(1.99298465e-05)
  %broadcast.576 = f32[896]{0} broadcast(f32[] %constant_592), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.463 = f32[896]{0} multiply(f32[896]{0} %param_5.131, f32[896]{0} %broadcast.576), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.575 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.463), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.59 = f32[16,56,56,896]{2,1,3,0} subtract(f32[16,56,56,896]{2,1,3,0} %param_4.356, f32[16,56,56,896]{2,1,3,0} %broadcast.575), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.541 = f32[896]{0} parameter(3)
  %constant_591 = f32[] constant(1e-05)
  %broadcast.574 = f32[896]{0} broadcast(f32[] %constant_591), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.215 = f32[896]{0} add(f32[896]{0} %param_3.541, f32[896]{0} %broadcast.574), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.87 = f32[896]{0} rsqrt(f32[896]{0} %add.215), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_2.533 = f32[896]{0} parameter(2)
  %multiply.462 = f32[896]{0} multiply(f32[896]{0} %rsqrt.87, f32[896]{0} %param_2.533), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.573 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.462), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.461 = f32[16,56,56,896]{2,1,3,0} multiply(f32[16,56,56,896]{2,1,3,0} %subtract.59, f32[16,56,56,896]{2,1,3,0} %broadcast.573), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_1.624 = f32[896]{0} parameter(1)
  %broadcast.572 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %param_1.624), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.214 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %multiply.461, f32[16,56,56,896]{2,1,3,0} %broadcast.572), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.213 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %param_0.442, f32[16,56,56,896]{2,1,3,0} %add.214), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %constant_303 = f32[] constant(0)
  %broadcast.431 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[] %constant_303), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %maximum.37 = f32[16,56,56,896]{2,1,3,0} maximum(f32[16,56,56,896]{2,1,3,0} %add.213, f32[16,56,56,896]{2,1,3,0} %broadcast.431), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_1/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
}

%fused_computation.142 (param_0.249: f32[896], param_1.733: f32[896], param_2.686: f32[896], param_3.684: f32[16,896], param_4.528: f32[896], param_5.340: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.249 = f32[896]{0} parameter(0)
  %param_1.733 = f32[896]{0} parameter(1)
  %constant_306 = f32[] constant(0.9)
  %broadcast.437 = f32[896]{0} broadcast(f32[] %constant_306), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.343 = f32[896]{0} multiply(f32[896]{0} %param_1.733, f32[896]{0} %broadcast.437), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.684 = f32[16,896]{1,0} parameter(3)
  %constant_308_clone_1 = f32[] constant(0)
  %reduce.340.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.684, f32[] %constant_308_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_582_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.439.clone.1 = f32[896]{0} broadcast(f32[] %constant_582_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.345.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.340.clone.1, f32[896]{0} %broadcast.439.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.686 = f32[896]{0} parameter(2)
  %multiply.451.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.686, f32[896]{0} %broadcast.439.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.344.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.451.clone.1, f32[896]{0} %multiply.451.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.41.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.345.clone.1, f32[896]{0} %multiply.344.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.438.clone.1 = f32[896]{0} broadcast(f32[] %constant_308_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.38.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.41.clone.1, f32[896]{0} %broadcast.438.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_305 = f32[] constant(0.1)
  %broadcast.436 = f32[896]{0} broadcast(f32[] %constant_305), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.342 = f32[896]{0} multiply(f32[896]{0} %maximum.38.clone.1, f32[896]{0} %broadcast.436), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.158 = f32[896]{0} add(f32[896]{0} %multiply.343, f32[896]{0} %multiply.342), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.157 = f32[896]{0} add(f32[896]{0} %param_0.249, f32[896]{0} %add.158), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %param_4.528 = f32[896]{0} parameter(4)
  %param_5.340 = f32[896]{0} parameter(5)
  %multiply.349.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.340, f32[896]{0} %broadcast.437), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_311_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.441.clone.1 = f32[896]{0} broadcast(f32[] %constant_311_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.348.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.686, f32[896]{0} %broadcast.441.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.160.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.349.clone.1, f32[896]{0} %multiply.348.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.159.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.528, f32[896]{0} %add.160.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  ROOT %tuple.76 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.157, f32[896]{0} %maximum.38.clone.1, f32[896]{0} %add.159.clone.1)
}

%fused_computation.148 (param_0.261: f32[448], param_1.447: f32[448], param_2.370: f32[448], param_3.363: f32[16,56,56,448], param_4.346: f32[448]) -> f32[16,56,56,448] {
  %param_3.363 = f32[16,56,56,448]{2,1,3,0} parameter(3)
  %param_4.346 = f32[448]{0} parameter(4)
  %constant_573 = f32[] constant(1.99298465e-05)
  %broadcast.550 = f32[448]{0} broadcast(f32[] %constant_573), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.445 = f32[448]{0} multiply(f32[448]{0} %param_4.346, f32[448]{0} %broadcast.550), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.445 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.445), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.42 = f32[16,56,56,448]{2,1,3,0} subtract(f32[16,56,56,448]{2,1,3,0} %param_3.363, f32[16,56,56,448]{2,1,3,0} %broadcast.445), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.370 = f32[448]{0} parameter(2)
  %constant_314 = f32[] constant(1e-05)
  %broadcast.447 = f32[448]{0} broadcast(f32[] %constant_314), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.162 = f32[448]{0} add(f32[448]{0} %param_2.370, f32[448]{0} %broadcast.447), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.49 = f32[448]{0} rsqrt(f32[448]{0} %add.162), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.447 = f32[448]{0} parameter(1)
  %multiply.351 = f32[448]{0} multiply(f32[448]{0} %rsqrt.49, f32[448]{0} %param_1.447), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.444 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.351), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.350 = f32[16,56,56,448]{2,1,3,0} multiply(f32[16,56,56,448]{2,1,3,0} %subtract.42, f32[16,56,56,448]{2,1,3,0} %broadcast.444), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.261 = f32[448]{0} parameter(0)
  %broadcast.443 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %param_0.261), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.161 = f32[16,56,56,448]{2,1,3,0} add(f32[16,56,56,448]{2,1,3,0} %multiply.350, f32[16,56,56,448]{2,1,3,0} %broadcast.443), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_315 = f32[] constant(0)
  %broadcast.446 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[] %constant_315), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  ROOT %maximum.39 = f32[16,56,56,448]{2,1,3,0} maximum(f32[16,56,56,448]{2,1,3,0} %add.161, f32[16,56,56,448]{2,1,3,0} %broadcast.446), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
}

%fused_computation.149 (param_0.262: f32[448], param_1.736: f32[448], param_2.690: f32[448], param_3.689: f32[16,448], param_4.534: f32[448], param_5.347: f32[448]) -> (f32[448], f32[448], f32[448]) {
  %param_0.262 = f32[448]{0} parameter(0)
  %param_1.736 = f32[448]{0} parameter(1)
  %constant_317 = f32[] constant(0.9)
  %broadcast.449 = f32[448]{0} broadcast(f32[] %constant_317), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.353 = f32[448]{0} multiply(f32[448]{0} %param_1.736, f32[448]{0} %broadcast.449), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.689 = f32[16,448]{1,0} parameter(3)
  %constant_319_clone_1 = f32[] constant(0)
  %reduce.343.clone.1 = f32[448]{0} reduce(f32[16,448]{1,0} %param_3.689, f32[] %constant_319_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_576_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.451.clone.1 = f32[448]{0} broadcast(f32[] %constant_576_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.355.clone.1 = f32[448]{0} multiply(f32[448]{0} %reduce.343.clone.1, f32[448]{0} %broadcast.451.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.690 = f32[448]{0} parameter(2)
  %multiply.447.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.690, f32[448]{0} %broadcast.451.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.354.clone.1 = f32[448]{0} multiply(f32[448]{0} %multiply.447.clone.1, f32[448]{0} %multiply.447.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.43.clone.1 = f32[448]{0} subtract(f32[448]{0} %multiply.355.clone.1, f32[448]{0} %multiply.354.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.450.clone.1 = f32[448]{0} broadcast(f32[] %constant_319_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.40.clone.1 = f32[448]{0} maximum(f32[448]{0} %subtract.43.clone.1, f32[448]{0} %broadcast.450.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_316 = f32[] constant(0.1)
  %broadcast.448 = f32[448]{0} broadcast(f32[] %constant_316), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.352 = f32[448]{0} multiply(f32[448]{0} %maximum.40.clone.1, f32[448]{0} %broadcast.448), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.164 = f32[448]{0} add(f32[448]{0} %multiply.353, f32[448]{0} %multiply.352), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.163 = f32[448]{0} add(f32[448]{0} %param_0.262, f32[448]{0} %add.164), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %param_4.534 = f32[448]{0} parameter(4)
  %param_5.347 = f32[448]{0} parameter(5)
  %multiply.359.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_5.347, f32[448]{0} %broadcast.449), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_322_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.453.clone.1 = f32[448]{0} broadcast(f32[] %constant_322_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.358.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.690, f32[448]{0} %broadcast.453.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.166.clone.1 = f32[448]{0} add(f32[448]{0} %multiply.359.clone.1, f32[448]{0} %multiply.358.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.165.clone.1 = f32[448]{0} add(f32[448]{0} %param_4.534, f32[448]{0} %add.166.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  ROOT %tuple.79 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) tuple(f32[448]{0} %add.163, f32[448]{0} %maximum.40.clone.1, f32[448]{0} %add.165.clone.1)
}

%fused_computation.155 (param_0.274: f32[224], param_1.469: f32[224], param_2.389: f32[224], param_3.382: f32[16,56,56,224], param_4.345: f32[224]) -> f32[16,56,56,224] {
  %param_3.382 = f32[16,56,56,224]{2,1,3,0} parameter(3)
  %param_4.345 = f32[224]{0} parameter(4)
  %constant_567 = f32[] constant(1.99298465e-05)
  %broadcast.546 = f32[224]{0} broadcast(f32[] %constant_567), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.441 = f32[224]{0} multiply(f32[224]{0} %param_4.345, f32[224]{0} %broadcast.546), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.457 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %multiply.441), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.44 = f32[16,56,56,224]{2,1,3,0} subtract(f32[16,56,56,224]{2,1,3,0} %param_3.382, f32[16,56,56,224]{2,1,3,0} %broadcast.457), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.389 = f32[224]{0} parameter(2)
  %constant_325 = f32[] constant(1e-05)
  %broadcast.459 = f32[224]{0} broadcast(f32[] %constant_325), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.168 = f32[224]{0} add(f32[224]{0} %param_2.389, f32[224]{0} %broadcast.459), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.50 = f32[224]{0} rsqrt(f32[224]{0} %add.168), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.469 = f32[224]{0} parameter(1)
  %multiply.361 = f32[224]{0} multiply(f32[224]{0} %rsqrt.50, f32[224]{0} %param_1.469), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.456 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %multiply.361), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.360 = f32[16,56,56,224]{2,1,3,0} multiply(f32[16,56,56,224]{2,1,3,0} %subtract.44, f32[16,56,56,224]{2,1,3,0} %broadcast.456), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.274 = f32[224]{0} parameter(0)
  %broadcast.455 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %param_0.274), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.167 = f32[16,56,56,224]{2,1,3,0} add(f32[16,56,56,224]{2,1,3,0} %multiply.360, f32[16,56,56,224]{2,1,3,0} %broadcast.455), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_326 = f32[] constant(0)
  %broadcast.458 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[] %constant_326), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  ROOT %maximum.41 = f32[16,56,56,224]{2,1,3,0} maximum(f32[16,56,56,224]{2,1,3,0} %add.167, f32[16,56,56,224]{2,1,3,0} %broadcast.458), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
}

%fused_computation.156 (param_0.275: f32[224], param_1.739: f32[224], param_2.694: f32[224], param_3.694: f32[16,224], param_4.540: f32[224], param_5.354: f32[224]) -> (f32[224], f32[224], f32[224]) {
  %param_0.275 = f32[224]{0} parameter(0)
  %param_1.739 = f32[224]{0} parameter(1)
  %constant_328 = f32[] constant(0.9)
  %broadcast.461 = f32[224]{0} broadcast(f32[] %constant_328), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.363 = f32[224]{0} multiply(f32[224]{0} %param_1.739, f32[224]{0} %broadcast.461), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.694 = f32[16,224]{1,0} parameter(3)
  %constant_330_clone_1 = f32[] constant(0)
  %reduce.346.clone.1 = f32[224]{0} reduce(f32[16,224]{1,0} %param_3.694, f32[] %constant_330_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_570_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.462.clone.1 = f32[224]{0} broadcast(f32[] %constant_570_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.365.clone.1 = f32[224]{0} multiply(f32[224]{0} %reduce.346.clone.1, f32[224]{0} %broadcast.462.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.694 = f32[224]{0} parameter(2)
  %multiply.443.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_2.694, f32[224]{0} %broadcast.462.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.364.clone.1 = f32[224]{0} multiply(f32[224]{0} %multiply.443.clone.1, f32[224]{0} %multiply.443.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.45.clone.1 = f32[224]{0} subtract(f32[224]{0} %multiply.365.clone.1, f32[224]{0} %multiply.364.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.463.clone.1 = f32[224]{0} broadcast(f32[] %constant_330_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.42.clone.1 = f32[224]{0} maximum(f32[224]{0} %subtract.45.clone.1, f32[224]{0} %broadcast.463.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_327 = f32[] constant(0.1)
  %broadcast.460 = f32[224]{0} broadcast(f32[] %constant_327), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.362 = f32[224]{0} multiply(f32[224]{0} %maximum.42.clone.1, f32[224]{0} %broadcast.460), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.170 = f32[224]{0} add(f32[224]{0} %multiply.363, f32[224]{0} %multiply.362), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.169 = f32[224]{0} add(f32[224]{0} %param_0.275, f32[224]{0} %add.170), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %param_4.540 = f32[224]{0} parameter(4)
  %param_5.354 = f32[224]{0} parameter(5)
  %multiply.369.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_5.354, f32[224]{0} %broadcast.461), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_333_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.465.clone.1 = f32[224]{0} broadcast(f32[] %constant_333_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.368.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_2.694, f32[224]{0} %broadcast.465.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.172.clone.1 = f32[224]{0} add(f32[224]{0} %multiply.369.clone.1, f32[224]{0} %multiply.368.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.171.clone.1 = f32[224]{0} add(f32[224]{0} %param_4.540, f32[224]{0} %add.172.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  ROOT %tuple.82 = (f32[224]{0}, f32[224]{0}, f32[224]{0}) tuple(f32[224]{0} %add.169, f32[224]{0} %maximum.42.clone.1, f32[224]{0} %add.171.clone.1)
}

%fused_computation.162 (param_0.397: f32[896], param_1.601: f32[896], param_2.494: f32[896], param_3.517: f32[16,56,56,896], param_4.344: f32[896], param_5.114: f32[896], param_6.96: f32[896], param_7.34: f32[16,56,56,896], param_8.22: f32[896], param_9.22: f32[896]) -> f32[16,56,56,896] {
  %param_7.34 = f32[16,56,56,896]{2,1,3,0} parameter(7)
  %param_9.22 = f32[896]{0} parameter(9)
  %constant_555 = f32[] constant(1.99298465e-05)
  %broadcast.542 = f32[896]{0} broadcast(f32[] %constant_555), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.437 = f32[896]{0} multiply(f32[896]{0} %param_9.22, f32[896]{0} %broadcast.542), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.474 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.437), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.47 = f32[16,56,56,896]{2,1,3,0} subtract(f32[16,56,56,896]{2,1,3,0} %param_7.34, f32[16,56,56,896]{2,1,3,0} %broadcast.474), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_6.96 = f32[896]{0} parameter(6)
  %constant_336 = f32[] constant(1e-05)
  %broadcast.473 = f32[896]{0} broadcast(f32[] %constant_336), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.177 = f32[896]{0} add(f32[896]{0} %param_6.96, f32[896]{0} %broadcast.473), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.52 = f32[896]{0} rsqrt(f32[896]{0} %add.177), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_5.114 = f32[896]{0} parameter(5)
  %multiply.373 = f32[896]{0} multiply(f32[896]{0} %rsqrt.52, f32[896]{0} %param_5.114), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.472 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.373), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.372 = f32[16,56,56,896]{2,1,3,0} multiply(f32[16,56,56,896]{2,1,3,0} %subtract.47, f32[16,56,56,896]{2,1,3,0} %broadcast.472), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_4.344 = f32[896]{0} parameter(4)
  %broadcast.471 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %param_4.344), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.176 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %multiply.372, f32[16,56,56,896]{2,1,3,0} %broadcast.471), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %param_3.517 = f32[16,56,56,896]{2,1,3,0} parameter(3)
  %param_8.22 = f32[896]{0} parameter(8)
  %multiply.433 = f32[896]{0} multiply(f32[896]{0} %param_8.22, f32[896]{0} %broadcast.542), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.470 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.433), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.46 = f32[16,56,56,896]{2,1,3,0} subtract(f32[16,56,56,896]{2,1,3,0} %param_3.517, f32[16,56,56,896]{2,1,3,0} %broadcast.470), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.494 = f32[896]{0} parameter(2)
  %add.175 = f32[896]{0} add(f32[896]{0} %param_2.494, f32[896]{0} %broadcast.473), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.51 = f32[896]{0} rsqrt(f32[896]{0} %add.175), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.601 = f32[896]{0} parameter(1)
  %multiply.371 = f32[896]{0} multiply(f32[896]{0} %rsqrt.51, f32[896]{0} %param_1.601), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.469 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.371), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.370 = f32[16,56,56,896]{2,1,3,0} multiply(f32[16,56,56,896]{2,1,3,0} %subtract.46, f32[16,56,56,896]{2,1,3,0} %broadcast.469), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.397 = f32[896]{0} parameter(0)
  %broadcast.468 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %param_0.397), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.174 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %multiply.370, f32[16,56,56,896]{2,1,3,0} %broadcast.468), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.173 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %add.176, f32[16,56,56,896]{2,1,3,0} %add.174), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %constant_337 = f32[] constant(0)
  %broadcast.467 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[] %constant_337), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %maximum.43 = f32[16,56,56,896]{2,1,3,0} maximum(f32[16,56,56,896]{2,1,3,0} %add.173, f32[16,56,56,896]{2,1,3,0} %broadcast.467), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
}

%fused_computation.163 (param_0.287: f32[896], param_1.742: f32[896], param_2.698: f32[896], param_3.699: f32[16,896], param_4.546: f32[896], param_5.361: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.287 = f32[896]{0} parameter(0)
  %param_1.742 = f32[896]{0} parameter(1)
  %constant_339 = f32[] constant(0.9)
  %broadcast.476 = f32[896]{0} broadcast(f32[] %constant_339), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.375 = f32[896]{0} multiply(f32[896]{0} %param_1.742, f32[896]{0} %broadcast.476), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.699 = f32[16,896]{1,0} parameter(3)
  %constant_368_clone_1 = f32[] constant(0)
  %reduce.349.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.699, f32[] %constant_368_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_564_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.478.clone.1 = f32[896]{0} broadcast(f32[] %constant_564_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.377.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.349.clone.1, f32[896]{0} %broadcast.478.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.698 = f32[896]{0} parameter(2)
  %multiply.439.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.698, f32[896]{0} %broadcast.478.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.376.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.439.clone.1, f32[896]{0} %multiply.439.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.48.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.377.clone.1, f32[896]{0} %multiply.376.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.477.clone.1 = f32[896]{0} broadcast(f32[] %constant_368_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.44.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.48.clone.1, f32[896]{0} %broadcast.477.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_338 = f32[] constant(0.1)
  %broadcast.475 = f32[896]{0} broadcast(f32[] %constant_338), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.374 = f32[896]{0} multiply(f32[896]{0} %maximum.44.clone.1, f32[896]{0} %broadcast.475), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.179 = f32[896]{0} add(f32[896]{0} %multiply.375, f32[896]{0} %multiply.374), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.178 = f32[896]{0} add(f32[896]{0} %param_0.287, f32[896]{0} %add.179), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %param_4.546 = f32[896]{0} parameter(4)
  %param_5.361 = f32[896]{0} parameter(5)
  %multiply.381.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.361, f32[896]{0} %broadcast.476), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_374_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.480.clone.1 = f32[896]{0} broadcast(f32[] %constant_374_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.380.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.698, f32[896]{0} %broadcast.480.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.181.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.381.clone.1, f32[896]{0} %multiply.380.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.180.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.546, f32[896]{0} %add.181.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  ROOT %tuple.85 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.178, f32[896]{0} %maximum.44.clone.1, f32[896]{0} %add.180.clone.1)
}

%fused_computation.169 (param_0.297: f32[896], param_1.745: f32[896], param_2.702: f32[896], param_3.704: f32[16,896], param_4.552: f32[896], param_5.368: f32[896]) -> (f32[896], f32[896], f32[896]) {
  %param_0.297 = f32[896]{0} parameter(0)
  %param_1.745 = f32[896]{0} parameter(1)
  %constant_382 = f32[] constant(0.9)
  %broadcast.483 = f32[896]{0} broadcast(f32[] %constant_382), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.383 = f32[896]{0} multiply(f32[896]{0} %param_1.745, f32[896]{0} %broadcast.483), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.704 = f32[16,896]{1,0} parameter(3)
  %constant_386_clone_1 = f32[] constant(0)
  %reduce.352.clone.1 = f32[896]{0} reduce(f32[16,896]{1,0} %param_3.704, f32[] %constant_386_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_558_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.485.clone.1 = f32[896]{0} broadcast(f32[] %constant_558_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.385.clone.1 = f32[896]{0} multiply(f32[896]{0} %reduce.352.clone.1, f32[896]{0} %broadcast.485.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.702 = f32[896]{0} parameter(2)
  %multiply.435.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.702, f32[896]{0} %broadcast.485.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.384.clone.1 = f32[896]{0} multiply(f32[896]{0} %multiply.435.clone.1, f32[896]{0} %multiply.435.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.49.clone.1 = f32[896]{0} subtract(f32[896]{0} %multiply.385.clone.1, f32[896]{0} %multiply.384.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.484.clone.1 = f32[896]{0} broadcast(f32[] %constant_386_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.45.clone.1 = f32[896]{0} maximum(f32[896]{0} %subtract.49.clone.1, f32[896]{0} %broadcast.484.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_380 = f32[] constant(0.1)
  %broadcast.482 = f32[896]{0} broadcast(f32[] %constant_380), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.382 = f32[896]{0} multiply(f32[896]{0} %maximum.45.clone.1, f32[896]{0} %broadcast.482), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.183 = f32[896]{0} add(f32[896]{0} %multiply.383, f32[896]{0} %multiply.382), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.182 = f32[896]{0} add(f32[896]{0} %param_0.297, f32[896]{0} %add.183), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %param_4.552 = f32[896]{0} parameter(4)
  %param_5.368 = f32[896]{0} parameter(5)
  %multiply.389.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.368, f32[896]{0} %broadcast.483), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_392_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.487.clone.1 = f32[896]{0} broadcast(f32[] %constant_392_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.388.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_2.702, f32[896]{0} %broadcast.487.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.185.clone.1 = f32[896]{0} add(f32[896]{0} %multiply.389.clone.1, f32[896]{0} %multiply.388.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.184.clone.1 = f32[896]{0} add(f32[896]{0} %param_4.552, f32[896]{0} %add.185.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  ROOT %tuple.88 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) tuple(f32[896]{0} %add.182, f32[896]{0} %maximum.45.clone.1, f32[896]{0} %add.184.clone.1)
}

%fused_computation.176 (param_0.308: f32[448], param_1.754: f32[448], param_2.711: f32[448], param_3.713: f32[16,448], param_4.563: f32[448], param_5.377: f32[448]) -> (f32[448], f32[448], f32[448]) {
  %param_0.308 = f32[448]{0} parameter(0)
  %param_1.754 = f32[448]{0} parameter(1)
  %constant_404 = f32[] constant(0.9)
  %broadcast.495 = f32[448]{0} broadcast(f32[] %constant_404), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.393 = f32[448]{0} multiply(f32[448]{0} %param_1.754, f32[448]{0} %broadcast.495), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.713 = f32[16,448]{1,0} parameter(3)
  %constant_408_clone_1 = f32[] constant(0)
  %reduce.355.clone.1 = f32[448]{0} reduce(f32[16,448]{1,0} %param_3.713, f32[] %constant_408_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_552_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.497.clone.1 = f32[448]{0} broadcast(f32[] %constant_552_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.395.clone.1 = f32[448]{0} multiply(f32[448]{0} %reduce.355.clone.1, f32[448]{0} %broadcast.497.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.711 = f32[448]{0} parameter(2)
  %multiply.431.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.711, f32[448]{0} %broadcast.497.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.394.clone.1 = f32[448]{0} multiply(f32[448]{0} %multiply.431.clone.1, f32[448]{0} %multiply.431.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.51.clone.1 = f32[448]{0} subtract(f32[448]{0} %multiply.395.clone.1, f32[448]{0} %multiply.394.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.496.clone.1 = f32[448]{0} broadcast(f32[] %constant_408_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.47.clone.1 = f32[448]{0} maximum(f32[448]{0} %subtract.51.clone.1, f32[448]{0} %broadcast.496.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_402 = f32[] constant(0.1)
  %broadcast.494 = f32[448]{0} broadcast(f32[] %constant_402), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.392 = f32[448]{0} multiply(f32[448]{0} %maximum.47.clone.1, f32[448]{0} %broadcast.494), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.189 = f32[448]{0} add(f32[448]{0} %multiply.393, f32[448]{0} %multiply.392), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.188 = f32[448]{0} add(f32[448]{0} %param_0.308, f32[448]{0} %add.189), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  %param_4.563 = f32[448]{0} parameter(4)
  %param_5.377 = f32[448]{0} parameter(5)
  %multiply.399.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_5.377, f32[448]{0} %broadcast.495), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_414_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.499.clone.1 = f32[448]{0} broadcast(f32[] %constant_414_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.398.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_2.711, f32[448]{0} %broadcast.499.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.191.clone.1 = f32[448]{0} add(f32[448]{0} %multiply.399.clone.1, f32[448]{0} %multiply.398.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.190.clone.1 = f32[448]{0} add(f32[448]{0} %param_4.563, f32[448]{0} %add.191.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  ROOT %tuple.92 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) tuple(f32[448]{0} %add.188, f32[448]{0} %maximum.47.clone.1, f32[448]{0} %add.190.clone.1)
}

%fused_computation.182 (param_0.407: f32[224], param_1.605: f32[224], param_2.502: f32[224], param_3.525: f32[16,56,56,224], param_4.342: f32[224]) -> f32[16,56,56,224] {
  %param_3.525 = f32[16,56,56,224]{2,1,3,0} parameter(3)
  %param_4.342 = f32[224]{0} parameter(4)
  %constant_543 = f32[] constant(1.99298465e-05)
  %broadcast.530 = f32[224]{0} broadcast(f32[] %constant_543), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.425 = f32[224]{0} multiply(f32[224]{0} %param_4.342, f32[224]{0} %broadcast.530), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.504 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %multiply.425), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.52 = f32[16,56,56,224]{2,1,3,0} subtract(f32[16,56,56,224]{2,1,3,0} %param_3.525, f32[16,56,56,224]{2,1,3,0} %broadcast.504), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.502 = f32[224]{0} parameter(2)
  %constant_420 = f32[] constant(1e-05)
  %broadcast.505 = f32[224]{0} broadcast(f32[] %constant_420), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.193 = f32[224]{0} add(f32[224]{0} %param_2.502, f32[224]{0} %broadcast.505), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.54 = f32[224]{0} rsqrt(f32[224]{0} %add.193), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.605 = f32[224]{0} parameter(1)
  %multiply.401 = f32[224]{0} multiply(f32[224]{0} %rsqrt.54, f32[224]{0} %param_1.605), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.503 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %multiply.401), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.400 = f32[16,56,56,224]{2,1,3,0} multiply(f32[16,56,56,224]{2,1,3,0} %subtract.52, f32[16,56,56,224]{2,1,3,0} %broadcast.503), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.407 = f32[224]{0} parameter(0)
  %broadcast.502 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[224]{0} %param_0.407), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.192 = f32[16,56,56,224]{2,1,3,0} add(f32[16,56,56,224]{2,1,3,0} %multiply.400, f32[16,56,56,224]{2,1,3,0} %broadcast.502), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_422 = f32[] constant(0)
  %broadcast.501 = f32[16,56,56,224]{2,1,3,0} broadcast(f32[] %constant_422), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  ROOT %maximum.48 = f32[16,56,56,224]{2,1,3,0} maximum(f32[16,56,56,224]{2,1,3,0} %add.192, f32[16,56,56,224]{2,1,3,0} %broadcast.501), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
}

%fused_computation.183 (param_0.319: f32[224], param_1.757: f32[224], param_2.715: f32[224], param_3.718: f32[16,224], param_4.569: f32[224], param_5.384: f32[224]) -> (f32[224], f32[224], f32[224]) {
  %param_0.319 = f32[224]{0} parameter(0)
  %param_1.757 = f32[224]{0} parameter(1)
  %constant_426 = f32[] constant(0.9)
  %broadcast.507 = f32[224]{0} broadcast(f32[] %constant_426), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.403 = f32[224]{0} multiply(f32[224]{0} %param_1.757, f32[224]{0} %broadcast.507), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.718 = f32[16,224]{1,0} parameter(3)
  %constant_430_clone_1 = f32[] constant(0)
  %reduce.358.clone.1 = f32[224]{0} reduce(f32[16,224]{1,0} %param_3.718, f32[] %constant_430_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_546_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.508.clone.1 = f32[224]{0} broadcast(f32[] %constant_546_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.405.clone.1 = f32[224]{0} multiply(f32[224]{0} %reduce.358.clone.1, f32[224]{0} %broadcast.508.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.715 = f32[224]{0} parameter(2)
  %multiply.427.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_2.715, f32[224]{0} %broadcast.508.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.404.clone.1 = f32[224]{0} multiply(f32[224]{0} %multiply.427.clone.1, f32[224]{0} %multiply.427.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.53.clone.1 = f32[224]{0} subtract(f32[224]{0} %multiply.405.clone.1, f32[224]{0} %multiply.404.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.509.clone.1 = f32[224]{0} broadcast(f32[] %constant_430_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.49.clone.1 = f32[224]{0} maximum(f32[224]{0} %subtract.53.clone.1, f32[224]{0} %broadcast.509.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_424 = f32[] constant(0.1)
  %broadcast.506 = f32[224]{0} broadcast(f32[] %constant_424), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.402 = f32[224]{0} multiply(f32[224]{0} %maximum.49.clone.1, f32[224]{0} %broadcast.506), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.195 = f32[224]{0} add(f32[224]{0} %multiply.403, f32[224]{0} %multiply.402), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.194 = f32[224]{0} add(f32[224]{0} %param_0.319, f32[224]{0} %add.195), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  %param_4.569 = f32[224]{0} parameter(4)
  %param_5.384 = f32[224]{0} parameter(5)
  %multiply.409.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_5.384, f32[224]{0} %broadcast.507), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_436_clone_1 = f32[] constant(1.99298461e-06)
  %broadcast.511.clone.1 = f32[224]{0} broadcast(f32[] %constant_436_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.408.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_2.715, f32[224]{0} %broadcast.511.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.197.clone.1 = f32[224]{0} add(f32[224]{0} %multiply.409.clone.1, f32[224]{0} %multiply.408.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.196.clone.1 = f32[224]{0} add(f32[224]{0} %param_4.569, f32[224]{0} %add.197.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  ROOT %tuple.95 = (f32[224]{0}, f32[224]{0}, f32[224]{0}) tuple(f32[224]{0} %add.194, f32[224]{0} %maximum.49.clone.1, f32[224]{0} %add.196.clone.1)
}

%fused_computation.190 (param_0.333: f32[224], param_1.767: f32[224], param_2.724: f32[224], param_3.727: f32[16,224], param_4.580: f32[224], param_5.393: f32[224]) -> (f32[224], f32[224], f32[224]) {
  %param_0.333 = f32[224]{0} parameter(0)
  %param_1.767 = f32[224]{0} parameter(1)
  %constant_444 = f32[] constant(0.9)
  %broadcast.519 = f32[224]{0} broadcast(f32[] %constant_444), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.413 = f32[224]{0} multiply(f32[224]{0} %param_1.767, f32[224]{0} %broadcast.519), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %param_3.727 = f32[16,224]{1,0} parameter(3)
  %constant_446_clone_1 = f32[] constant(0)
  %reduce.361.clone.1 = f32[224]{0} reduce(f32[16,224]{1,0} %param_3.727, f32[] %constant_446_clone_1), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %constant_540_clone_1 = f32[] constant(4.98246163e-06)
  %broadcast.521.clone.1 = f32[224]{0} broadcast(f32[] %constant_540_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.415.clone.1 = f32[224]{0} multiply(f32[224]{0} %reduce.361.clone.1, f32[224]{0} %broadcast.521.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=83}
  %param_2.724 = f32[224]{0} parameter(2)
  %multiply.423.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_2.724, f32[224]{0} %broadcast.521.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.414.clone.1 = f32[224]{0} multiply(f32[224]{0} %multiply.423.clone.1, f32[224]{0} %multiply.423.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %subtract.55.clone.1 = f32[224]{0} subtract(f32[224]{0} %multiply.415.clone.1, f32[224]{0} %multiply.414.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %broadcast.520.clone.1 = f32[224]{0} broadcast(f32[] %constant_446_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %maximum.51.clone.1 = f32[224]{0} maximum(f32[224]{0} %subtract.55.clone.1, f32[224]{0} %broadcast.520.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/max" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=93}
  %constant_443 = f32[] constant(0.1)
  %broadcast.518 = f32[224]{0} broadcast(f32[] %constant_443), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.412 = f32[224]{0} multiply(f32[224]{0} %maximum.51.clone.1, f32[224]{0} %broadcast.518), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.201 = f32[224]{0} add(f32[224]{0} %multiply.413, f32[224]{0} %multiply.412), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=267}
  %add.200 = f32[224]{0} add(f32[224]{0} %param_0.333, f32[224]{0} %add.201), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  %param_4.580 = f32[224]{0} parameter(4)
  %param_5.393 = f32[224]{0} parameter(5)
  %multiply.419.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_5.393, f32[224]{0} %broadcast.519), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %constant_87_clone_1 = f32[] constant(4.98246152e-07)
  %broadcast.523.clone.1 = f32[224]{0} broadcast(f32[] %constant_87_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %multiply.418.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_2.724, f32[224]{0} %broadcast.523.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.203.clone.1 = f32[224]{0} add(f32[224]{0} %multiply.419.clone.1, f32[224]{0} %multiply.418.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=265}
  %add.202.clone.1 = f32[224]{0} add(f32[224]{0} %param_4.580, f32[224]{0} %add.203.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  ROOT %tuple.99 = (f32[224]{0}, f32[224]{0}, f32[224]{0}) tuple(f32[224]{0} %add.200, f32[224]{0} %maximum.51.clone.1, f32[224]{0} %add.202.clone.1)
}

%fused_computation.196 (param_0.343: s32[16,224,224,3]) -> f32[16,224,224,3] {
  %param_0.343 = s32[16,224,224,3]{3,2,1,0} parameter(0)
  %convert.1 = f32[16,224,224,3]{3,2,1,0} convert(s32[16,224,224,3]{3,2,1,0} %param_0.343), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/conv_init/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/dtypes.py" source_line=97}
  ROOT %copy.40 = f32[16,224,224,3]{2,1,3,0} copy(f32[16,224,224,3]{3,2,1,0} %convert.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/conv_init/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/dtypes.py" source_line=97}
}

%fused_computation.197 (param_0.440: f32[16,56,56,896], param_1.620: f32[896], param_2.528: f32[896], param_3.535: f32[896], param_4.351: f32[16,56,56,896], param_5.125: f32[896]) -> f32[16,56,56,896] {
  %param_0.440 = f32[16,56,56,896]{2,1,3,0} parameter(0)
  %param_4.351 = f32[16,56,56,896]{2,1,3,0} parameter(4)
  %param_5.125 = f32[896]{0} parameter(5)
  %constant_587 = f32[] constant(1.99298465e-05)
  %broadcast.566 = f32[896]{0} broadcast(f32[] %constant_587), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.457 = f32[896]{0} multiply(f32[896]{0} %param_5.125, f32[896]{0} %broadcast.566), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.565 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.457), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.57 = f32[16,56,56,896]{2,1,3,0} subtract(f32[16,56,56,896]{2,1,3,0} %param_4.351, f32[16,56,56,896]{2,1,3,0} %broadcast.565), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.535 = f32[896]{0} parameter(3)
  %constant_586 = f32[] constant(1e-05)
  %broadcast.564 = f32[896]{0} broadcast(f32[] %constant_586), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.209 = f32[896]{0} add(f32[896]{0} %param_3.535, f32[896]{0} %broadcast.564), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.85 = f32[896]{0} rsqrt(f32[896]{0} %add.209), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_2.528 = f32[896]{0} parameter(2)
  %multiply.456 = f32[896]{0} multiply(f32[896]{0} %rsqrt.85, f32[896]{0} %param_2.528), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.563 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.456), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.455 = f32[16,56,56,896]{2,1,3,0} multiply(f32[16,56,56,896]{2,1,3,0} %subtract.57, f32[16,56,56,896]{2,1,3,0} %broadcast.563), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_1.620 = f32[896]{0} parameter(1)
  %broadcast.562 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %param_1.620), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.208 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %multiply.455, f32[16,56,56,896]{2,1,3,0} %broadcast.562), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.207 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %param_0.440, f32[16,56,56,896]{2,1,3,0} %add.208), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %copy.41 = f32[16,56,56,896]{3,2,1,0} copy(f32[16,56,56,896]{2,1,3,0} %add.207), metadata={op_name="tuple.66"}
}

%fused_computation.198 (param_0.467: f32[1792], param_1.636: f32[1792], param_2.553: f32[1792], param_3.549: f32[16,28,28,1792], param_4.368: f32[1792], param_5.146: f32[1792], param_6.113: f32[1792], param_7.43: f32[1792], param_8.31: f32[16,28,28,1792], param_9.32: f32[1792]) -> f32[16,28,28,1792] {
  %param_8.31 = f32[16,28,28,1792]{2,1,3,0} parameter(8)
  %param_9.32 = f32[1792]{0} parameter(9)
  %constant_639 = f32[] constant(7.97193861e-05)
  %broadcast.622 = f32[1792]{0} broadcast(f32[] %constant_639), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.503 = f32[1792]{0} multiply(f32[1792]{0} %param_9.32, f32[1792]{0} %broadcast.622), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.621 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.503), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.63 = f32[16,28,28,1792]{2,1,3,0} subtract(f32[16,28,28,1792]{2,1,3,0} %param_8.31, f32[16,28,28,1792]{2,1,3,0} %broadcast.621), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_7.43 = f32[1792]{0} parameter(7)
  %constant_640 = f32[] constant(1e-05)
  %broadcast.620 = f32[1792]{0} broadcast(f32[] %constant_640), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.225 = f32[1792]{0} add(f32[1792]{0} %param_7.43, f32[1792]{0} %broadcast.620), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.91 = f32[1792]{0} rsqrt(f32[1792]{0} %add.225), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_6.113 = f32[1792]{0} parameter(6)
  %multiply.502 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.91, f32[1792]{0} %param_6.113), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.619 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.502), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.501 = f32[16,28,28,1792]{2,1,3,0} multiply(f32[16,28,28,1792]{2,1,3,0} %subtract.63, f32[16,28,28,1792]{2,1,3,0} %broadcast.619), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_5.146 = f32[1792]{0} parameter(5)
  %broadcast.618 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_5.146), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.224 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %multiply.501, f32[16,28,28,1792]{2,1,3,0} %broadcast.618), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %param_3.549 = f32[16,28,28,1792]{2,1,3,0} parameter(3)
  %param_4.368 = f32[1792]{0} parameter(4)
  %multiply.500 = f32[1792]{0} multiply(f32[1792]{0} %param_4.368, f32[1792]{0} %broadcast.622), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.616 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.500), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.62 = f32[16,28,28,1792]{2,1,3,0} subtract(f32[16,28,28,1792]{2,1,3,0} %param_3.549, f32[16,28,28,1792]{2,1,3,0} %broadcast.616), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.553 = f32[1792]{0} parameter(2)
  %add.223 = f32[1792]{0} add(f32[1792]{0} %param_2.553, f32[1792]{0} %broadcast.620), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.90 = f32[1792]{0} rsqrt(f32[1792]{0} %add.223), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.636 = f32[1792]{0} parameter(1)
  %multiply.499 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.90, f32[1792]{0} %param_1.636), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.615 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.499), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.498 = f32[16,28,28,1792]{2,1,3,0} multiply(f32[16,28,28,1792]{2,1,3,0} %subtract.62, f32[16,28,28,1792]{2,1,3,0} %broadcast.615), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.467 = f32[1792]{0} parameter(0)
  %broadcast.614 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_0.467), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.222 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %multiply.498, f32[16,28,28,1792]{2,1,3,0} %broadcast.614), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.221 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %add.224, f32[16,28,28,1792]{2,1,3,0} %add.222), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %copy.42 = f32[16,28,28,1792]{3,2,1,0} copy(f32[16,28,28,1792]{2,1,3,0} %add.221), metadata={op_name="tuple.66"}
}

%fused_computation.199 (param_0.482: f32[16,28,28,1792], param_1.648: f32[1792], param_2.570: f32[1792], param_3.563: f32[1792], param_4.382: f32[16,28,28,1792], param_5.162: f32[1792]) -> f32[16,28,28,1792] {
  %param_0.482 = f32[16,28,28,1792]{2,1,3,0} parameter(0)
  %param_4.382 = f32[16,28,28,1792]{2,1,3,0} parameter(4)
  %param_5.162 = f32[1792]{0} parameter(5)
  %constant_671 = f32[] constant(7.97193861e-05)
  %broadcast.662 = f32[1792]{0} broadcast(f32[] %constant_671), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.533 = f32[1792]{0} multiply(f32[1792]{0} %param_5.162, f32[1792]{0} %broadcast.662), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.661 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.533), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.69 = f32[16,28,28,1792]{2,1,3,0} subtract(f32[16,28,28,1792]{2,1,3,0} %param_4.382, f32[16,28,28,1792]{2,1,3,0} %broadcast.661), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.563 = f32[1792]{0} parameter(3)
  %constant_670 = f32[] constant(1e-05)
  %broadcast.660 = f32[1792]{0} broadcast(f32[] %constant_670), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.241 = f32[1792]{0} add(f32[1792]{0} %param_3.563, f32[1792]{0} %broadcast.660), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.97 = f32[1792]{0} rsqrt(f32[1792]{0} %add.241), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_2.570 = f32[1792]{0} parameter(2)
  %multiply.532 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.97, f32[1792]{0} %param_2.570), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.659 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.532), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.531 = f32[16,28,28,1792]{2,1,3,0} multiply(f32[16,28,28,1792]{2,1,3,0} %subtract.69, f32[16,28,28,1792]{2,1,3,0} %broadcast.659), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_1.648 = f32[1792]{0} parameter(1)
  %broadcast.658 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_1.648), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.240 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %multiply.531, f32[16,28,28,1792]{2,1,3,0} %broadcast.658), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.239 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %param_0.482, f32[16,28,28,1792]{2,1,3,0} %add.240), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  ROOT %copy.43 = f32[16,28,28,1792]{3,2,1,0} copy(f32[16,28,28,1792]{2,1,3,0} %add.239), metadata={op_name="tuple.66"}
}

%fused_computation.200 (param_0.517: f32[16,28,28,1792], param_1.679: f32[1792], param_2.615: f32[1792], param_3.594: f32[1792], param_4.422: f32[16,28,28,1792], param_5.216: f32[1792]) -> (f32[16,28,28,1792], f32[16,28,28,1792], f32[16,28,28,1792]) {
  %param_0.517 = f32[16,28,28,1792]{2,1,3,0} parameter(0)
  %param_4.422 = f32[16,28,28,1792]{2,1,3,0} parameter(4)
  %param_5.216 = f32[1792]{0} parameter(5)
  %constant_709_clone_1 = f32[] constant(7.97193861e-05)
  %broadcast.694.clone.1 = f32[1792]{0} broadcast(f32[] %constant_709_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.561.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %param_5.216, f32[1792]{0} %broadcast.694.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.243.clone.1 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.561.clone.1), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.8.clone.1 = f32[16,28,28,1792]{2,1,3,0} subtract(f32[16,28,28,1792]{2,1,3,0} %param_4.422, f32[16,28,28,1792]{2,1,3,0} %broadcast.243.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.594 = f32[1792]{0} parameter(3)
  %constant_129_clone_1 = f32[] constant(1e-05)
  %broadcast.245.clone.1 = f32[1792]{0} broadcast(f32[] %constant_129_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.55.clone.1 = f32[1792]{0} add(f32[1792]{0} %param_3.594, f32[1792]{0} %broadcast.245.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.32.clone.1 = f32[1792]{0} rsqrt(f32[1792]{0} %add.55.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_2.615 = f32[1792]{0} parameter(2)
  %multiply.181.clone.1 = f32[1792]{0} multiply(f32[1792]{0} %rsqrt.32.clone.1, f32[1792]{0} %param_2.615), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.242.clone.1 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %multiply.181.clone.1), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.180.clone.1 = f32[16,28,28,1792]{2,1,3,0} multiply(f32[16,28,28,1792]{2,1,3,0} %subtract.8.clone.1, f32[16,28,28,1792]{2,1,3,0} %broadcast.242.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_1.679 = f32[1792]{0} parameter(1)
  %broadcast.241.clone.1 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[1792]{0} %param_1.679), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.54.clone.1 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %multiply.180.clone.1, f32[16,28,28,1792]{2,1,3,0} %broadcast.241.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.53.clone.1 = f32[16,28,28,1792]{2,1,3,0} add(f32[16,28,28,1792]{2,1,3,0} %param_0.517, f32[16,28,28,1792]{2,1,3,0} %add.54.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %constant_130_clone_1 = f32[] constant(0)
  %broadcast.244.clone.1 = f32[16,28,28,1792]{2,1,3,0} broadcast(f32[] %constant_130_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %maximum.6.clone.1 = f32[16,28,28,1792]{2,1,3,0} maximum(f32[16,28,28,1792]{2,1,3,0} %add.53.clone.1, f32[16,28,28,1792]{2,1,3,0} %broadcast.244.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %copy.44 = f32[16,28,28,1792]{3,2,1,0} copy(f32[16,28,28,1792]{2,1,3,0} %maximum.6.clone.1), metadata={op_name="tuple.66"}
  %copy.45 = f32[16,28,28,1792]{3,2,1,0} copy(f32[16,28,28,1792]{2,1,3,0} %param_0.517), metadata={op_name="tuple.66"}
  ROOT %tuple.23 = (f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{2,1,3,0}, f32[16,28,28,1792]{3,2,1,0}) tuple(f32[16,28,28,1792]{3,2,1,0} %copy.44, f32[16,28,28,1792]{2,1,3,0} %maximum.6.clone.1, f32[16,28,28,1792]{3,2,1,0} %copy.45)
}

%fused_computation.201 (param_0.520: f32[16,56,56,896], param_1.723: f32[896], param_2.671: f32[896], param_3.664: f32[896], param_4.504: f32[16,56,56,896], param_5.312: f32[896]) -> (f32[16,56,56,896], f32[16,56,56,896]) {
  %param_0.520 = f32[16,56,56,896]{2,1,3,0} parameter(0)
  %param_4.504 = f32[16,56,56,896]{2,1,3,0} parameter(4)
  %param_5.312 = f32[896]{0} parameter(5)
  %constant_607_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.586.clone.1 = f32[896]{0} broadcast(f32[] %constant_607_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.473.clone.1 = f32[896]{0} multiply(f32[896]{0} %param_5.312, f32[896]{0} %broadcast.586.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.397.clone.1 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.473.clone.1), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.34.clone.1 = f32[16,56,56,896]{2,1,3,0} subtract(f32[16,56,56,896]{2,1,3,0} %param_4.504, f32[16,56,56,896]{2,1,3,0} %broadcast.397.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_3.664 = f32[896]{0} parameter(3)
  %constant_270_clone_1 = f32[] constant(1e-05)
  %broadcast.399.clone.1 = f32[896]{0} broadcast(f32[] %constant_270_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.137.clone.1 = f32[896]{0} add(f32[896]{0} %param_3.664, f32[896]{0} %broadcast.399.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.45.clone.1 = f32[896]{0} rsqrt(f32[896]{0} %add.137.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_2.671 = f32[896]{0} parameter(2)
  %multiply.311.clone.1 = f32[896]{0} multiply(f32[896]{0} %rsqrt.45.clone.1, f32[896]{0} %param_2.671), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.396.clone.1 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %multiply.311.clone.1), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.310.clone.1 = f32[16,56,56,896]{2,1,3,0} multiply(f32[16,56,56,896]{2,1,3,0} %subtract.34.clone.1, f32[16,56,56,896]{2,1,3,0} %broadcast.396.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_1.723 = f32[896]{0} parameter(1)
  %broadcast.395.clone.1 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[896]{0} %param_1.723), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.136.clone.1 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %multiply.310.clone.1, f32[16,56,56,896]{2,1,3,0} %broadcast.395.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.135.clone.1 = f32[16,56,56,896]{2,1,3,0} add(f32[16,56,56,896]{2,1,3,0} %param_0.520, f32[16,56,56,896]{2,1,3,0} %add.136.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/add" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %constant_271_clone_1 = f32[] constant(0)
  %broadcast.398.clone.1 = f32[16,56,56,896]{2,1,3,0} broadcast(f32[] %constant_271_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %maximum.31.clone.1 = f32[16,56,56,896]{2,1,3,0} maximum(f32[16,56,56,896]{2,1,3,0} %add.135.clone.1, f32[16,56,56,896]{2,1,3,0} %broadcast.398.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %copy.46 = f32[16,56,56,896]{3,2,1,0} copy(f32[16,56,56,896]{2,1,3,0} %maximum.31.clone.1), metadata={op_name="tuple.66"}
  ROOT %tuple.65 = (f32[16,56,56,896]{3,2,1,0}, f32[16,56,56,896]{2,1,3,0}) tuple(f32[16,56,56,896]{3,2,1,0} %copy.46, f32[16,56,56,896]{2,1,3,0} %maximum.31.clone.1)
}

%fused_computation.202 (param_0.524: f32[448], param_1.753: f32[448], param_2.708: f32[448], param_3.708: f32[16,56,56,448], param_4.557: f32[448]) -> (f32[16,56,56,448], f32[16,56,56,448]) {
  %param_3.708 = f32[16,56,56,448]{2,1,3,0} parameter(3)
  %param_4.557 = f32[448]{0} parameter(4)
  %constant_549_clone_1 = f32[] constant(1.99298465e-05)
  %broadcast.534.clone.1 = f32[448]{0} broadcast(f32[] %constant_549_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.429.clone.1 = f32[448]{0} multiply(f32[448]{0} %param_4.557, f32[448]{0} %broadcast.534.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.493.clone.1 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.429.clone.1), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.50.clone.1 = f32[16,56,56,448]{2,1,3,0} subtract(f32[16,56,56,448]{2,1,3,0} %param_3.708, f32[16,56,56,448]{2,1,3,0} %broadcast.493.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.708 = f32[448]{0} parameter(2)
  %constant_398_clone_1 = f32[] constant(1e-05)
  %broadcast.492.clone.1 = f32[448]{0} broadcast(f32[] %constant_398_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.187.clone.1 = f32[448]{0} add(f32[448]{0} %param_2.708, f32[448]{0} %broadcast.492.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.53.clone.1 = f32[448]{0} rsqrt(f32[448]{0} %add.187.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.753 = f32[448]{0} parameter(1)
  %multiply.391.clone.1 = f32[448]{0} multiply(f32[448]{0} %rsqrt.53.clone.1, f32[448]{0} %param_1.753), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.491.clone.1 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %multiply.391.clone.1), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.390.clone.1 = f32[16,56,56,448]{2,1,3,0} multiply(f32[16,56,56,448]{2,1,3,0} %subtract.50.clone.1, f32[16,56,56,448]{2,1,3,0} %broadcast.491.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.524 = f32[448]{0} parameter(0)
  %broadcast.490.clone.1 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[448]{0} %param_0.524), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.186.clone.1 = f32[16,56,56,448]{2,1,3,0} add(f32[16,56,56,448]{2,1,3,0} %multiply.390.clone.1, f32[16,56,56,448]{2,1,3,0} %broadcast.490.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_400_clone_1 = f32[] constant(0)
  %broadcast.489.clone.1 = f32[16,56,56,448]{2,1,3,0} broadcast(f32[] %constant_400_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %maximum.46.clone.1 = f32[16,56,56,448]{2,1,3,0} maximum(f32[16,56,56,448]{2,1,3,0} %add.186.clone.1, f32[16,56,56,448]{2,1,3,0} %broadcast.489.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %copy.47 = f32[16,56,56,448]{3,2,1,0} copy(f32[16,56,56,448]{2,1,3,0} %maximum.46.clone.1), metadata={op_name="tuple.66"}
  ROOT %tuple.90 = (f32[16,56,56,448]{3,2,1,0}, f32[16,56,56,448]{2,1,3,0}) tuple(f32[16,56,56,448]{3,2,1,0} %copy.47, f32[16,56,56,448]{2,1,3,0} %maximum.46.clone.1)
}

%region_2.1254.0 (Arg_0.1255: f32[], Arg_1.1256: f32[]) -> f32[] {
  %Arg_0.1255 = f32[] parameter(0)
  %Arg_1.1256 = f32[] parameter(1)
  ROOT %maximum.1257 = f32[] maximum(f32[] %Arg_0.1255, f32[] %Arg_1.1256), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/reduce_window_max[window_dimensions=(1, 3, 3, 1) window_strides=(1, 2, 2, 1) padding=((0, 0), (0, 1), (0, 1), (0, 0)) base_dilation=(1, 1, 1, 1) window_dilation=(1, 1, 1, 1)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/pooling.py" source_line=66}
}

%fused_computation.203 (param_0.529: f32[224], param_1.766: f32[224], param_2.721: f32[224], param_3.722: f32[16,112,112,224], param_4.574: f32[224]) -> (f32[16,56,56,224], f32[16,56,56,224]) {
  %param_3.722 = f32[16,112,112,224]{2,1,3,0} parameter(3)
  %param_4.574 = f32[224]{0} parameter(4)
  %constant_537_clone_1 = f32[] constant(4.98246163e-06)
  %broadcast.526.clone.1 = f32[224]{0} broadcast(f32[] %constant_537_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %multiply.421.clone.1 = f32[224]{0} multiply(f32[224]{0} %param_4.574, f32[224]{0} %broadcast.526.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/div" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %broadcast.517.clone.1 = f32[16,112,112,224]{2,1,3,0} broadcast(f32[224]{0} %multiply.421.clone.1), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %subtract.54.clone.1 = f32[16,112,112,224]{2,1,3,0} subtract(f32[16,112,112,224]{2,1,3,0} %param_3.722, f32[16,112,112,224]{2,1,3,0} %broadcast.517.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/sub" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=138}
  %param_2.721 = f32[224]{0} parameter(2)
  %constant_441_clone_1 = f32[] constant(1e-05)
  %broadcast.516.clone.1 = f32[224]{0} broadcast(f32[] %constant_441_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %add.199.clone.1 = f32[224]{0} add(f32[224]{0} %param_2.721, f32[224]{0} %broadcast.516.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %rsqrt.55.clone.1 = f32[224]{0} rsqrt(f32[224]{0} %add.199.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/rsqrt" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=139}
  %param_1.766 = f32[224]{0} parameter(1)
  %multiply.411.clone.1 = f32[224]{0} multiply(f32[224]{0} %rsqrt.55.clone.1, f32[224]{0} %param_1.766), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=144}
  %broadcast.515.clone.1 = f32[16,112,112,224]{2,1,3,0} broadcast(f32[224]{0} %multiply.411.clone.1), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %multiply.410.clone.1 = f32[16,112,112,224]{2,1,3,0} multiply(f32[16,112,112,224]{2,1,3,0} %subtract.54.clone.1, f32[16,112,112,224]{2,1,3,0} %broadcast.515.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=146}
  %param_0.529 = f32[224]{0} parameter(0)
  %broadcast.514.clone.1 = f32[16,112,112,224]{2,1,3,0} broadcast(f32[224]{0} %param_0.529), dimensions={3}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %add.198.clone.1 = f32[16,112,112,224]{2,1,3,0} add(f32[16,112,112,224]{2,1,3,0} %multiply.410.clone.1, f32[16,112,112,224]{2,1,3,0} %broadcast.514.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/add" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=150}
  %constant_442_clone_1 = f32[] constant(0)
  %broadcast.513.clone.1 = f32[16,112,112,224]{2,1,3,0} broadcast(f32[] %constant_442_clone_1), dimensions={}, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=124}
  %maximum.50.clone.1 = f32[16,112,112,224]{2,1,3,0} maximum(f32[16,112,112,224]{2,1,3,0} %add.198.clone.1, f32[16,112,112,224]{2,1,3,0} %broadcast.513.clone.1), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=124}
  %constant_86_clone_1 = f32[] constant(-inf)
  %reduce-window.0.clone.1 = f32[16,56,56,224]{2,1,3,0} reduce-window(f32[16,112,112,224]{2,1,3,0} %maximum.50.clone.1, f32[] %constant_86_clone_1), window={size=1x3x3x1 stride=1x2x2x1 pad=0_0x0_1x0_1x0_0}, to_apply=%region_2.1254.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/reduce_window_max[window_dimensions=(1, 3, 3, 1) window_strides=(1, 2, 2, 1) padding=((0, 0), (0, 1), (0, 1), (0, 0)) base_dilation=(1, 1, 1, 1) window_dilation=(1, 1, 1, 1)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/pooling.py" source_line=66}
  %copy.48 = f32[16,56,56,224]{3,2,1,0} copy(f32[16,56,56,224]{2,1,3,0} %reduce-window.0.clone.1), metadata={op_name="tuple.66"}
  ROOT %tuple.97 = (f32[16,56,56,224]{3,2,1,0}, f32[16,56,56,224]{2,1,3,0}) tuple(f32[16,56,56,224]{3,2,1,0} %copy.48, f32[16,56,56,224]{2,1,3,0} %reduce-window.0.clone.1)
}

%fused_computation.6.clone (param_0.531: f32[16,3584,196]) -> (f32[16,3584], f32[16,3584]) {
  %param_0.531 = f32[16,3584,196]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 3584) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_737 = f32[] constant(0)
  %reduce.364 = f32[16,3584]{1,0} reduce(f32[16,3584,196]{2,1,0} %param_0.531, f32[] %constant_737), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.149.clone.3 = f32[16,3584,196]{2,1,0} multiply(f32[16,3584,196]{2,1,0} %param_0.531, f32[16,3584,196]{2,1,0} %param_0.531), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.281.clone.2 = f32[16,3584]{1,0} reduce(f32[16,3584,196]{2,1,0} %multiply.149.clone.3, f32[] %constant_737), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.101 = (f32[16,3584]{1,0}, f32[16,3584]{1,0}) tuple(f32[16,3584]{1,0} %reduce.364, f32[16,3584]{1,0} %reduce.281.clone.2)
}

%fused_computation.12.clone (param_0.533: f32[16,3584,196]) -> (f32[16,3584], f32[16,3584]) {
  %param_0.533 = f32[16,3584,196]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 14, 14, 1792) rhs_shape=(1, 1, 1792, 3584) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_738 = f32[] constant(0)
  %reduce.365 = f32[16,3584]{1,0} reduce(f32[16,3584,196]{2,1,0} %param_0.533, f32[] %constant_738), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.157.clone.3 = f32[16,3584,196]{2,1,0} multiply(f32[16,3584,196]{2,1,0} %param_0.533, f32[16,3584,196]{2,1,0} %param_0.533), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.284.clone.2 = f32[16,3584]{1,0} reduce(f32[16,3584,196]{2,1,0} %multiply.157.clone.3, f32[] %constant_738), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.102 = (f32[16,3584]{1,0}, f32[16,3584]{1,0}) tuple(f32[16,3584]{1,0} %reduce.365, f32[16,3584]{1,0} %reduce.284.clone.2)
}

%fused_computation.19.clone (param_0.535: f32[16,1792,196]) -> (f32[16,1792], f32[16,1792]) {
  %param_0.535 = f32[16,1792,196]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_1/conv_general_dilated[window_strides=(2, 2) padding=((0, 1), (0, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(3, 3, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_739 = f32[] constant(0)
  %reduce.366 = f32[16,1792]{1,0} reduce(f32[16,1792,196]{2,1,0} %param_0.535, f32[] %constant_739), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.167.clone.3 = f32[16,1792,196]{2,1,0} multiply(f32[16,1792,196]{2,1,0} %param_0.535, f32[16,1792,196]{2,1,0} %param_0.535), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.287.clone.2 = f32[16,1792]{1,0} reduce(f32[16,1792,196]{2,1,0} %multiply.167.clone.3, f32[] %constant_739), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.103 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) tuple(f32[16,1792]{1,0} %reduce.366, f32[16,1792]{1,0} %reduce.287.clone.2)
}

%fused_computation.26.clone (param_0.537: f32[16,896,784]) -> (f32[16,896], f32[16,896]) {
  %param_0.537 = f32[16,896,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_740 = f32[] constant(0)
  %reduce.367 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %param_0.537, f32[] %constant_740), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.177.clone.3 = f32[16,896,784]{2,1,0} multiply(f32[16,896,784]{2,1,0} %param_0.537, f32[16,896,784]{2,1,0} %param_0.537), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.290.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %multiply.177.clone.3, f32[] %constant_740), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.104 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.367, f32[16,896]{1,0} %reduce.290.clone.2)
}

%fused_computation.33.clone (param_0.539: f32[16,1792,784]) -> (f32[16,1792], f32[16,1792]) {
  %param_0.539 = f32[16,1792,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_741 = f32[] constant(0)
  %reduce.368 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %param_0.539, f32[] %constant_741), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.187.clone.3 = f32[16,1792,784]{2,1,0} multiply(f32[16,1792,784]{2,1,0} %param_0.539, f32[16,1792,784]{2,1,0} %param_0.539), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.293.clone.2 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %multiply.187.clone.3, f32[] %constant_741), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.105 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) tuple(f32[16,1792]{1,0} %reduce.368, f32[16,1792]{1,0} %reduce.293.clone.2)
}

%fused_computation.40.clone (param_0.541: f32[16,896,784]) -> (f32[16,896], f32[16,896]) {
  %param_0.541 = f32[16,896,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_742 = f32[] constant(0)
  %reduce.369 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %param_0.541, f32[] %constant_742), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.197.clone.3 = f32[16,896,784]{2,1,0} multiply(f32[16,896,784]{2,1,0} %param_0.541, f32[16,896,784]{2,1,0} %param_0.541), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.296.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %multiply.197.clone.3, f32[] %constant_742), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.106 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.369, f32[16,896]{1,0} %reduce.296.clone.2)
}

%fused_computation.47.clone (param_0.543: f32[16,448,784]) -> (f32[16,448], f32[16,448]) {
  %param_0.543 = f32[16,448,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_743 = f32[] constant(0)
  %reduce.370 = f32[16,448]{1,0} reduce(f32[16,448,784]{2,1,0} %param_0.543, f32[] %constant_743), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.207.clone.3 = f32[16,448,784]{2,1,0} multiply(f32[16,448,784]{2,1,0} %param_0.543, f32[16,448,784]{2,1,0} %param_0.543), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.299.clone.2 = f32[16,448]{1,0} reduce(f32[16,448,784]{2,1,0} %multiply.207.clone.3, f32[] %constant_743), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.107 = (f32[16,448]{1,0}, f32[16,448]{1,0}) tuple(f32[16,448]{1,0} %reduce.370, f32[16,448]{1,0} %reduce.299.clone.2)
}

%fused_computation.54.clone (param_0.545: f32[16,1792,784]) -> (f32[16,1792], f32[16,1792]) {
  %param_0.545 = f32[16,1792,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_744 = f32[] constant(0)
  %reduce.371 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %param_0.545, f32[] %constant_744), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.217.clone.3 = f32[16,1792,784]{2,1,0} multiply(f32[16,1792,784]{2,1,0} %param_0.545, f32[16,1792,784]{2,1,0} %param_0.545), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.302.clone.2 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %multiply.217.clone.3, f32[] %constant_744), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.108 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) tuple(f32[16,1792]{1,0} %reduce.371, f32[16,1792]{1,0} %reduce.302.clone.2)
}

%fused_computation.61.clone (param_0.547: f32[16,896,784]) -> (f32[16,896], f32[16,896]) {
  %param_0.547 = f32[16,896,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_745 = f32[] constant(0)
  %reduce.372 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %param_0.547, f32[] %constant_745), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.227.clone.3 = f32[16,896,784]{2,1,0} multiply(f32[16,896,784]{2,1,0} %param_0.547, f32[16,896,784]{2,1,0} %param_0.547), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.305.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %multiply.227.clone.3, f32[] %constant_745), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.109 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.372, f32[16,896]{1,0} %reduce.305.clone.2)
}

%fused_computation.68.clone (param_0.549: f32[16,448,784]) -> (f32[16,448], f32[16,448]) {
  %param_0.549 = f32[16,448,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_746 = f32[] constant(0)
  %reduce.373 = f32[16,448]{1,0} reduce(f32[16,448,784]{2,1,0} %param_0.549, f32[] %constant_746), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.237.clone.3 = f32[16,448,784]{2,1,0} multiply(f32[16,448,784]{2,1,0} %param_0.549, f32[16,448,784]{2,1,0} %param_0.549), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.308.clone.2 = f32[16,448]{1,0} reduce(f32[16,448,784]{2,1,0} %multiply.237.clone.3, f32[] %constant_746), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.110 = (f32[16,448]{1,0}, f32[16,448]{1,0}) tuple(f32[16,448]{1,0} %reduce.373, f32[16,448]{1,0} %reduce.308.clone.2)
}

%fused_computation.76.clone (param_0.551: f32[16,1792,784]) -> (f32[16,1792], f32[16,1792]) {
  %param_0.551 = f32[16,1792,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_747 = f32[] constant(0)
  %reduce.374 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %param_0.551, f32[] %constant_747), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.247.clone.3 = f32[16,1792,784]{2,1,0} multiply(f32[16,1792,784]{2,1,0} %param_0.551, f32[16,1792,784]{2,1,0} %param_0.551), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.311.clone.2 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %multiply.247.clone.3, f32[] %constant_747), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.111 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) tuple(f32[16,1792]{1,0} %reduce.374, f32[16,1792]{1,0} %reduce.311.clone.2)
}

%fused_computation.83.clone (param_0.553: f32[16,896,784]) -> (f32[16,896], f32[16,896]) {
  %param_0.553 = f32[16,896,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_748 = f32[] constant(0)
  %reduce.375 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %param_0.553, f32[] %constant_748), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.257.clone.3 = f32[16,896,784]{2,1,0} multiply(f32[16,896,784]{2,1,0} %param_0.553, f32[16,896,784]{2,1,0} %param_0.553), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.314.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %multiply.257.clone.3, f32[] %constant_748), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.112 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.375, f32[16,896]{1,0} %reduce.314.clone.2)
}

%fused_computation.90.clone (param_0.555: f32[16,448,784]) -> (f32[16,448], f32[16,448]) {
  %param_0.555 = f32[16,448,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_749 = f32[] constant(0)
  %reduce.376 = f32[16,448]{1,0} reduce(f32[16,448,784]{2,1,0} %param_0.555, f32[] %constant_749), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.267.clone.3 = f32[16,448,784]{2,1,0} multiply(f32[16,448,784]{2,1,0} %param_0.555, f32[16,448,784]{2,1,0} %param_0.555), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.317.clone.2 = f32[16,448]{1,0} reduce(f32[16,448,784]{2,1,0} %multiply.267.clone.3, f32[] %constant_749), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.113 = (f32[16,448]{1,0}, f32[16,448]{1,0}) tuple(f32[16,448]{1,0} %reduce.376, f32[16,448]{1,0} %reduce.317.clone.2)
}

%fused_computation.98.clone (param_0.557: f32[16,1792,784]) -> (f32[16,1792], f32[16,1792]) {
  %param_0.557 = f32[16,1792,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_750 = f32[] constant(0)
  %reduce.377 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %param_0.557, f32[] %constant_750), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.279.clone.3 = f32[16,1792,784]{2,1,0} multiply(f32[16,1792,784]{2,1,0} %param_0.557, f32[16,1792,784]{2,1,0} %param_0.557), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.320.clone.2 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %multiply.279.clone.3, f32[] %constant_750), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.114 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) tuple(f32[16,1792]{1,0} %reduce.377, f32[16,1792]{1,0} %reduce.320.clone.2)
}

%fused_computation.104.clone (param_0.559: f32[16,1792,784]) -> (f32[16,1792], f32[16,1792]) {
  %param_0.559 = f32[16,1792,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_751 = f32[] constant(0)
  %reduce.378 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %param_0.559, f32[] %constant_751), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.287.clone.3 = f32[16,1792,784]{2,1,0} multiply(f32[16,1792,784]{2,1,0} %param_0.559, f32[16,1792,784]{2,1,0} %param_0.559), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.323.clone.2 = f32[16,1792]{1,0} reduce(f32[16,1792,784]{2,1,0} %multiply.287.clone.3, f32[] %constant_751), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.115 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) tuple(f32[16,1792]{1,0} %reduce.378, f32[16,1792]{1,0} %reduce.323.clone.2)
}

%fused_computation.111.clone (param_0.561: f32[16,896,784]) -> (f32[16,896], f32[16,896]) {
  %param_0.561 = f32[16,896,784]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_1/conv_general_dilated[window_strides=(2, 2) padding=((0, 1), (0, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_752 = f32[] constant(0)
  %reduce.379 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %param_0.561, f32[] %constant_752), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.297.clone.3 = f32[16,896,784]{2,1,0} multiply(f32[16,896,784]{2,1,0} %param_0.561, f32[16,896,784]{2,1,0} %param_0.561), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.326.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,784]{2,1,0} %multiply.297.clone.3, f32[] %constant_752), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.116 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.379, f32[16,896]{1,0} %reduce.326.clone.2)
}

%fused_computation.118.clone (param_0.563: f32[16,448,3136]) -> (f32[16,448], f32[16,448]) {
  %param_0.563 = f32[16,448,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_753 = f32[] constant(0)
  %reduce.380 = f32[16,448]{1,0} reduce(f32[16,448,3136]{2,1,0} %param_0.563, f32[] %constant_753), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.307.clone.3 = f32[16,448,3136]{2,1,0} multiply(f32[16,448,3136]{2,1,0} %param_0.563, f32[16,448,3136]{2,1,0} %param_0.563), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.329.clone.2 = f32[16,448]{1,0} reduce(f32[16,448,3136]{2,1,0} %multiply.307.clone.3, f32[] %constant_753), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.117 = (f32[16,448]{1,0}, f32[16,448]{1,0}) tuple(f32[16,448]{1,0} %reduce.380, f32[16,448]{1,0} %reduce.329.clone.2)
}

%fused_computation.125.clone (param_0.565: f32[16,896,3136]) -> (f32[16,896], f32[16,896]) {
  %param_0.565 = f32[16,896,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_754 = f32[] constant(0)
  %reduce.381 = f32[16,896]{1,0} reduce(f32[16,896,3136]{2,1,0} %param_0.565, f32[] %constant_754), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.317.clone.3 = f32[16,896,3136]{2,1,0} multiply(f32[16,896,3136]{2,1,0} %param_0.565, f32[16,896,3136]{2,1,0} %param_0.565), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.332.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,3136]{2,1,0} %multiply.317.clone.3, f32[] %constant_754), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.118 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.381, f32[16,896]{1,0} %reduce.332.clone.2)
}

%fused_computation.132.clone (param_0.567: f32[16,448,3136]) -> (f32[16,448], f32[16,448]) {
  %param_0.567 = f32[16,448,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_755 = f32[] constant(0)
  %reduce.382 = f32[16,448]{1,0} reduce(f32[16,448,3136]{2,1,0} %param_0.567, f32[] %constant_755), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.327.clone.3 = f32[16,448,3136]{2,1,0} multiply(f32[16,448,3136]{2,1,0} %param_0.567, f32[16,448,3136]{2,1,0} %param_0.567), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.335.clone.2 = f32[16,448]{1,0} reduce(f32[16,448,3136]{2,1,0} %multiply.327.clone.3, f32[] %constant_755), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.119 = (f32[16,448]{1,0}, f32[16,448]{1,0}) tuple(f32[16,448]{1,0} %reduce.382, f32[16,448]{1,0} %reduce.335.clone.2)
}

%fused_computation.139.clone (param_0.569: f32[16,224,3136]) -> (f32[16,224], f32[16,224]) {
  %param_0.569 = f32[16,224,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_756 = f32[] constant(0)
  %reduce.383 = f32[16,224]{1,0} reduce(f32[16,224,3136]{2,1,0} %param_0.569, f32[] %constant_756), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.337.clone.3 = f32[16,224,3136]{2,1,0} multiply(f32[16,224,3136]{2,1,0} %param_0.569, f32[16,224,3136]{2,1,0} %param_0.569), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.338.clone.2 = f32[16,224]{1,0} reduce(f32[16,224,3136]{2,1,0} %multiply.337.clone.3, f32[] %constant_756), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.120 = (f32[16,224]{1,0}, f32[16,224]{1,0}) tuple(f32[16,224]{1,0} %reduce.383, f32[16,224]{1,0} %reduce.338.clone.2)
}

%fused_computation.147.clone (param_0.571: f32[16,896,3136]) -> (f32[16,896], f32[16,896]) {
  %param_0.571 = f32[16,896,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_757 = f32[] constant(0)
  %reduce.384 = f32[16,896]{1,0} reduce(f32[16,896,3136]{2,1,0} %param_0.571, f32[] %constant_757), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.347.clone.3 = f32[16,896,3136]{2,1,0} multiply(f32[16,896,3136]{2,1,0} %param_0.571, f32[16,896,3136]{2,1,0} %param_0.571), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.341.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,3136]{2,1,0} %multiply.347.clone.3, f32[] %constant_757), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.121 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.384, f32[16,896]{1,0} %reduce.341.clone.2)
}

%fused_computation.154.clone (param_0.573: f32[16,448,3136]) -> (f32[16,448], f32[16,448]) {
  %param_0.573 = f32[16,448,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_758 = f32[] constant(0)
  %reduce.385 = f32[16,448]{1,0} reduce(f32[16,448,3136]{2,1,0} %param_0.573, f32[] %constant_758), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.357.clone.3 = f32[16,448,3136]{2,1,0} multiply(f32[16,448,3136]{2,1,0} %param_0.573, f32[16,448,3136]{2,1,0} %param_0.573), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.344.clone.2 = f32[16,448]{1,0} reduce(f32[16,448,3136]{2,1,0} %multiply.357.clone.3, f32[] %constant_758), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.122 = (f32[16,448]{1,0}, f32[16,448]{1,0}) tuple(f32[16,448]{1,0} %reduce.385, f32[16,448]{1,0} %reduce.344.clone.2)
}

%fused_computation.161.clone (param_0.575: f32[16,224,3136]) -> (f32[16,224], f32[16,224]) {
  %param_0.575 = f32[16,224,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_759 = f32[] constant(0)
  %reduce.386 = f32[16,224]{1,0} reduce(f32[16,224,3136]{2,1,0} %param_0.575, f32[] %constant_759), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.367.clone.3 = f32[16,224,3136]{2,1,0} multiply(f32[16,224,3136]{2,1,0} %param_0.575, f32[16,224,3136]{2,1,0} %param_0.575), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.347.clone.2 = f32[16,224]{1,0} reduce(f32[16,224,3136]{2,1,0} %multiply.367.clone.3, f32[] %constant_759), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.123 = (f32[16,224]{1,0}, f32[16,224]{1,0}) tuple(f32[16,224]{1,0} %reduce.386, f32[16,224]{1,0} %reduce.347.clone.2)
}

%fused_computation.168.clone (param_0.577: f32[16,896,3136]) -> (f32[16,896], f32[16,896]) {
  %param_0.577 = f32[16,896,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(1, 1, 224, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_760 = f32[] constant(0)
  %reduce.387 = f32[16,896]{1,0} reduce(f32[16,896,3136]{2,1,0} %param_0.577, f32[] %constant_760), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.379.clone.3 = f32[16,896,3136]{2,1,0} multiply(f32[16,896,3136]{2,1,0} %param_0.577, f32[16,896,3136]{2,1,0} %param_0.577), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.350.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,3136]{2,1,0} %multiply.379.clone.3, f32[] %constant_760), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.124 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.387, f32[16,896]{1,0} %reduce.350.clone.2)
}

%fused_computation.174.clone (param_0.579: f32[16,896,3136]) -> (f32[16,896], f32[16,896]) {
  %param_0.579 = f32[16,896,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_761 = f32[] constant(0)
  %reduce.388 = f32[16,896]{1,0} reduce(f32[16,896,3136]{2,1,0} %param_0.579, f32[] %constant_761), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.387.clone.3 = f32[16,896,3136]{2,1,0} multiply(f32[16,896,3136]{2,1,0} %param_0.579, f32[16,896,3136]{2,1,0} %param_0.579), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.353.clone.2 = f32[16,896]{1,0} reduce(f32[16,896,3136]{2,1,0} %multiply.387.clone.3, f32[] %constant_761), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.125 = (f32[16,896]{1,0}, f32[16,896]{1,0}) tuple(f32[16,896]{1,0} %reduce.388, f32[16,896]{1,0} %reduce.353.clone.2)
}

%fused_computation.181.clone (param_0.581: f32[16,448,3136]) -> (f32[16,448], f32[16,448]) {
  %param_0.581 = f32[16,448,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_762 = f32[] constant(0)
  %reduce.389 = f32[16,448]{1,0} reduce(f32[16,448,3136]{2,1,0} %param_0.581, f32[] %constant_762), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.397.clone.3 = f32[16,448,3136]{2,1,0} multiply(f32[16,448,3136]{2,1,0} %param_0.581, f32[16,448,3136]{2,1,0} %param_0.581), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.356.clone.2 = f32[16,448]{1,0} reduce(f32[16,448,3136]{2,1,0} %multiply.397.clone.3, f32[] %constant_762), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.126 = (f32[16,448]{1,0}, f32[16,448]{1,0}) tuple(f32[16,448]{1,0} %reduce.389, f32[16,448]{1,0} %reduce.356.clone.2)
}

%fused_computation.188.clone (param_0.583: f32[16,224,3136]) -> (f32[16,224], f32[16,224]) {
  %param_0.583 = f32[16,224,3136]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(1, 1, 224, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_763 = f32[] constant(0)
  %reduce.390 = f32[16,224]{1,0} reduce(f32[16,224,3136]{2,1,0} %param_0.583, f32[] %constant_763), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.407.clone.3 = f32[16,224,3136]{2,1,0} multiply(f32[16,224,3136]{2,1,0} %param_0.583, f32[16,224,3136]{2,1,0} %param_0.583), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.359.clone.2 = f32[16,224]{1,0} reduce(f32[16,224,3136]{2,1,0} %multiply.407.clone.3, f32[] %constant_763), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.127 = (f32[16,224]{1,0}, f32[16,224]{1,0}) tuple(f32[16,224]{1,0} %reduce.390, f32[16,224]{1,0} %reduce.359.clone.2)
}

%fused_computation.195.clone (param_0.585: f32[16,224,12544]) -> (f32[16,224], f32[16,224]) {
  %param_0.585 = f32[16,224,12544]{2,1,0} parameter(0), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 224, 224, 3) rhs_shape=(7, 7, 3, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %constant_764 = f32[] constant(0)
  %reduce.391 = f32[16,224]{1,0} reduce(f32[16,224,12544]{2,1,0} %param_0.585, f32[] %constant_764), dimensions={2}, to_apply=%region_0.1246.0
  %multiply.417.clone.3 = f32[16,224,12544]{2,1,0} multiply(f32[16,224,12544]{2,1,0} %param_0.585, f32[16,224,12544]{2,1,0} %param_0.585), metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/mul" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=46}
  %reduce.362.clone.2 = f32[16,224]{1,0} reduce(f32[16,224,12544]{2,1,0} %multiply.417.clone.3, f32[] %constant_764), dimensions={2}, to_apply=%region_0.1246.0
  ROOT %tuple.128 = (f32[16,224]{1,0}, f32[16,224]{1,0}) tuple(f32[16,224]{1,0} %reduce.391, f32[16,224]{1,0} %reduce.362.clone.2)
}

ENTRY %main.11352-0 (param_0: f32[224], param_1: f32[224], param_2: f32[224], param_3: f32[224], param_4: f32[448], param_5: f32[448], param_6: f32[896], param_7: f32[896], param_8: f32[896], param_9: f32[896], param_10: f32[224], param_11: f32[224], param_12: f32[448], param_13: f32[448], param_14: f32[896], param_15: f32[896], param_16: f32[224], param_17: f32[224], param_18: f32[448], param_19: f32[448], param_20: f32[896], param_21: f32[896], param_22: f32[448], param_23: f32[448], param_24: f32[896], param_25: f32[896], param_26: f32[1792], param_27: f32[1792], param_28: f32[1792], param_29: f32[1792], param_30: f32[448], param_31: f32[448], param_32: f32[896], param_33: f32[896], param_34: f32[1792], param_35: f32[1792], param_36: f32[448], param_37: f32[448], param_38: f32[896], param_39: f32[896], param_40: f32[1792], param_41: f32[1792], param_42: f32[448], param_43: f32[448], param_44: f32[896], param_45: f32[896], param_46: f32[1792], param_47: f32[1792], param_48: f32[896], param_49: f32[896], param_50: f32[1792], param_51: f32[1792], param_52: f32[3584], param_53: f32[3584], param_54: f32[3584], param_55: f32[3584], param_56: f32[7,7,3,224], param_57: f32[224], param_58: f32[224], param_59: f32[1,1,224,224], param_60: f32[224], param_61: f32[224], param_62: f32[3,3,224,448], param_63: f32[448], param_64: f32[448], param_65: s32[16,224,224,3], param_66: f32[224], param_67: f32[224], param_68: f32[224], param_69: f32[224], param_70: f32[448], param_71: f32[448], param_72: f32[1,1,448,896], param_73: f32[896], param_74: f32[896], param_75: f32[1,1,224,896], param_76: f32[896], param_77: f32[896], param_78: f32[1,1,896,224], param_79: f32[224], param_80: f32[224], param_81: f32[3,3,224,448], param_82: f32[448], param_83: f32[448], param_84: f32[1,1,448,896], param_85: f32[896], param_86: f32[896], param_87: f32[896], param_88: f32[896], param_89: f32[896], param_90: f32[896], param_91: f32[224], param_92: f32[224], param_93: f32[448], param_94: f32[448], param_95: f32[896], param_96: f32[896], param_97: f32[1,1,896,224], param_98: f32[224], param_99: f32[224], param_100: f32[3,3,224,448], param_101: f32[448], param_102: f32[448], param_103: f32[1,1,448,896], param_104: f32[896], param_105: f32[896], param_106: f32[224], param_107: f32[224], param_108: f32[448], param_109: f32[448], param_110: f32[896], param_111: f32[896], param_112: f32[1,1,896,448], param_113: f32[448], param_114: f32[448], param_115: f32[3,3,448,896], param_116: f32[896], param_117: f32[896], param_118: f32[1,1,896,1792], param_119: f32[1792], param_120: f32[1792], param_121: f32[1,1,896,1792], param_122: f32[1792], param_123: f32[1792], param_124: f32[448], param_125: f32[448], param_126: f32[896], param_127: f32[896], param_128: f32[1792], param_129: f32[1792], param_130: f32[1792], param_131: f32[1792], param_132: f32[1,1,1792,448], param_133: f32[448], param_134: f32[448], param_135: f32[3,3,448,896], param_136: f32[896], param_137: f32[896], param_138: f32[1,1,896,1792], param_139: f32[1792], param_140: f32[1792], param_141: f32[448], param_142: f32[448], param_143: f32[896], param_144: f32[896], param_145: f32[1792], param_146: f32[1792], param_147: f32[1,1,1792,448], param_148: f32[448], param_149: f32[448], param_150: f32[3,3,448,896], param_151: f32[896], param_152: f32[896], param_153: f32[1,1,896,1792], param_154: f32[1792], param_155: f32[1792], param_156: f32[448], param_157: f32[448], param_158: f32[896], param_159: f32[896], param_160: f32[1792], param_161: f32[1792], param_162: f32[1,1,1792,448], param_163: f32[448], param_164: f32[448], param_165: f32[3,3,448,896], param_166: f32[896], param_167: f32[896], param_168: f32[1,1,896,1792], param_169: f32[1792], param_170: f32[1792], param_171: f32[448], param_172: f32[448], param_173: f32[896], param_174: f32[896], param_175: f32[1792], param_176: f32[1792], param_177: f32[1,1,1792,896], param_178: f32[896], param_179: f32[896], param_180: f32[3,3,896,1792], param_181: f32[1792], param_182: f32[1792], param_183: f32[1,1,1792,3584], param_184: f32[3584], param_185: f32[3584], param_186: f32[1,1,1792,3584], param_187: f32[3584], param_188: f32[3584], param_189: f32[896], param_190: f32[896], param_191: f32[1792], param_192: f32[1792], param_193: f32[3584], param_194: f32[3584], param_195: f32[3584], param_196: f32[3584]) -> (f32[224], f32[224], f32[224], f32[224], f32[448], /*index=5*/f32[448], f32[896], f32[896], f32[896], f32[896], /*index=10*/f32[224], f32[224], f32[448], f32[448], f32[896], /*index=15*/f32[896], f32[224], f32[224], f32[448], f32[448], /*index=20*/f32[896], f32[896], f32[448], f32[448], f32[896], /*index=25*/f32[896], f32[1792], f32[1792], f32[1792], f32[1792], /*index=30*/f32[448], f32[448], f32[896], f32[896], f32[1792], /*index=35*/f32[1792], f32[448], f32[448], f32[896], f32[896], /*index=40*/f32[1792], f32[1792], f32[448], f32[448], f32[896], /*index=45*/f32[896], f32[1792], f32[1792], f32[896], f32[896], /*index=50*/f32[1792], f32[1792], f32[3584], f32[3584], f32[3584], /*index=55*/f32[3584], f32[16,56,56,448], f32[16,56,56,224], f32[16,56,56,896], f32[16,56,56,896], /*index=60*/f32[16,28,28,1792], f32[16,28,28,1792], f32[16,28,28,1792], f32[16,28,28,1792], f32[16,14,14,3584]) {
  %param_1 = f32[224]{0} parameter(1), metadata={op_name="0$start"}
  %param_67 = f32[224]{0} parameter(67), metadata={op_name="0$start"}
  %param_65 = s32[16,224,224,3]{3,2,1,0} parameter(65), metadata={op_name="0$start"}
  %fusion.196 = f32[16,224,224,3]{2,1,3,0} fusion(s32[16,224,224,3]{3,2,1,0} %param_65), kind=kLoop, calls=%fused_computation.196, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/conv_init/convert_element_type[new_dtype=float32 weak_type=False]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/dtypes.py" source_line=97}
  %param_56 = f32[7,7,3,224]{3,2,1,0} parameter(56), metadata={op_name="0$start"}
  %copy.1 = f32[7,7,3,224]{1,0,2,3} copy(f32[7,7,3,224]{3,2,1,0} %param_56), metadata={op_name="0$start"}
  %cudnn-conv = (f32[16,112,112,224]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,224,224,3]{2,1,3,0} %fusion.196, f32[7,7,3,224]{1,0,2,3} %copy.1), window={size=7x7 stride=2x2 pad=3_3x3_3}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 224, 224, 3) rhs_shape=(7, 7, 3, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element = f32[16,112,112,224]{2,1,3,0} get-tuple-element((f32[16,112,112,224]{2,1,3,0}, u8[0]{0}) %cudnn-conv), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/conv_init/conv_general_dilated[window_strides=(2, 2) padding=((3, 3), (3, 3)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 224, 224, 3) rhs_shape=(7, 7, 3, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.335 = f32[16,224,12544]{2,1,0} bitcast(f32[16,112,112,224]{2,1,3,0} %get-tuple-element)
  %fusion.195 = (f32[16,224]{1,0}, f32[16,224]{1,0}) fusion(f32[16,224,12544]{2,1,0} %bitcast.335), kind=kInput, calls=%fused_computation.195.clone
  %get-tuple-element.240 = f32[16,224]{1,0} get-tuple-element((f32[16,224]{1,0}, f32[16,224]{1,0}) %fusion.195), index=0
  %constant_519 = f32[] constant(0)
  %reduce.196 = f32[224]{0} reduce(f32[16,224]{1,0} %get-tuple-element.240, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/bn_init/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.241 = f32[16,224]{1,0} get-tuple-element((f32[16,224]{1,0}, f32[16,224]{1,0}) %fusion.195), index=1
  %param_0 = f32[224]{0} parameter(0), metadata={op_name="0$start"}
  %param_66 = f32[224]{0} parameter(66), metadata={op_name="0$start"}
  %fusion.190 = (f32[224]{0}, f32[224]{0}, f32[224]{0}) fusion(f32[224]{0} %param_1, f32[224]{0} %param_67, f32[224]{0} %reduce.196, f32[16,224]{1,0} %get-tuple-element.241, f32[224]{0} %param_0, /*index=5*/f32[224]{0} %param_66), kind=kLoop, calls=%fused_computation.190, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  %get-tuple-element.242 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.190), index=2
  %get-tuple-element.243 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.190), index=0
  %param_3 = f32[224]{0} parameter(3), metadata={op_name="0$start"}
  %param_69 = f32[224]{0} parameter(69), metadata={op_name="0$start"}
  %param_58 = f32[224]{0} parameter(58), metadata={op_name="0$start"}
  %param_57 = f32[224]{0} parameter(57), metadata={op_name="0$start"}
  %get-tuple-element.238 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.190), index=1
  %fusion.203 = (f32[16,56,56,224]{3,2,1,0}, f32[16,56,56,224]{2,1,3,0}) fusion(f32[224]{0} %param_58, f32[224]{0} %param_57, f32[224]{0} %get-tuple-element.238, f32[16,112,112,224]{2,1,3,0} %get-tuple-element, f32[224]{0} %reduce.196), kind=kInput, calls=%fused_computation.203, metadata={op_name="tuple.66"}
  %get-tuple-element.236 = f32[16,56,56,224]{2,1,3,0} get-tuple-element((f32[16,56,56,224]{3,2,1,0}, f32[16,56,56,224]{2,1,3,0}) %fusion.203), index=1
  %param_59 = f32[1,1,224,224]{3,2,1,0} parameter(59), metadata={op_name="0$start"}
  %copy.2 = f32[1,1,224,224]{1,0,2,3} copy(f32[1,1,224,224]{3,2,1,0} %param_59), metadata={op_name="0$start"}
  %cudnn-conv.1 = (f32[16,56,56,224]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,224]{2,1,3,0} %get-tuple-element.236, f32[1,1,224,224]{1,0,2,3} %copy.2), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(1, 1, 224, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.1 = f32[16,56,56,224]{2,1,3,0} get-tuple-element((f32[16,56,56,224]{2,1,3,0}, u8[0]{0}) %cudnn-conv.1), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(1, 1, 224, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.331 = f32[16,224,3136]{2,1,0} bitcast(f32[16,56,56,224]{2,1,3,0} %get-tuple-element.1)
  %fusion.188 = (f32[16,224]{1,0}, f32[16,224]{1,0}) fusion(f32[16,224,3136]{2,1,0} %bitcast.331), kind=kInput, calls=%fused_computation.188.clone
  %get-tuple-element.233 = f32[16,224]{1,0} get-tuple-element((f32[16,224]{1,0}, f32[16,224]{1,0}) %fusion.188), index=0
  %reduce.198 = f32[224]{0} reduce(f32[16,224]{1,0} %get-tuple-element.233, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.234 = f32[16,224]{1,0} get-tuple-element((f32[16,224]{1,0}, f32[16,224]{1,0}) %fusion.188), index=1
  %param_2 = f32[224]{0} parameter(2), metadata={op_name="0$start"}
  %param_68 = f32[224]{0} parameter(68), metadata={op_name="0$start"}
  %fusion.183 = (f32[224]{0}, f32[224]{0}, f32[224]{0}) fusion(f32[224]{0} %param_3, f32[224]{0} %param_69, f32[224]{0} %reduce.198, f32[16,224]{1,0} %get-tuple-element.234, f32[224]{0} %param_2, /*index=5*/f32[224]{0} %param_68), kind=kLoop, calls=%fused_computation.183, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  %get-tuple-element.244 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.183), index=2
  %get-tuple-element.245 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.183), index=0
  %param_5 = f32[448]{0} parameter(5), metadata={op_name="0$start"}
  %param_71 = f32[448]{0} parameter(71), metadata={op_name="0$start"}
  %param_61 = f32[224]{0} parameter(61), metadata={op_name="0$start"}
  %param_60 = f32[224]{0} parameter(60), metadata={op_name="0$start"}
  %get-tuple-element.231 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.183), index=1
  %fusion.182 = f32[16,56,56,224]{2,1,3,0} fusion(f32[224]{0} %param_61, f32[224]{0} %param_60, f32[224]{0} %get-tuple-element.231, f32[16,56,56,224]{2,1,3,0} %get-tuple-element.1, f32[224]{0} %reduce.198), kind=kLoop, calls=%fused_computation.182, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  %param_62 = f32[3,3,224,448]{3,2,1,0} parameter(62), metadata={op_name="0$start"}
  %copy.3 = f32[3,3,224,448]{1,0,2,3} copy(f32[3,3,224,448]{3,2,1,0} %param_62), metadata={op_name="0$start"}
  %cudnn-conv.2 = (f32[16,56,56,448]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,224]{2,1,3,0} %fusion.182, f32[3,3,224,448]{1,0,2,3} %copy.3), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.2 = f32[16,56,56,448]{2,1,3,0} get-tuple-element((f32[16,56,56,448]{2,1,3,0}, u8[0]{0}) %cudnn-conv.2), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.327 = f32[16,448,3136]{2,1,0} bitcast(f32[16,56,56,448]{2,1,3,0} %get-tuple-element.2)
  %fusion.181 = (f32[16,448]{1,0}, f32[16,448]{1,0}) fusion(f32[16,448,3136]{2,1,0} %bitcast.327), kind=kInput, calls=%fused_computation.181.clone
  %get-tuple-element.228 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.181), index=0
  %reduce.200 = f32[448]{0} reduce(f32[16,448]{1,0} %get-tuple-element.228, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.229 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.181), index=1
  %param_4 = f32[448]{0} parameter(4), metadata={op_name="0$start"}
  %param_70 = f32[448]{0} parameter(70), metadata={op_name="0$start"}
  %fusion.176 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) fusion(f32[448]{0} %param_5, f32[448]{0} %param_71, f32[448]{0} %reduce.200, f32[16,448]{1,0} %get-tuple-element.229, f32[448]{0} %param_4, /*index=5*/f32[448]{0} %param_70), kind=kLoop, calls=%fused_computation.176, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(00)/add"}
  %get-tuple-element.246 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.176), index=2
  %get-tuple-element.247 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.176), index=0
  %param_7 = f32[896]{0} parameter(7), metadata={op_name="0$start"}
  %param_88 = f32[896]{0} parameter(88), metadata={op_name="0$start"}
  %param_64 = f32[448]{0} parameter(64), metadata={op_name="0$start"}
  %param_63 = f32[448]{0} parameter(63), metadata={op_name="0$start"}
  %get-tuple-element.226 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.176), index=1
  %fusion.202 = (f32[16,56,56,448]{3,2,1,0}, f32[16,56,56,448]{2,1,3,0}) fusion(f32[448]{0} %param_64, f32[448]{0} %param_63, f32[448]{0} %get-tuple-element.226, f32[16,56,56,448]{2,1,3,0} %get-tuple-element.2, f32[448]{0} %reduce.200), kind=kInput, calls=%fused_computation.202, metadata={op_name="tuple.66"}
  %get-tuple-element.224 = f32[16,56,56,448]{2,1,3,0} get-tuple-element((f32[16,56,56,448]{3,2,1,0}, f32[16,56,56,448]{2,1,3,0}) %fusion.202), index=1
  %param_72 = f32[1,1,448,896]{3,2,1,0} parameter(72), metadata={op_name="0$start"}
  %copy.4 = f32[1,1,448,896]{1,0,2,3} copy(f32[1,1,448,896]{3,2,1,0} %param_72), metadata={op_name="0$start"}
  %cudnn-conv.3 = (f32[16,56,56,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,448]{2,1,3,0} %get-tuple-element.224, f32[1,1,448,896]{1,0,2,3} %copy.4), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.3 = f32[16,56,56,896]{2,1,3,0} get-tuple-element((f32[16,56,56,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.3), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.323 = f32[16,896,3136]{2,1,0} bitcast(f32[16,56,56,896]{2,1,3,0} %get-tuple-element.3)
  %fusion.174 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,3136]{2,1,0} %bitcast.323), kind=kInput, calls=%fused_computation.174.clone
  %get-tuple-element.221 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.174), index=0
  %reduce.204 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.221, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.222 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.174), index=1
  %param_6 = f32[896]{0} parameter(6), metadata={op_name="0$start"}
  %param_87 = f32[896]{0} parameter(87), metadata={op_name="0$start"}
  %fusion.169 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_7, f32[896]{0} %param_88, f32[896]{0} %reduce.204, f32[16,896]{1,0} %get-tuple-element.222, f32[896]{0} %param_6, /*index=5*/f32[896]{0} %param_87), kind=kLoop, calls=%fused_computation.169, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %get-tuple-element.248 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.169), index=2
  %get-tuple-element.249 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.169), index=0
  %param_9 = f32[896]{0} parameter(9), metadata={op_name="0$start"}
  %param_90 = f32[896]{0} parameter(90), metadata={op_name="0$start"}
  %param_75 = f32[1,1,224,896]{3,2,1,0} parameter(75), metadata={op_name="0$start"}
  %copy.5 = f32[1,1,224,896]{1,0,2,3} copy(f32[1,1,224,896]{3,2,1,0} %param_75), metadata={op_name="0$start"}
  %cudnn-conv.4 = (f32[16,56,56,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,224]{2,1,3,0} %get-tuple-element.236, f32[1,1,224,896]{1,0,2,3} %copy.5), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(1, 1, 224, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.4 = f32[16,56,56,896]{2,1,3,0} get-tuple-element((f32[16,56,56,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.4), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/conv_proj/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(1, 1, 224, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.319 = f32[16,896,3136]{2,1,0} bitcast(f32[16,56,56,896]{2,1,3,0} %get-tuple-element.4)
  %fusion.168 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,3136]{2,1,0} %bitcast.319), kind=kInput, calls=%fused_computation.168.clone
  %get-tuple-element.216 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.168), index=0
  %reduce.202 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.216, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/norm_proj/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.217 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.168), index=1
  %param_8 = f32[896]{0} parameter(8), metadata={op_name="0$start"}
  %param_89 = f32[896]{0} parameter(89), metadata={op_name="0$start"}
  %fusion.163 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_9, f32[896]{0} %param_90, f32[896]{0} %reduce.202, f32[16,896]{1,0} %get-tuple-element.217, f32[896]{0} %param_8, /*index=5*/f32[896]{0} %param_89), kind=kLoop, calls=%fused_computation.163, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %get-tuple-element.250 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.163), index=2
  %get-tuple-element.251 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.163), index=0
  %param_11 = f32[224]{0} parameter(11), metadata={op_name="0$start"}
  %param_92 = f32[224]{0} parameter(92), metadata={op_name="0$start"}
  %param_74 = f32[896]{0} parameter(74), metadata={op_name="0$start"}
  %param_73 = f32[896]{0} parameter(73), metadata={op_name="0$start"}
  %get-tuple-element.219 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.169), index=1
  %param_77 = f32[896]{0} parameter(77), metadata={op_name="0$start"}
  %param_76 = f32[896]{0} parameter(76), metadata={op_name="0$start"}
  %get-tuple-element.214 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.163), index=1
  %fusion.162 = f32[16,56,56,896]{2,1,3,0} fusion(f32[896]{0} %param_74, f32[896]{0} %param_73, f32[896]{0} %get-tuple-element.219, f32[16,56,56,896]{2,1,3,0} %get-tuple-element.3, f32[896]{0} %param_77, /*index=5*/f32[896]{0} %param_76, f32[896]{0} %get-tuple-element.214, f32[16,56,56,896]{2,1,3,0} %get-tuple-element.4, f32[896]{0} %reduce.204, f32[896]{0} %reduce.202), kind=kLoop, calls=%fused_computation.162, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_0/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %param_78 = f32[1,1,896,224]{3,2,1,0} parameter(78), metadata={op_name="0$start"}
  %copy.6 = f32[1,1,896,224]{1,0,2,3} copy(f32[1,1,896,224]{3,2,1,0} %param_78), metadata={op_name="0$start"}
  %cudnn-conv.5 = (f32[16,56,56,224]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,896]{2,1,3,0} %fusion.162, f32[1,1,896,224]{1,0,2,3} %copy.6), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.5 = f32[16,56,56,224]{2,1,3,0} get-tuple-element((f32[16,56,56,224]{2,1,3,0}, u8[0]{0}) %cudnn-conv.5), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.315 = f32[16,224,3136]{2,1,0} bitcast(f32[16,56,56,224]{2,1,3,0} %get-tuple-element.5)
  %fusion.161 = (f32[16,224]{1,0}, f32[16,224]{1,0}) fusion(f32[16,224,3136]{2,1,0} %bitcast.315), kind=kInput, calls=%fused_computation.161.clone
  %get-tuple-element.211 = f32[16,224]{1,0} get-tuple-element((f32[16,224]{1,0}, f32[16,224]{1,0}) %fusion.161), index=0
  %reduce.206 = f32[224]{0} reduce(f32[16,224]{1,0} %get-tuple-element.211, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.212 = f32[16,224]{1,0} get-tuple-element((f32[16,224]{1,0}, f32[16,224]{1,0}) %fusion.161), index=1
  %param_10 = f32[224]{0} parameter(10), metadata={op_name="0$start"}
  %param_91 = f32[224]{0} parameter(91), metadata={op_name="0$start"}
  %fusion.156 = (f32[224]{0}, f32[224]{0}, f32[224]{0}) fusion(f32[224]{0} %param_11, f32[224]{0} %param_92, f32[224]{0} %reduce.206, f32[16,224]{1,0} %get-tuple-element.212, f32[224]{0} %param_10, /*index=5*/f32[224]{0} %param_91), kind=kLoop, calls=%fused_computation.156, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %get-tuple-element.252 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.156), index=2
  %get-tuple-element.253 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.156), index=0
  %param_13 = f32[448]{0} parameter(13), metadata={op_name="0$start"}
  %param_94 = f32[448]{0} parameter(94), metadata={op_name="0$start"}
  %param_80 = f32[224]{0} parameter(80), metadata={op_name="0$start"}
  %param_79 = f32[224]{0} parameter(79), metadata={op_name="0$start"}
  %get-tuple-element.209 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.156), index=1
  %fusion.155 = f32[16,56,56,224]{2,1,3,0} fusion(f32[224]{0} %param_80, f32[224]{0} %param_79, f32[224]{0} %get-tuple-element.209, f32[16,56,56,224]{2,1,3,0} %get-tuple-element.5, f32[224]{0} %reduce.206), kind=kLoop, calls=%fused_computation.155, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  %param_81 = f32[3,3,224,448]{3,2,1,0} parameter(81), metadata={op_name="0$start"}
  %copy.7 = f32[3,3,224,448]{1,0,2,3} copy(f32[3,3,224,448]{3,2,1,0} %param_81), metadata={op_name="0$start"}
  %cudnn-conv.6 = (f32[16,56,56,448]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,224]{2,1,3,0} %fusion.155, f32[3,3,224,448]{1,0,2,3} %copy.7), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.6 = f32[16,56,56,448]{2,1,3,0} get-tuple-element((f32[16,56,56,448]{2,1,3,0}, u8[0]{0}) %cudnn-conv.6), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.311 = f32[16,448,3136]{2,1,0} bitcast(f32[16,56,56,448]{2,1,3,0} %get-tuple-element.6)
  %fusion.154 = (f32[16,448]{1,0}, f32[16,448]{1,0}) fusion(f32[16,448,3136]{2,1,0} %bitcast.311), kind=kInput, calls=%fused_computation.154.clone
  %get-tuple-element.206 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.154), index=0
  %reduce.208 = f32[448]{0} reduce(f32[16,448]{1,0} %get-tuple-element.206, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.207 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.154), index=1
  %param_12 = f32[448]{0} parameter(12), metadata={op_name="0$start"}
  %param_93 = f32[448]{0} parameter(93), metadata={op_name="0$start"}
  %fusion.149 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) fusion(f32[448]{0} %param_13, f32[448]{0} %param_94, f32[448]{0} %reduce.208, f32[16,448]{1,0} %get-tuple-element.207, f32[448]{0} %param_12, /*index=5*/f32[448]{0} %param_93), kind=kLoop, calls=%fused_computation.149, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %get-tuple-element.254 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.149), index=2
  %get-tuple-element.255 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.149), index=0
  %param_15 = f32[896]{0} parameter(15), metadata={op_name="0$start"}
  %param_96 = f32[896]{0} parameter(96), metadata={op_name="0$start"}
  %param_83 = f32[448]{0} parameter(83), metadata={op_name="0$start"}
  %param_82 = f32[448]{0} parameter(82), metadata={op_name="0$start"}
  %get-tuple-element.204 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.149), index=1
  %fusion.148 = f32[16,56,56,448]{2,1,3,0} fusion(f32[448]{0} %param_83, f32[448]{0} %param_82, f32[448]{0} %get-tuple-element.204, f32[16,56,56,448]{2,1,3,0} %get-tuple-element.6, f32[448]{0} %reduce.208), kind=kLoop, calls=%fused_computation.148, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %param_84 = f32[1,1,448,896]{3,2,1,0} parameter(84), metadata={op_name="0$start"}
  %copy.8 = f32[1,1,448,896]{1,0,2,3} copy(f32[1,1,448,896]{3,2,1,0} %param_84), metadata={op_name="0$start"}
  %cudnn-conv.7 = (f32[16,56,56,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,448]{2,1,3,0} %fusion.148, f32[1,1,448,896]{1,0,2,3} %copy.8), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.7 = f32[16,56,56,896]{2,1,3,0} get-tuple-element((f32[16,56,56,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.7), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.307 = f32[16,896,3136]{2,1,0} bitcast(f32[16,56,56,896]{2,1,3,0} %get-tuple-element.7)
  %fusion.147 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,3136]{2,1,0} %bitcast.307), kind=kInput, calls=%fused_computation.147.clone
  %get-tuple-element.201 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.147), index=0
  %reduce.210 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.201, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/jvp(WideResNet)/BottleneckResNetBlock_1/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.202 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.147), index=1
  %param_14 = f32[896]{0} parameter(14), metadata={op_name="0$start"}
  %param_95 = f32[896]{0} parameter(95), metadata={op_name="0$start"}
  %fusion.142 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_15, f32[896]{0} %param_96, f32[896]{0} %reduce.210, f32[16,896]{1,0} %get-tuple-element.202, f32[896]{0} %param_14, /*index=5*/f32[896]{0} %param_95), kind=kLoop, calls=%fused_computation.142, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(01)/add"}
  %get-tuple-element.256 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.142), index=2
  %get-tuple-element.257 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.142), index=0
  %param_17 = f32[224]{0} parameter(17), metadata={op_name="0$start"}
  %param_107 = f32[224]{0} parameter(107), metadata={op_name="0$start"}
  %param_86 = f32[896]{0} parameter(86), metadata={op_name="0$start"}
  %param_85 = f32[896]{0} parameter(85), metadata={op_name="0$start"}
  %get-tuple-element.199 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.142), index=1
  %fusion.140 = f32[16,56,56,896]{2,1,3,0} fusion(f32[16,56,56,896]{2,1,3,0} %fusion.162, f32[896]{0} %param_86, f32[896]{0} %param_85, f32[896]{0} %get-tuple-element.199, f32[16,56,56,896]{2,1,3,0} %get-tuple-element.7, /*index=5*/f32[896]{0} %reduce.210), kind=kLoop, calls=%fused_computation.140, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_1/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %param_97 = f32[1,1,896,224]{3,2,1,0} parameter(97), metadata={op_name="0$start"}
  %copy.9 = f32[1,1,896,224]{1,0,2,3} copy(f32[1,1,896,224]{3,2,1,0} %param_97), metadata={op_name="0$start"}
  %cudnn-conv.8 = (f32[16,56,56,224]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,896]{2,1,3,0} %fusion.140, f32[1,1,896,224]{1,0,2,3} %copy.9), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.8 = f32[16,56,56,224]{2,1,3,0} get-tuple-element((f32[16,56,56,224]{2,1,3,0}, u8[0]{0}) %cudnn-conv.8), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 224) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.303 = f32[16,224,3136]{2,1,0} bitcast(f32[16,56,56,224]{2,1,3,0} %get-tuple-element.8)
  %fusion.139 = (f32[16,224]{1,0}, f32[16,224]{1,0}) fusion(f32[16,224,3136]{2,1,0} %bitcast.303), kind=kInput, calls=%fused_computation.139.clone
  %get-tuple-element.196 = f32[16,224]{1,0} get-tuple-element((f32[16,224]{1,0}, f32[16,224]{1,0}) %fusion.139), index=0
  %reduce.212 = f32[224]{0} reduce(f32[16,224]{1,0} %get-tuple-element.196, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.197 = f32[16,224]{1,0} get-tuple-element((f32[16,224]{1,0}, f32[16,224]{1,0}) %fusion.139), index=1
  %param_16 = f32[224]{0} parameter(16), metadata={op_name="0$start"}
  %param_106 = f32[224]{0} parameter(106), metadata={op_name="0$start"}
  %fusion.134 = (f32[224]{0}, f32[224]{0}, f32[224]{0}) fusion(f32[224]{0} %param_17, f32[224]{0} %param_107, f32[224]{0} %reduce.212, f32[16,224]{1,0} %get-tuple-element.197, f32[224]{0} %param_16, /*index=5*/f32[224]{0} %param_106), kind=kLoop, calls=%fused_computation.134, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  %get-tuple-element.258 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.134), index=2
  %get-tuple-element.259 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.134), index=0
  %param_19 = f32[448]{0} parameter(19), metadata={op_name="0$start"}
  %param_109 = f32[448]{0} parameter(109), metadata={op_name="0$start"}
  %param_99 = f32[224]{0} parameter(99), metadata={op_name="0$start"}
  %param_98 = f32[224]{0} parameter(98), metadata={op_name="0$start"}
  %get-tuple-element.194 = f32[224]{0} get-tuple-element((f32[224]{0}, f32[224]{0}, f32[224]{0}) %fusion.134), index=1
  %fusion.133 = f32[16,56,56,224]{2,1,3,0} fusion(f32[224]{0} %param_99, f32[224]{0} %param_98, f32[224]{0} %get-tuple-element.194, f32[16,56,56,224]{2,1,3,0} %get-tuple-element.8, f32[224]{0} %reduce.212), kind=kLoop, calls=%fused_computation.133, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  %param_100 = f32[3,3,224,448]{3,2,1,0} parameter(100), metadata={op_name="0$start"}
  %copy.10 = f32[3,3,224,448]{1,0,2,3} copy(f32[3,3,224,448]{3,2,1,0} %param_100), metadata={op_name="0$start"}
  %cudnn-conv.9 = (f32[16,56,56,448]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,224]{2,1,3,0} %fusion.133, f32[3,3,224,448]{1,0,2,3} %copy.10), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.9 = f32[16,56,56,448]{2,1,3,0} get-tuple-element((f32[16,56,56,448]{2,1,3,0}, u8[0]{0}) %cudnn-conv.9), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 224) rhs_shape=(3, 3, 224, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.299 = f32[16,448,3136]{2,1,0} bitcast(f32[16,56,56,448]{2,1,3,0} %get-tuple-element.9)
  %fusion.132 = (f32[16,448]{1,0}, f32[16,448]{1,0}) fusion(f32[16,448,3136]{2,1,0} %bitcast.299), kind=kInput, calls=%fused_computation.132.clone
  %get-tuple-element.191 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.132), index=0
  %reduce.214 = f32[448]{0} reduce(f32[16,448]{1,0} %get-tuple-element.191, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.192 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.132), index=1
  %param_18 = f32[448]{0} parameter(18), metadata={op_name="0$start"}
  %param_108 = f32[448]{0} parameter(108), metadata={op_name="0$start"}
  %fusion.127 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) fusion(f32[448]{0} %param_19, f32[448]{0} %param_109, f32[448]{0} %reduce.214, f32[16,448]{1,0} %get-tuple-element.192, f32[448]{0} %param_18, /*index=5*/f32[448]{0} %param_108), kind=kLoop, calls=%fused_computation.127, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  %get-tuple-element.260 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.127), index=2
  %get-tuple-element.261 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.127), index=0
  %param_21 = f32[896]{0} parameter(21), metadata={op_name="0$start"}
  %param_111 = f32[896]{0} parameter(111), metadata={op_name="0$start"}
  %param_102 = f32[448]{0} parameter(102), metadata={op_name="0$start"}
  %param_101 = f32[448]{0} parameter(101), metadata={op_name="0$start"}
  %get-tuple-element.189 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.127), index=1
  %fusion.126 = f32[16,56,56,448]{2,1,3,0} fusion(f32[448]{0} %param_102, f32[448]{0} %param_101, f32[448]{0} %get-tuple-element.189, f32[16,56,56,448]{2,1,3,0} %get-tuple-element.9, f32[448]{0} %reduce.214), kind=kLoop, calls=%fused_computation.126, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %param_103 = f32[1,1,448,896]{3,2,1,0} parameter(103), metadata={op_name="0$start"}
  %copy.11 = f32[1,1,448,896]{1,0,2,3} copy(f32[1,1,448,896]{3,2,1,0} %param_103), metadata={op_name="0$start"}
  %cudnn-conv.10 = (f32[16,56,56,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,448]{2,1,3,0} %fusion.126, f32[1,1,448,896]{1,0,2,3} %copy.11), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.10 = f32[16,56,56,896]{2,1,3,0} get-tuple-element((f32[16,56,56,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.10), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(1, 1, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.295 = f32[16,896,3136]{2,1,0} bitcast(f32[16,56,56,896]{2,1,3,0} %get-tuple-element.10)
  %fusion.125 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,3136]{2,1,0} %bitcast.295), kind=kInput, calls=%fused_computation.125.clone
  %get-tuple-element.186 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.125), index=0
  %reduce.216 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.186, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/jvp(WideResNet)/BottleneckResNetBlock_2/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.187 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.125), index=1
  %param_20 = f32[896]{0} parameter(20), metadata={op_name="0$start"}
  %param_110 = f32[896]{0} parameter(110), metadata={op_name="0$start"}
  %fusion.120 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_21, f32[896]{0} %param_111, f32[896]{0} %reduce.216, f32[16,896]{1,0} %get-tuple-element.187, f32[896]{0} %param_20, /*index=5*/f32[896]{0} %param_110), kind=kLoop, calls=%fused_computation.120, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(02)/add"}
  %get-tuple-element.262 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.120), index=2
  %get-tuple-element.263 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.120), index=0
  %param_23 = f32[448]{0} parameter(23), metadata={op_name="0$start"}
  %param_125 = f32[448]{0} parameter(125), metadata={op_name="0$start"}
  %param_105 = f32[896]{0} parameter(105), metadata={op_name="0$start"}
  %param_104 = f32[896]{0} parameter(104), metadata={op_name="0$start"}
  %get-tuple-element.184 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.120), index=1
  %fusion.201 = (f32[16,56,56,896]{3,2,1,0}, f32[16,56,56,896]{2,1,3,0}) fusion(f32[16,56,56,896]{2,1,3,0} %fusion.140, f32[896]{0} %param_105, f32[896]{0} %param_104, f32[896]{0} %get-tuple-element.184, f32[16,56,56,896]{2,1,3,0} %get-tuple-element.10, /*index=5*/f32[896]{0} %reduce.216), kind=kInput, calls=%fused_computation.201, metadata={op_name="tuple.66"}
  %get-tuple-element.182 = f32[16,56,56,896]{2,1,3,0} get-tuple-element((f32[16,56,56,896]{3,2,1,0}, f32[16,56,56,896]{2,1,3,0}) %fusion.201), index=1
  %param_112 = f32[1,1,896,448]{3,2,1,0} parameter(112), metadata={op_name="0$start"}
  %copy.12 = f32[1,1,896,448]{1,0,2,3} copy(f32[1,1,896,448]{3,2,1,0} %param_112), metadata={op_name="0$start"}
  %cudnn-conv.12 = (f32[16,56,56,448]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,896]{2,1,3,0} %get-tuple-element.182, f32[1,1,896,448]{1,0,2,3} %copy.12), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.12 = f32[16,56,56,448]{2,1,3,0} get-tuple-element((f32[16,56,56,448]{2,1,3,0}, u8[0]{0}) %cudnn-conv.12), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.291 = f32[16,448,3136]{2,1,0} bitcast(f32[16,56,56,448]{2,1,3,0} %get-tuple-element.12)
  %fusion.118 = (f32[16,448]{1,0}, f32[16,448]{1,0}) fusion(f32[16,448,3136]{2,1,0} %bitcast.291), kind=kInput, calls=%fused_computation.118.clone
  %get-tuple-element.179 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.118), index=0
  %reduce.220 = f32[448]{0} reduce(f32[16,448]{1,0} %get-tuple-element.179, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.180 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.118), index=1
  %param_22 = f32[448]{0} parameter(22), metadata={op_name="0$start"}
  %param_124 = f32[448]{0} parameter(124), metadata={op_name="0$start"}
  %fusion.113 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) fusion(f32[448]{0} %param_23, f32[448]{0} %param_125, f32[448]{0} %reduce.220, f32[16,448]{1,0} %get-tuple-element.180, f32[448]{0} %param_22, /*index=5*/f32[448]{0} %param_124), kind=kLoop, calls=%fused_computation.113, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  %get-tuple-element.264 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.113), index=2
  %get-tuple-element.265 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.113), index=0
  %param_25 = f32[896]{0} parameter(25), metadata={op_name="0$start"}
  %param_127 = f32[896]{0} parameter(127), metadata={op_name="0$start"}
  %param_114 = f32[448]{0} parameter(114), metadata={op_name="0$start"}
  %param_113 = f32[448]{0} parameter(113), metadata={op_name="0$start"}
  %get-tuple-element.177 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.113), index=1
  %fusion.112 = f32[16,57,57,448]{2,1,3,0} fusion(f32[448]{0} %param_114, f32[448]{0} %param_113, f32[448]{0} %get-tuple-element.177, f32[16,56,56,448]{2,1,3,0} %get-tuple-element.12, f32[448]{0} %reduce.220), kind=kLoop, calls=%fused_computation.112, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  %param_115 = f32[3,3,448,896]{3,2,1,0} parameter(115), metadata={op_name="0$start"}
  %copy.13 = f32[3,3,448,896]{1,0,2,3} copy(f32[3,3,448,896]{3,2,1,0} %param_115), metadata={op_name="0$start"}
  %cudnn-conv.13 = (f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,57,57,448]{2,1,3,0} %fusion.112, f32[3,3,448,896]{1,0,2,3} %copy.13), window={size=3x3 stride=2x2}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_1/conv_general_dilated[window_strides=(2, 2) padding=((0, 1), (0, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.13 = f32[16,28,28,896]{2,1,3,0} get-tuple-element((f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.13), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_1/conv_general_dilated[window_strides=(2, 2) padding=((0, 1), (0, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.287 = f32[16,896,784]{2,1,0} bitcast(f32[16,28,28,896]{2,1,3,0} %get-tuple-element.13)
  %fusion.111 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,784]{2,1,0} %bitcast.287), kind=kInput, calls=%fused_computation.111.clone
  %get-tuple-element.174 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.111), index=0
  %reduce.222 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.174, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.175 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.111), index=1
  %param_24 = f32[896]{0} parameter(24), metadata={op_name="0$start"}
  %param_126 = f32[896]{0} parameter(126), metadata={op_name="0$start"}
  %fusion.106 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_25, f32[896]{0} %param_127, f32[896]{0} %reduce.222, f32[16,896]{1,0} %get-tuple-element.175, f32[896]{0} %param_24, /*index=5*/f32[896]{0} %param_126), kind=kLoop, calls=%fused_computation.106, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  %get-tuple-element.266 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.106), index=2
  %get-tuple-element.267 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.106), index=0
  %param_27 = f32[1792]{0} parameter(27), metadata={op_name="0$start"}
  %param_129 = f32[1792]{0} parameter(129), metadata={op_name="0$start"}
  %param_117 = f32[896]{0} parameter(117), metadata={op_name="0$start"}
  %param_116 = f32[896]{0} parameter(116), metadata={op_name="0$start"}
  %get-tuple-element.172 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.106), index=1
  %fusion.105 = f32[16,28,28,896]{2,1,3,0} fusion(f32[896]{0} %param_117, f32[896]{0} %param_116, f32[896]{0} %get-tuple-element.172, f32[16,28,28,896]{2,1,3,0} %get-tuple-element.13, f32[896]{0} %reduce.222), kind=kLoop, calls=%fused_computation.105, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %param_118 = f32[1,1,896,1792]{3,2,1,0} parameter(118), metadata={op_name="0$start"}
  %copy.14 = f32[1,1,896,1792]{1,0,2,3} copy(f32[1,1,896,1792]{3,2,1,0} %param_118), metadata={op_name="0$start"}
  %cudnn-conv.14 = (f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,896]{2,1,3,0} %fusion.105, f32[1,1,896,1792]{1,0,2,3} %copy.14), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.14 = f32[16,28,28,1792]{2,1,3,0} get-tuple-element((f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) %cudnn-conv.14), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.283 = f32[16,1792,784]{2,1,0} bitcast(f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.14)
  %fusion.104 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) fusion(f32[16,1792,784]{2,1,0} %bitcast.283), kind=kInput, calls=%fused_computation.104.clone
  %get-tuple-element.169 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.104), index=0
  %reduce.224 = f32[1792]{0} reduce(f32[16,1792]{1,0} %get-tuple-element.169, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.170 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.104), index=1
  %param_26 = f32[1792]{0} parameter(26), metadata={op_name="0$start"}
  %param_128 = f32[1792]{0} parameter(128), metadata={op_name="0$start"}
  %fusion.99 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) fusion(f32[1792]{0} %param_27, f32[1792]{0} %param_129, f32[1792]{0} %reduce.224, f32[16,1792]{1,0} %get-tuple-element.170, f32[1792]{0} %param_26, /*index=5*/f32[1792]{0} %param_128), kind=kLoop, calls=%fused_computation.99, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  %get-tuple-element.268 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.99), index=2
  %get-tuple-element.269 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.99), index=0
  %param_29 = f32[1792]{0} parameter(29), metadata={op_name="0$start"}
  %param_131 = f32[1792]{0} parameter(131), metadata={op_name="0$start"}
  %param_121 = f32[1,1,896,1792]{3,2,1,0} parameter(121), metadata={op_name="0$start"}
  %copy.15 = f32[1,1,896,1792]{1,0,2,3} copy(f32[1,1,896,1792]{3,2,1,0} %param_121), metadata={op_name="0$start"}
  %cudnn-conv.11 = (f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,56,56,896]{2,1,3,0} %get-tuple-element.182, f32[1,1,896,1792]{1,0,2,3} %copy.15), window={size=1x1 stride=2x2}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.11 = f32[16,28,28,1792]{2,1,3,0} get-tuple-element((f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) %cudnn-conv.11), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 56, 56, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.279 = f32[16,1792,784]{2,1,0} bitcast(f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.11)
  %fusion.98 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) fusion(f32[16,1792,784]{2,1,0} %bitcast.279), kind=kInput, calls=%fused_computation.98.clone
  %get-tuple-element.164 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.98), index=0
  %reduce.218 = f32[1792]{0} reduce(f32[16,1792]{1,0} %get-tuple-element.164, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/jvp(WideResNet)/BottleneckResNetBlock_3/norm_proj/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.165 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.98), index=1
  %param_28 = f32[1792]{0} parameter(28), metadata={op_name="0$start"}
  %param_130 = f32[1792]{0} parameter(130), metadata={op_name="0$start"}
  %fusion.93 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) fusion(f32[1792]{0} %param_29, f32[1792]{0} %param_131, f32[1792]{0} %reduce.218, f32[16,1792]{1,0} %get-tuple-element.165, f32[1792]{0} %param_28, /*index=5*/f32[1792]{0} %param_130), kind=kLoop, calls=%fused_computation.93, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(03)/add"}
  %get-tuple-element.270 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.93), index=2
  %get-tuple-element.271 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.93), index=0
  %param_31 = f32[448]{0} parameter(31), metadata={op_name="0$start"}
  %param_142 = f32[448]{0} parameter(142), metadata={op_name="0$start"}
  %param_120 = f32[1792]{0} parameter(120), metadata={op_name="0$start"}
  %param_119 = f32[1792]{0} parameter(119), metadata={op_name="0$start"}
  %get-tuple-element.167 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.99), index=1
  %param_123 = f32[1792]{0} parameter(123), metadata={op_name="0$start"}
  %param_122 = f32[1792]{0} parameter(122), metadata={op_name="0$start"}
  %get-tuple-element.162 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.93), index=1
  %fusion.91 = f32[16,28,28,1792]{2,1,3,0} fusion(f32[1792]{0} %param_120, f32[1792]{0} %param_119, f32[1792]{0} %get-tuple-element.167, f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.14, f32[1792]{0} %reduce.224, /*index=5*/f32[1792]{0} %param_123, f32[1792]{0} %param_122, f32[1792]{0} %get-tuple-element.162, f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.11, f32[1792]{0} %reduce.218), kind=kLoop, calls=%fused_computation.91, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_3/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %param_132 = f32[1,1,1792,448]{3,2,1,0} parameter(132), metadata={op_name="0$start"}
  %copy.16 = f32[1,1,1792,448]{1,0,2,3} copy(f32[1,1,1792,448]{3,2,1,0} %param_132), metadata={op_name="0$start"}
  %cudnn-conv.15 = (f32[16,28,28,448]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,1792]{2,1,3,0} %fusion.91, f32[1,1,1792,448]{1,0,2,3} %copy.16), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.15 = f32[16,28,28,448]{2,1,3,0} get-tuple-element((f32[16,28,28,448]{2,1,3,0}, u8[0]{0}) %cudnn-conv.15), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.275 = f32[16,448,784]{2,1,0} bitcast(f32[16,28,28,448]{2,1,3,0} %get-tuple-element.15)
  %fusion.90 = (f32[16,448]{1,0}, f32[16,448]{1,0}) fusion(f32[16,448,784]{2,1,0} %bitcast.275), kind=kInput, calls=%fused_computation.90.clone
  %get-tuple-element.159 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.90), index=0
  %reduce.226 = f32[448]{0} reduce(f32[16,448]{1,0} %get-tuple-element.159, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.160 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.90), index=1
  %param_30 = f32[448]{0} parameter(30), metadata={op_name="0$start"}
  %param_141 = f32[448]{0} parameter(141), metadata={op_name="0$start"}
  %fusion.85 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) fusion(f32[448]{0} %param_31, f32[448]{0} %param_142, f32[448]{0} %reduce.226, f32[16,448]{1,0} %get-tuple-element.160, f32[448]{0} %param_30, /*index=5*/f32[448]{0} %param_141), kind=kLoop, calls=%fused_computation.85, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  %get-tuple-element.272 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.85), index=2
  %get-tuple-element.273 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.85), index=0
  %param_33 = f32[896]{0} parameter(33), metadata={op_name="0$start"}
  %param_144 = f32[896]{0} parameter(144), metadata={op_name="0$start"}
  %param_134 = f32[448]{0} parameter(134), metadata={op_name="0$start"}
  %param_133 = f32[448]{0} parameter(133), metadata={op_name="0$start"}
  %get-tuple-element.157 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.85), index=1
  %fusion.84 = f32[16,28,28,448]{2,1,3,0} fusion(f32[448]{0} %param_134, f32[448]{0} %param_133, f32[448]{0} %get-tuple-element.157, f32[16,28,28,448]{2,1,3,0} %get-tuple-element.15, f32[448]{0} %reduce.226), kind=kLoop, calls=%fused_computation.84, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  %param_135 = f32[3,3,448,896]{3,2,1,0} parameter(135), metadata={op_name="0$start"}
  %copy.17 = f32[3,3,448,896]{1,0,2,3} copy(f32[3,3,448,896]{3,2,1,0} %param_135), metadata={op_name="0$start"}
  %cudnn-conv.16 = (f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,448]{2,1,3,0} %fusion.84, f32[3,3,448,896]{1,0,2,3} %copy.17), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.16 = f32[16,28,28,896]{2,1,3,0} get-tuple-element((f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.16), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.271 = f32[16,896,784]{2,1,0} bitcast(f32[16,28,28,896]{2,1,3,0} %get-tuple-element.16)
  %fusion.83 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,784]{2,1,0} %bitcast.271), kind=kInput, calls=%fused_computation.83.clone
  %get-tuple-element.154 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.83), index=0
  %reduce.228 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.154, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.155 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.83), index=1
  %param_32 = f32[896]{0} parameter(32), metadata={op_name="0$start"}
  %param_143 = f32[896]{0} parameter(143), metadata={op_name="0$start"}
  %fusion.78 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_33, f32[896]{0} %param_144, f32[896]{0} %reduce.228, f32[16,896]{1,0} %get-tuple-element.155, f32[896]{0} %param_32, /*index=5*/f32[896]{0} %param_143), kind=kLoop, calls=%fused_computation.78, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  %get-tuple-element.274 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.78), index=2
  %get-tuple-element.275 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.78), index=0
  %param_35 = f32[1792]{0} parameter(35), metadata={op_name="0$start"}
  %param_146 = f32[1792]{0} parameter(146), metadata={op_name="0$start"}
  %param_137 = f32[896]{0} parameter(137), metadata={op_name="0$start"}
  %param_136 = f32[896]{0} parameter(136), metadata={op_name="0$start"}
  %get-tuple-element.152 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.78), index=1
  %fusion.77 = f32[16,28,28,896]{2,1,3,0} fusion(f32[896]{0} %param_137, f32[896]{0} %param_136, f32[896]{0} %get-tuple-element.152, f32[16,28,28,896]{2,1,3,0} %get-tuple-element.16, f32[896]{0} %reduce.228), kind=kLoop, calls=%fused_computation.77, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %param_138 = f32[1,1,896,1792]{3,2,1,0} parameter(138), metadata={op_name="0$start"}
  %copy.18 = f32[1,1,896,1792]{1,0,2,3} copy(f32[1,1,896,1792]{3,2,1,0} %param_138), metadata={op_name="0$start"}
  %cudnn-conv.17 = (f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,896]{2,1,3,0} %fusion.77, f32[1,1,896,1792]{1,0,2,3} %copy.18), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.17 = f32[16,28,28,1792]{2,1,3,0} get-tuple-element((f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) %cudnn-conv.17), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.267 = f32[16,1792,784]{2,1,0} bitcast(f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.17)
  %fusion.76 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) fusion(f32[16,1792,784]{2,1,0} %bitcast.267), kind=kInput, calls=%fused_computation.76.clone
  %get-tuple-element.149 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.76), index=0
  %reduce.230 = f32[1792]{0} reduce(f32[16,1792]{1,0} %get-tuple-element.149, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/jvp(WideResNet)/BottleneckResNetBlock_4/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.150 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.76), index=1
  %param_34 = f32[1792]{0} parameter(34), metadata={op_name="0$start"}
  %param_145 = f32[1792]{0} parameter(145), metadata={op_name="0$start"}
  %fusion.71 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) fusion(f32[1792]{0} %param_35, f32[1792]{0} %param_146, f32[1792]{0} %reduce.230, f32[16,1792]{1,0} %get-tuple-element.150, f32[1792]{0} %param_34, /*index=5*/f32[1792]{0} %param_145), kind=kLoop, calls=%fused_computation.71, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(04)/add"}
  %get-tuple-element.276 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.71), index=2
  %get-tuple-element.277 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.71), index=0
  %param_37 = f32[448]{0} parameter(37), metadata={op_name="0$start"}
  %param_157 = f32[448]{0} parameter(157), metadata={op_name="0$start"}
  %param_140 = f32[1792]{0} parameter(140), metadata={op_name="0$start"}
  %param_139 = f32[1792]{0} parameter(139), metadata={op_name="0$start"}
  %get-tuple-element.147 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.71), index=1
  %fusion.69 = f32[16,28,28,1792]{2,1,3,0} fusion(f32[16,28,28,1792]{2,1,3,0} %fusion.91, f32[1792]{0} %param_140, f32[1792]{0} %param_139, f32[1792]{0} %get-tuple-element.147, f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.17, /*index=5*/f32[1792]{0} %reduce.230), kind=kLoop, calls=%fused_computation.69, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_4/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %param_147 = f32[1,1,1792,448]{3,2,1,0} parameter(147), metadata={op_name="0$start"}
  %copy.19 = f32[1,1,1792,448]{1,0,2,3} copy(f32[1,1,1792,448]{3,2,1,0} %param_147), metadata={op_name="0$start"}
  %cudnn-conv.18 = (f32[16,28,28,448]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,1792]{2,1,3,0} %fusion.69, f32[1,1,1792,448]{1,0,2,3} %copy.19), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.18 = f32[16,28,28,448]{2,1,3,0} get-tuple-element((f32[16,28,28,448]{2,1,3,0}, u8[0]{0}) %cudnn-conv.18), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.263 = f32[16,448,784]{2,1,0} bitcast(f32[16,28,28,448]{2,1,3,0} %get-tuple-element.18)
  %fusion.68 = (f32[16,448]{1,0}, f32[16,448]{1,0}) fusion(f32[16,448,784]{2,1,0} %bitcast.263), kind=kInput, calls=%fused_computation.68.clone
  %get-tuple-element.144 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.68), index=0
  %reduce.232 = f32[448]{0} reduce(f32[16,448]{1,0} %get-tuple-element.144, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.145 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.68), index=1
  %param_36 = f32[448]{0} parameter(36), metadata={op_name="0$start"}
  %param_156 = f32[448]{0} parameter(156), metadata={op_name="0$start"}
  %fusion.63 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) fusion(f32[448]{0} %param_37, f32[448]{0} %param_157, f32[448]{0} %reduce.232, f32[16,448]{1,0} %get-tuple-element.145, f32[448]{0} %param_36, /*index=5*/f32[448]{0} %param_156), kind=kLoop, calls=%fused_computation.63, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  %get-tuple-element.278 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.63), index=2
  %get-tuple-element.279 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.63), index=0
  %param_39 = f32[896]{0} parameter(39), metadata={op_name="0$start"}
  %param_159 = f32[896]{0} parameter(159), metadata={op_name="0$start"}
  %param_149 = f32[448]{0} parameter(149), metadata={op_name="0$start"}
  %param_148 = f32[448]{0} parameter(148), metadata={op_name="0$start"}
  %get-tuple-element.142 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.63), index=1
  %fusion.62 = f32[16,28,28,448]{2,1,3,0} fusion(f32[448]{0} %param_149, f32[448]{0} %param_148, f32[448]{0} %get-tuple-element.142, f32[16,28,28,448]{2,1,3,0} %get-tuple-element.18, f32[448]{0} %reduce.232), kind=kLoop, calls=%fused_computation.62, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  %param_150 = f32[3,3,448,896]{3,2,1,0} parameter(150), metadata={op_name="0$start"}
  %copy.20 = f32[3,3,448,896]{1,0,2,3} copy(f32[3,3,448,896]{3,2,1,0} %param_150), metadata={op_name="0$start"}
  %cudnn-conv.19 = (f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,448]{2,1,3,0} %fusion.62, f32[3,3,448,896]{1,0,2,3} %copy.20), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.19 = f32[16,28,28,896]{2,1,3,0} get-tuple-element((f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.19), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.259 = f32[16,896,784]{2,1,0} bitcast(f32[16,28,28,896]{2,1,3,0} %get-tuple-element.19)
  %fusion.61 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,784]{2,1,0} %bitcast.259), kind=kInput, calls=%fused_computation.61.clone
  %get-tuple-element.139 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.61), index=0
  %reduce.234 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.139, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.140 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.61), index=1
  %param_38 = f32[896]{0} parameter(38), metadata={op_name="0$start"}
  %param_158 = f32[896]{0} parameter(158), metadata={op_name="0$start"}
  %fusion.56 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_39, f32[896]{0} %param_159, f32[896]{0} %reduce.234, f32[16,896]{1,0} %get-tuple-element.140, f32[896]{0} %param_38, /*index=5*/f32[896]{0} %param_158), kind=kLoop, calls=%fused_computation.56, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  %get-tuple-element.280 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.56), index=2
  %get-tuple-element.281 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.56), index=0
  %param_41 = f32[1792]{0} parameter(41), metadata={op_name="0$start"}
  %param_161 = f32[1792]{0} parameter(161), metadata={op_name="0$start"}
  %param_152 = f32[896]{0} parameter(152), metadata={op_name="0$start"}
  %param_151 = f32[896]{0} parameter(151), metadata={op_name="0$start"}
  %get-tuple-element.137 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.56), index=1
  %fusion.55 = f32[16,28,28,896]{2,1,3,0} fusion(f32[896]{0} %param_152, f32[896]{0} %param_151, f32[896]{0} %get-tuple-element.137, f32[16,28,28,896]{2,1,3,0} %get-tuple-element.19, f32[896]{0} %reduce.234), kind=kLoop, calls=%fused_computation.55, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %param_153 = f32[1,1,896,1792]{3,2,1,0} parameter(153), metadata={op_name="0$start"}
  %copy.21 = f32[1,1,896,1792]{1,0,2,3} copy(f32[1,1,896,1792]{3,2,1,0} %param_153), metadata={op_name="0$start"}
  %cudnn-conv.20 = (f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,896]{2,1,3,0} %fusion.55, f32[1,1,896,1792]{1,0,2,3} %copy.21), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.20 = f32[16,28,28,1792]{2,1,3,0} get-tuple-element((f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) %cudnn-conv.20), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.255 = f32[16,1792,784]{2,1,0} bitcast(f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.20)
  %fusion.54 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) fusion(f32[16,1792,784]{2,1,0} %bitcast.255), kind=kInput, calls=%fused_computation.54.clone
  %get-tuple-element.134 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.54), index=0
  %reduce.236 = f32[1792]{0} reduce(f32[16,1792]{1,0} %get-tuple-element.134, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.135 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.54), index=1
  %param_40 = f32[1792]{0} parameter(40), metadata={op_name="0$start"}
  %param_160 = f32[1792]{0} parameter(160), metadata={op_name="0$start"}
  %fusion.49 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) fusion(f32[1792]{0} %param_41, f32[1792]{0} %param_161, f32[1792]{0} %reduce.236, f32[16,1792]{1,0} %get-tuple-element.135, f32[1792]{0} %param_40, /*index=5*/f32[1792]{0} %param_160), kind=kLoop, calls=%fused_computation.49, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/add"}
  %get-tuple-element.282 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.49), index=2
  %get-tuple-element.283 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.49), index=0
  %param_43 = f32[448]{0} parameter(43), metadata={op_name="0$start"}
  %param_172 = f32[448]{0} parameter(172), metadata={op_name="0$start"}
  %param_155 = f32[1792]{0} parameter(155), metadata={op_name="0$start"}
  %param_154 = f32[1792]{0} parameter(154), metadata={op_name="0$start"}
  %get-tuple-element.132 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.49), index=1
  %fusion.48 = f32[16,28,28,1792]{2,1,3,0} fusion(f32[16,28,28,1792]{2,1,3,0} %fusion.69, f32[1792]{0} %param_155, f32[1792]{0} %param_154, f32[1792]{0} %get-tuple-element.132, f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.20, /*index=5*/f32[1792]{0} %reduce.236), kind=kLoop, calls=%fused_computation.48, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(05)/jvp(WideResNet)/BottleneckResNetBlock_5/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=98}
  %param_162 = f32[1,1,1792,448]{3,2,1,0} parameter(162), metadata={op_name="0$start"}
  %copy.22 = f32[1,1,1792,448]{1,0,2,3} copy(f32[1,1,1792,448]{3,2,1,0} %param_162), metadata={op_name="0$start"}
  %cudnn-conv.21 = (f32[16,28,28,448]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,1792]{2,1,3,0} %fusion.48, f32[1,1,1792,448]{1,0,2,3} %copy.22), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.21 = f32[16,28,28,448]{2,1,3,0} get-tuple-element((f32[16,28,28,448]{2,1,3,0}, u8[0]{0}) %cudnn-conv.21), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 448) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.251 = f32[16,448,784]{2,1,0} bitcast(f32[16,28,28,448]{2,1,3,0} %get-tuple-element.21)
  %fusion.47 = (f32[16,448]{1,0}, f32[16,448]{1,0}) fusion(f32[16,448,784]{2,1,0} %bitcast.251), kind=kInput, calls=%fused_computation.47.clone
  %get-tuple-element.128 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.47), index=0
  %reduce.238 = f32[448]{0} reduce(f32[16,448]{1,0} %get-tuple-element.128, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.129 = f32[16,448]{1,0} get-tuple-element((f32[16,448]{1,0}, f32[16,448]{1,0}) %fusion.47), index=1
  %param_42 = f32[448]{0} parameter(42), metadata={op_name="0$start"}
  %param_171 = f32[448]{0} parameter(171), metadata={op_name="0$start"}
  %fusion.42 = (f32[448]{0}, f32[448]{0}, f32[448]{0}) fusion(f32[448]{0} %param_43, f32[448]{0} %param_172, f32[448]{0} %reduce.238, f32[16,448]{1,0} %get-tuple-element.129, f32[448]{0} %param_42, /*index=5*/f32[448]{0} %param_171), kind=kLoop, calls=%fused_computation.42, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  %get-tuple-element.284 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.42), index=2
  %get-tuple-element.285 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.42), index=0
  %param_45 = f32[896]{0} parameter(45), metadata={op_name="0$start"}
  %param_174 = f32[896]{0} parameter(174), metadata={op_name="0$start"}
  %param_164 = f32[448]{0} parameter(164), metadata={op_name="0$start"}
  %param_163 = f32[448]{0} parameter(163), metadata={op_name="0$start"}
  %get-tuple-element.126 = f32[448]{0} get-tuple-element((f32[448]{0}, f32[448]{0}, f32[448]{0}) %fusion.42), index=1
  %fusion.41 = f32[16,28,28,448]{2,1,3,0} fusion(f32[448]{0} %param_164, f32[448]{0} %param_163, f32[448]{0} %get-tuple-element.126, f32[16,28,28,448]{2,1,3,0} %get-tuple-element.21, f32[448]{0} %reduce.238), kind=kLoop, calls=%fused_computation.41, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  %param_165 = f32[3,3,448,896]{3,2,1,0} parameter(165), metadata={op_name="0$start"}
  %copy.23 = f32[3,3,448,896]{1,0,2,3} copy(f32[3,3,448,896]{3,2,1,0} %param_165), metadata={op_name="0$start"}
  %cudnn-conv.22 = (f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,448]{2,1,3,0} %fusion.41, f32[3,3,448,896]{1,0,2,3} %copy.23), window={size=3x3 pad=1_1x1_1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.22 = f32[16,28,28,896]{2,1,3,0} get-tuple-element((f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.22), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_1/conv_general_dilated[window_strides=(1, 1) padding=((1, 1), (1, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 448) rhs_shape=(3, 3, 448, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.247 = f32[16,896,784]{2,1,0} bitcast(f32[16,28,28,896]{2,1,3,0} %get-tuple-element.22)
  %fusion.40 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,784]{2,1,0} %bitcast.247), kind=kInput, calls=%fused_computation.40.clone
  %get-tuple-element.123 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.40), index=0
  %reduce.240 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.123, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.124 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.40), index=1
  %param_44 = f32[896]{0} parameter(44), metadata={op_name="0$start"}
  %param_173 = f32[896]{0} parameter(173), metadata={op_name="0$start"}
  %fusion.35 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_45, f32[896]{0} %param_174, f32[896]{0} %reduce.240, f32[16,896]{1,0} %get-tuple-element.124, f32[896]{0} %param_44, /*index=5*/f32[896]{0} %param_173), kind=kLoop, calls=%fused_computation.35, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  %get-tuple-element.286 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.35), index=2
  %get-tuple-element.287 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.35), index=0
  %param_47 = f32[1792]{0} parameter(47), metadata={op_name="0$start"}
  %param_176 = f32[1792]{0} parameter(176), metadata={op_name="0$start"}
  %param_167 = f32[896]{0} parameter(167), metadata={op_name="0$start"}
  %param_166 = f32[896]{0} parameter(166), metadata={op_name="0$start"}
  %get-tuple-element.121 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.35), index=1
  %fusion.34 = f32[16,28,28,896]{2,1,3,0} fusion(f32[896]{0} %param_167, f32[896]{0} %param_166, f32[896]{0} %get-tuple-element.121, f32[16,28,28,896]{2,1,3,0} %get-tuple-element.22, f32[896]{0} %reduce.240), kind=kLoop, calls=%fused_computation.34, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %param_168 = f32[1,1,896,1792]{3,2,1,0} parameter(168), metadata={op_name="0$start"}
  %copy.24 = f32[1,1,896,1792]{1,0,2,3} copy(f32[1,1,896,1792]{3,2,1,0} %param_168), metadata={op_name="0$start"}
  %cudnn-conv.23 = (f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,896]{2,1,3,0} %fusion.34, f32[1,1,896,1792]{1,0,2,3} %copy.24), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.23 = f32[16,28,28,1792]{2,1,3,0} get-tuple-element((f32[16,28,28,1792]{2,1,3,0}, u8[0]{0}) %cudnn-conv.23), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(1, 1, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.243 = f32[16,1792,784]{2,1,0} bitcast(f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.23)
  %fusion.33 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) fusion(f32[16,1792,784]{2,1,0} %bitcast.243), kind=kInput, calls=%fused_computation.33.clone
  %get-tuple-element.118 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.33), index=0
  %reduce.242 = f32[1792]{0} reduce(f32[16,1792]{1,0} %get-tuple-element.118, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/jvp(WideResNet)/BottleneckResNetBlock_6/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.119 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.33), index=1
  %param_46 = f32[1792]{0} parameter(46), metadata={op_name="0$start"}
  %param_175 = f32[1792]{0} parameter(175), metadata={op_name="0$start"}
  %fusion.28 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) fusion(f32[1792]{0} %param_47, f32[1792]{0} %param_176, f32[1792]{0} %reduce.242, f32[16,1792]{1,0} %get-tuple-element.119, f32[1792]{0} %param_46, /*index=5*/f32[1792]{0} %param_175), kind=kLoop, calls=%fused_computation.28, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(06)/add"}
  %get-tuple-element.288 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.28), index=2
  %get-tuple-element.289 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.28), index=0
  %param_49 = f32[896]{0} parameter(49), metadata={op_name="0$start"}
  %param_190 = f32[896]{0} parameter(190), metadata={op_name="0$start"}
  %param_170 = f32[1792]{0} parameter(170), metadata={op_name="0$start"}
  %param_169 = f32[1792]{0} parameter(169), metadata={op_name="0$start"}
  %get-tuple-element.116 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.28), index=1
  %fusion.200 = (f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{2,1,3,0}, f32[16,28,28,1792]{3,2,1,0}) fusion(f32[16,28,28,1792]{2,1,3,0} %fusion.48, f32[1792]{0} %param_170, f32[1792]{0} %param_169, f32[1792]{0} %get-tuple-element.116, f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.23, /*index=5*/f32[1792]{0} %reduce.242), kind=kInput, calls=%fused_computation.200, metadata={op_name="tuple.66"}
  %get-tuple-element.114 = f32[16,28,28,1792]{2,1,3,0} get-tuple-element((f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{2,1,3,0}, f32[16,28,28,1792]{3,2,1,0}) %fusion.200), index=1
  %param_177 = f32[1,1,1792,896]{3,2,1,0} parameter(177), metadata={op_name="0$start"}
  %copy.25 = f32[1,1,1792,896]{1,0,2,3} copy(f32[1,1,1792,896]{3,2,1,0} %param_177), metadata={op_name="0$start"}
  %cudnn-conv.25 = (f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.114, f32[1,1,1792,896]{1,0,2,3} %copy.25), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.25 = f32[16,28,28,896]{2,1,3,0} get-tuple-element((f32[16,28,28,896]{2,1,3,0}, u8[0]{0}) %cudnn-conv.25), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_0/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 896) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.239 = f32[16,896,784]{2,1,0} bitcast(f32[16,28,28,896]{2,1,3,0} %get-tuple-element.25)
  %fusion.26 = (f32[16,896]{1,0}, f32[16,896]{1,0}) fusion(f32[16,896,784]{2,1,0} %bitcast.239), kind=kInput, calls=%fused_computation.26.clone
  %get-tuple-element.111 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.26), index=0
  %reduce.246 = f32[896]{0} reduce(f32[16,896]{1,0} %get-tuple-element.111, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_0/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.112 = f32[16,896]{1,0} get-tuple-element((f32[16,896]{1,0}, f32[16,896]{1,0}) %fusion.26), index=1
  %param_48 = f32[896]{0} parameter(48), metadata={op_name="0$start"}
  %param_189 = f32[896]{0} parameter(189), metadata={op_name="0$start"}
  %fusion.21 = (f32[896]{0}, f32[896]{0}, f32[896]{0}) fusion(f32[896]{0} %param_49, f32[896]{0} %param_190, f32[896]{0} %reduce.246, f32[16,896]{1,0} %get-tuple-element.112, f32[896]{0} %param_48, /*index=5*/f32[896]{0} %param_189), kind=kLoop, calls=%fused_computation.21, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  %get-tuple-element.290 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.21), index=2
  %get-tuple-element.291 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.21), index=0
  %param_51 = f32[1792]{0} parameter(51), metadata={op_name="0$start"}
  %param_192 = f32[1792]{0} parameter(192), metadata={op_name="0$start"}
  %param_179 = f32[896]{0} parameter(179), metadata={op_name="0$start"}
  %param_178 = f32[896]{0} parameter(178), metadata={op_name="0$start"}
  %get-tuple-element.109 = f32[896]{0} get-tuple-element((f32[896]{0}, f32[896]{0}, f32[896]{0}) %fusion.21), index=1
  %fusion.20 = f32[16,29,29,896]{2,1,3,0} fusion(f32[896]{0} %param_179, f32[896]{0} %param_178, f32[896]{0} %get-tuple-element.109, f32[16,28,28,896]{2,1,3,0} %get-tuple-element.25, f32[896]{0} %reduce.246), kind=kLoop, calls=%fused_computation.20, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=85}
  %param_180 = f32[3,3,896,1792]{3,2,1,0} parameter(180), metadata={op_name="0$start"}
  %copy.26 = f32[3,3,896,1792]{1,0,2,3} copy(f32[3,3,896,1792]{3,2,1,0} %param_180), metadata={op_name="0$start"}
  %cudnn-conv.26 = (f32[16,14,14,1792]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,29,29,896]{2,1,3,0} %fusion.20, f32[3,3,896,1792]{1,0,2,3} %copy.26), window={size=3x3 stride=2x2}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_1/conv_general_dilated[window_strides=(2, 2) padding=((0, 1), (0, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(3, 3, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.26 = f32[16,14,14,1792]{2,1,3,0} get-tuple-element((f32[16,14,14,1792]{2,1,3,0}, u8[0]{0}) %cudnn-conv.26), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_1/conv_general_dilated[window_strides=(2, 2) padding=((0, 1), (0, 1)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 896) rhs_shape=(3, 3, 896, 1792) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.235 = f32[16,1792,196]{2,1,0} bitcast(f32[16,14,14,1792]{2,1,3,0} %get-tuple-element.26)
  %fusion.19 = (f32[16,1792]{1,0}, f32[16,1792]{1,0}) fusion(f32[16,1792,196]{2,1,0} %bitcast.235), kind=kInput, calls=%fused_computation.19.clone
  %get-tuple-element.106 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.19), index=0
  %reduce.248 = f32[1792]{0} reduce(f32[16,1792]{1,0} %get-tuple-element.106, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_1/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.107 = f32[16,1792]{1,0} get-tuple-element((f32[16,1792]{1,0}, f32[16,1792]{1,0}) %fusion.19), index=1
  %param_50 = f32[1792]{0} parameter(50), metadata={op_name="0$start"}
  %param_191 = f32[1792]{0} parameter(191), metadata={op_name="0$start"}
  %fusion.14 = (f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) fusion(f32[1792]{0} %param_51, f32[1792]{0} %param_192, f32[1792]{0} %reduce.248, f32[16,1792]{1,0} %get-tuple-element.107, f32[1792]{0} %param_50, /*index=5*/f32[1792]{0} %param_191), kind=kLoop, calls=%fused_computation.14, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  %get-tuple-element.292 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.14), index=2
  %get-tuple-element.358 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.14), index=0
  %param_53 = f32[3584]{0} parameter(53), metadata={op_name="0$start"}
  %param_194 = f32[3584]{0} parameter(194), metadata={op_name="0$start"}
  %param_182 = f32[1792]{0} parameter(182), metadata={op_name="0$start"}
  %param_181 = f32[1792]{0} parameter(181), metadata={op_name="0$start"}
  %get-tuple-element.104 = f32[1792]{0} get-tuple-element((f32[1792]{0}, f32[1792]{0}, f32[1792]{0}) %fusion.14), index=1
  %fusion.13 = f32[16,14,14,1792]{2,1,3,0} fusion(f32[1792]{0} %param_182, f32[1792]{0} %param_181, f32[1792]{0} %get-tuple-element.104, f32[16,14,14,1792]{2,1,3,0} %get-tuple-element.26, f32[1792]{0} %reduce.248), kind=kLoop, calls=%fused_computation.13, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/max" source_file="/home/cyxue/Projects/crius/Crius/runtime/crius_worker/jax/model/wide_resnet.py" source_line=88}
  %param_183 = f32[1,1,1792,3584]{3,2,1,0} parameter(183), metadata={op_name="0$start"}
  %copy.27 = f32[1,1,1792,3584]{1,0,2,3} copy(f32[1,1,1792,3584]{3,2,1,0} %param_183), metadata={op_name="0$start"}
  %cudnn-conv.27 = (f32[16,14,14,3584]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,14,14,1792]{2,1,3,0} %fusion.13, f32[1,1,1792,3584]{1,0,2,3} %copy.27), window={size=1x1}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 14, 14, 1792) rhs_shape=(1, 1, 1792, 3584) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.27 = f32[16,14,14,3584]{2,1,3,0} get-tuple-element((f32[16,14,14,3584]{2,1,3,0}, u8[0]{0}) %cudnn-conv.27), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/Conv_2/conv_general_dilated[window_strides=(1, 1) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 14, 14, 1792) rhs_shape=(1, 1, 1792, 3584) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.231 = f32[16,3584,196]{2,1,0} bitcast(f32[16,14,14,3584]{2,1,3,0} %get-tuple-element.27)
  %fusion.12 = (f32[16,3584]{1,0}, f32[16,3584]{1,0}) fusion(f32[16,3584,196]{2,1,0} %bitcast.231), kind=kInput, calls=%fused_computation.12.clone
  %get-tuple-element.101 = f32[16,3584]{1,0} get-tuple-element((f32[16,3584]{1,0}, f32[16,3584]{1,0}) %fusion.12), index=0
  %reduce.250 = f32[3584]{0} reduce(f32[16,3584]{1,0} %get-tuple-element.101, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/BatchNorm_2/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.102 = f32[16,3584]{1,0} get-tuple-element((f32[16,3584]{1,0}, f32[16,3584]{1,0}) %fusion.12), index=1
  %param_52 = f32[3584]{0} parameter(52), metadata={op_name="0$start"}
  %param_193 = f32[3584]{0} parameter(193), metadata={op_name="0$start"}
  %fusion.7 = (f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) fusion(f32[3584]{0} %param_53, f32[3584]{0} %param_194, f32[3584]{0} %reduce.250, f32[16,3584]{1,0} %get-tuple-element.102, f32[3584]{0} %param_52, /*index=5*/f32[3584]{0} %param_193), kind=kLoop, calls=%fused_computation.7, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  %get-tuple-element.359 = f32[3584]{0} get-tuple-element((f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) %fusion.7), index=2
  %get-tuple-element.360 = f32[3584]{0} get-tuple-element((f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) %fusion.7), index=0
  %param_55 = f32[3584]{0} parameter(55), metadata={op_name="0$start"}
  %param_196 = f32[3584]{0} parameter(196), metadata={op_name="0$start"}
  %param_186 = f32[1,1,1792,3584]{3,2,1,0} parameter(186), metadata={op_name="0$start"}
  %copy.28 = f32[1,1,1792,3584]{1,0,2,3} copy(f32[1,1,1792,3584]{3,2,1,0} %param_186), metadata={op_name="0$start"}
  %cudnn-conv.24 = (f32[16,14,14,3584]{2,1,3,0}, u8[0]{0}) custom-call(f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.114, f32[1,1,1792,3584]{1,0,2,3} %copy.28), window={size=1x1 stride=2x2}, dim_labels=b01f_01io->b01f, custom_call_target="__cudnn$convForward", metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 3584) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}, backend_config="{\"conv_result_scale\":1,\"activation_mode\":\"0\",\"side_input_scale\":0}"
  %get-tuple-element.24 = f32[16,14,14,3584]{2,1,3,0} get-tuple-element((f32[16,14,14,3584]{2,1,3,0}, u8[0]{0}) %cudnn-conv.24), index=0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/conv_proj/conv_general_dilated[window_strides=(2, 2) padding=((0, 0), (0, 0)) lhs_dilation=(1, 1) rhs_dilation=(1, 1) dimension_numbers=ConvDimensionNumbers(lhs_spec=(0, 3, 1, 2), rhs_spec=(3, 2, 0, 1), out_spec=(0, 3, 1, 2)) feature_group_count=1 batch_group_count=1 lhs_shape=(16, 28, 28, 1792) rhs_shape=(1, 1, 1792, 3584) precision=None preferred_element_type=None]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/linear.py" source_line=435}
  %bitcast.227 = f32[16,3584,196]{2,1,0} bitcast(f32[16,14,14,3584]{2,1,3,0} %get-tuple-element.24)
  %fusion.6 = (f32[16,3584]{1,0}, f32[16,3584]{1,0}) fusion(f32[16,3584,196]{2,1,0} %bitcast.227), kind=kInput, calls=%fused_computation.6.clone
  %get-tuple-element.96 = f32[16,3584]{1,0} get-tuple-element((f32[16,3584]{1,0}, f32[16,3584]{1,0}) %fusion.6), index=0
  %reduce.244 = f32[3584]{0} reduce(f32[16,3584]{1,0} %get-tuple-element.96, f32[] %constant_519), dimensions={0}, to_apply=%region_0.1246.0, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/jvp(WideResNet)/BottleneckResNetBlock_7/norm_proj/reduce_sum[axes=(0, 1, 2)]" source_file="/home/cyxue/miniconda3/envs/alpa/lib/python3.8/site-packages/flax/linen/normalization.py" source_line=82}
  %get-tuple-element.97 = f32[16,3584]{1,0} get-tuple-element((f32[16,3584]{1,0}, f32[16,3584]{1,0}) %fusion.6), index=1
  %param_54 = f32[3584]{0} parameter(54), metadata={op_name="0$start"}
  %param_195 = f32[3584]{0} parameter(195), metadata={op_name="0$start"}
  %fusion.1 = (f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) fusion(f32[3584]{0} %param_55, f32[3584]{0} %param_196, f32[3584]{0} %reduce.244, f32[16,3584]{1,0} %get-tuple-element.97, f32[3584]{0} %param_54, /*index=5*/f32[3584]{0} %param_195), kind=kLoop, calls=%fused_computation.1, metadata={op_name="parallelize(train_step_func_pipeshard_parallel_mesh_0)/jit(main)/jit(0)/jit(07)/add"}
  %get-tuple-element.361 = f32[3584]{0} get-tuple-element((f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) %fusion.1), index=2
  %get-tuple-element.362 = f32[3584]{0} get-tuple-element((f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) %fusion.1), index=0
  %get-tuple-element.363 = f32[16,56,56,448]{3,2,1,0} get-tuple-element((f32[16,56,56,448]{3,2,1,0}, f32[16,56,56,448]{2,1,3,0}) %fusion.202), index=0
  %get-tuple-element.364 = f32[16,56,56,224]{3,2,1,0} get-tuple-element((f32[16,56,56,224]{3,2,1,0}, f32[16,56,56,224]{2,1,3,0}) %fusion.203), index=0
  %fusion.197 = f32[16,56,56,896]{3,2,1,0} fusion(f32[16,56,56,896]{2,1,3,0} %fusion.162, f32[896]{0} %param_86, f32[896]{0} %param_85, f32[896]{0} %get-tuple-element.199, f32[16,56,56,896]{2,1,3,0} %get-tuple-element.7, /*index=5*/f32[896]{0} %reduce.210), kind=kInput, calls=%fused_computation.197, metadata={op_name="tuple.66"}
  %get-tuple-element.366 = f32[16,56,56,896]{3,2,1,0} get-tuple-element((f32[16,56,56,896]{3,2,1,0}, f32[16,56,56,896]{2,1,3,0}) %fusion.201), index=0
  %fusion.198 = f32[16,28,28,1792]{3,2,1,0} fusion(f32[1792]{0} %param_120, f32[1792]{0} %param_119, f32[1792]{0} %get-tuple-element.167, f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.14, f32[1792]{0} %reduce.224, /*index=5*/f32[1792]{0} %param_123, f32[1792]{0} %param_122, f32[1792]{0} %get-tuple-element.162, f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.11, f32[1792]{0} %reduce.218), kind=kInput, calls=%fused_computation.198, metadata={op_name="tuple.66"}
  %fusion.199 = f32[16,28,28,1792]{3,2,1,0} fusion(f32[16,28,28,1792]{2,1,3,0} %fusion.91, f32[1792]{0} %param_140, f32[1792]{0} %param_139, f32[1792]{0} %get-tuple-element.147, f32[16,28,28,1792]{2,1,3,0} %get-tuple-element.17, /*index=5*/f32[1792]{0} %reduce.230), kind=kInput, calls=%fused_computation.199, metadata={op_name="tuple.66"}
  %get-tuple-element.369 = f32[16,28,28,1792]{3,2,1,0} get-tuple-element((f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{2,1,3,0}, f32[16,28,28,1792]{3,2,1,0}) %fusion.200), index=2
  %get-tuple-element.370 = f32[16,28,28,1792]{3,2,1,0} get-tuple-element((f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{2,1,3,0}, f32[16,28,28,1792]{3,2,1,0}) %fusion.200), index=0
  %param_185 = f32[3584]{0} parameter(185), metadata={op_name="0$start"}
  %param_184 = f32[3584]{0} parameter(184), metadata={op_name="0$start"}
  %get-tuple-element.99 = f32[3584]{0} get-tuple-element((f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) %fusion.7), index=1
  %param_188 = f32[3584]{0} parameter(188), metadata={op_name="0$start"}
  %param_187 = f32[3584]{0} parameter(187), metadata={op_name="0$start"}
  %get-tuple-element.94 = f32[3584]{0} get-tuple-element((f32[3584]{0}, f32[3584]{0}, f32[3584]{0}) %fusion.1), index=1
  %fusion = f32[16,14,14,3584]{3,2,1,0} fusion(f32[3584]{0} %param_185, f32[3584]{0} %param_184, f32[3584]{0} %get-tuple-element.99, f32[16,14,14,3584]{2,1,3,0} %get-tuple-element.27, f32[3584]{0} %param_188, /*index=5*/f32[3584]{0} %param_187, f32[3584]{0} %get-tuple-element.94, f32[16,14,14,3584]{2,1,3,0} %get-tuple-element.24, f32[3584]{0} %reduce.250, f32[3584]{0} %reduce.244), kind=kInput, calls=%fused_computation, metadata={op_name="tuple.66"}
  ROOT %tuple.129 = (f32[224]{0}, f32[224]{0}, f32[224]{0}, f32[224]{0}, f32[448]{0}, /*index=5*/f32[448]{0}, f32[896]{0}, f32[896]{0}, f32[896]{0}, f32[896]{0}, /*index=10*/f32[224]{0}, f32[224]{0}, f32[448]{0}, f32[448]{0}, f32[896]{0}, /*index=15*/f32[896]{0}, f32[224]{0}, f32[224]{0}, f32[448]{0}, f32[448]{0}, /*index=20*/f32[896]{0}, f32[896]{0}, f32[448]{0}, f32[448]{0}, f32[896]{0}, /*index=25*/f32[896]{0}, f32[1792]{0}, f32[1792]{0}, f32[1792]{0}, f32[1792]{0}, /*index=30*/f32[448]{0}, f32[448]{0}, f32[896]{0}, f32[896]{0}, f32[1792]{0}, /*index=35*/f32[1792]{0}, f32[448]{0}, f32[448]{0}, f32[896]{0}, f32[896]{0}, /*index=40*/f32[1792]{0}, f32[1792]{0}, f32[448]{0}, f32[448]{0}, f32[896]{0}, /*index=45*/f32[896]{0}, f32[1792]{0}, f32[1792]{0}, f32[896]{0}, f32[896]{0}, /*index=50*/f32[1792]{0}, f32[1792]{0}, f32[3584]{0}, f32[3584]{0}, f32[3584]{0}, /*index=55*/f32[3584]{0}, f32[16,56,56,448]{3,2,1,0}, f32[16,56,56,224]{3,2,1,0}, f32[16,56,56,896]{3,2,1,0}, f32[16,56,56,896]{3,2,1,0}, /*index=60*/f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{3,2,1,0}, f32[16,28,28,1792]{3,2,1,0}, f32[16,14,14,3584]{3,2,1,0}) tuple(f32[224]{0} %get-tuple-element.242, f32[224]{0} %get-tuple-element.243, f32[224]{0} %get-tuple-element.244, f32[224]{0} %get-tuple-element.245, f32[448]{0} %get-tuple-element.246, /*index=5*/f32[448]{0} %get-tuple-element.247, f32[896]{0} %get-tuple-element.248, f32[896]{0} %get-tuple-element.249, f32[896]{0} %get-tuple-element.250, f32[896]{0} %get-tuple-element.251, /*index=10*/f32[224]{0} %get-tuple-element.252, f32[224]{0} %get-tuple-element.253, f32[448]{0} %get-tuple-element.254, f32[448]{0} %get-tuple-element.255, f32[896]{0} %get-tuple-element.256, /*index=15*/f32[896]{0} %get-tuple-element.257, f32[224]{0} %get-tuple-element.258, f32[224]{0} %get-tuple-element.259, f32[448]{0} %get-tuple-element.260, f32[448]{0} %get-tuple-element.261, /*index=20*/f32[896]{0} %get-tuple-element.262, f32[896]{0} %get-tuple-element.263, f32[448]{0} %get-tuple-element.264, f32[448]{0} %get-tuple-element.265, f32[896]{0} %get-tuple-element.266, /*index=25*/f32[896]{0} %get-tuple-element.267, f32[1792]{0} %get-tuple-element.268, f32[1792]{0} %get-tuple-element.269, f32[1792]{0} %get-tuple-element.270, f32[1792]{0} %get-tuple-element.271, /*index=30*/f32[448]{0} %get-tuple-element.272, f32[448]{0} %get-tuple-element.273, f32[896]{0} %get-tuple-element.274, f32[896]{0} %get-tuple-element.275, f32[1792]{0} %get-tuple-element.276, /*index=35*/f32[1792]{0} %get-tuple-element.277, f32[448]{0} %get-tuple-element.278, f32[448]{0} %get-tuple-element.279, f32[896]{0} %get-tuple-element.280, f32[896]{0} %get-tuple-element.281, /*index=40*/f32[1792]{0} %get-tuple-element.282, f32[1792]{0} %get-tuple-element.283, f32[448]{0} %get-tuple-element.284, f32[448]{0} %get-tuple-element.285, f32[896]{0} %get-tuple-element.286, /*index=45*/f32[896]{0} %get-tuple-element.287, f32[1792]{0} %get-tuple-element.288, f32[1792]{0} %get-tuple-element.289, f32[896]{0} %get-tuple-element.290, f32[896]{0} %get-tuple-element.291, /*index=50*/f32[1792]{0} %get-tuple-element.292, f32[1792]{0} %get-tuple-element.358, f32[3584]{0} %get-tuple-element.359, f32[3584]{0} %get-tuple-element.360, f32[3584]{0} %get-tuple-element.361, /*index=55*/f32[3584]{0} %get-tuple-element.362, f32[16,56,56,448]{3,2,1,0} %get-tuple-element.363, f32[16,56,56,224]{3,2,1,0} %get-tuple-element.364, f32[16,56,56,896]{3,2,1,0} %fusion.197, f32[16,56,56,896]{3,2,1,0} %get-tuple-element.366, /*index=60*/f32[16,28,28,1792]{3,2,1,0} %fusion.198, f32[16,28,28,1792]{3,2,1,0} %fusion.199, f32[16,28,28,1792]{3,2,1,0} %get-tuple-element.369, f32[16,28,28,1792]{3,2,1,0} %get-tuple-element.370, f32[16,14,14,3584]{3,2,1,0} %fusion)
}

