#!/usr/bin/env python3
# -*- coding:utf-8 -*-
# Author: Chunyu Xue

"""
A script related to the crius runtime to runtime orchestrate and dispatch training jobs onto hosts 
in the physical cluster. 
"""

import uuid
import argparse
import json
import traceback
import numpy as np
import requests
from time import sleep
from typing import Sequence
from collections import namedtuple

# Back to upper dir
import os
import sys
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from resources.resource_abs import GPU, Node
from resources.hardware_specs import (
    NODE_CAPACITY, gpu_specs_suite, supported_model_cfgs, ray_port_option_list)
from job.job import Job, ResourceQuota
from trace_manager import TraceManager, ArrivalEvent
from job_parallel_tuner import ParallelTuner
from scheduler import (
    Scheduler, AblationOptions)
from baselines.fcfs_sched import FCFSSched
from baselines.elasticflow_sched import ElasticFlowSched
from baselines.gandiva_sched import GandivaSched
from baselines.gavel_sched import GavelSched
from baselines.sia_sched import SiaSched
from runtime.data.plot.plotter import (
    plot_cluster_thr, plot_jct, plot_queuing_time)
from utils import (
    read_json_content, translate_namedtuple_to_dict, 
    save_as_json, deepcopy)
from macro.macro_def import (
    LOCAL_ETA, GLOBAL_ETA, MAX_RETRY_TIMES, LOOSEN_DEADLINE, PREC, IDLE_STATUS, 
    USED_STATUS, IS_DISPATCH, IS_SUSPEND, IS_CHECK, EMPTY_JOB_ID, FAKE_JOB_ID, 
    RUNTIME_SCHEDULING_INTERVAL, RUNTIME_CHECK_INTERVAL, EMPTY_JOB_ALIAS, 
    IS_FORCE_DP, IS_FORCE_PP, IS_REC_STAGE_NUM, IS_END, IS_ERROR, 
    JOB_SUBMITTED_STATUS, LOOSEN_DEADLINE, JOB_RUNNING_STATUS, JOB_COMPLETED_STATUS,
    RESCHED_OVERHEAD_WITHOUT_PRUNE_RT, RESCHED_OVERHEAD_WITH_PRUNE,
    MAX_RESCHED_OVERHEAD_WITH_PRUNE, MAX_RESCHED_OVERHEAD_WITHOUT_PRUNE_RT)

# Args 
parser = argparse.ArgumentParser()
parser.add_argument("--policy", default="crius", type=str)
parser.add_argument("--trace_name", default="dummy_runtime_trace.csv", type=str)
parser.add_argument("--overwrite_net_if", default="none", type=str)
parser.add_argument("--max_sched_round", default=-1, type=int)
parser.add_argument("--enable_alpa", default=False, action='store_true', 
                    help="Whether to enable alpa search for the current policy.")
parser.add_argument("--visual", default=False, action="store_true", 
                    help="Visualize the runtime scheduling results.")
parser.add_argument("--visualized_metric", default="thr", type=str)
parser.add_argument("--plot_dir", default="plot", type=str)
parser.add_argument("--dummy_test", default=False, action='store_true', 
                    help="Run dummy test.")
parser.add_argument("--rt_dummy_test", default=False, action='store_true', 
                    help="Run rumtime dummy test on single host.")
parser.add_argument("--verbose", default=False, action="store_true", 
                    help="Run the test within debug mode.")
parser.add_argument("--sched_with_opt", default=False, action='store_true', 
                    help="Run scheduler and make scheduling decisions with the " + 
                         "throughput of optimal parallelism.")
args = parser.parse_args()

# Max check times before start to execution
MAX_CHECK_RESPONSE_TIMES=150


"""
A collection of single runtime operation.
"""
RuntimeOp = namedtuple("RuntimeOp", [
    "type", "job_id", "node_id_list", 
])

"""
A collection of runtime end event.
"""
RTEndEvent = namedtuple("RTEndEvent", [
    "type", "job_id", "job_alias", "model_name", "batch_size", "try_idx", "timestamp", 
    "node_id_list", "node_alias_list", "gpu_type", "gpu_num", "locality", 
    "executed_iter_num", "remained_iter_num", "avg_iter_time", "compilation_time"
])

# Current absolute path of this script
CUR_PATH = os.path.dirname(os.path.abspath(__file__))


class CriusRuntime:
    """
    The class of crius runtime, which runtime orchestrates training jobs based on 
    the scheduling decisions generated by crius scheduler. 
    """
    def __init__(self):
        # Global environmental variables
        os.environ["CRIUS_PROF_DB_PATH"] = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "../database/prof_database.pkl"
        )
        os.environ["CRIUS_TRACE_DIR"] = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), "../traces"
        )
        os.environ["SCHED_POLICY"] = str(args.policy)
        
        # Logical cluster resources
        self.existed_gpu_type = list()
        node_pool = self._construct_cluster(self._load_node_configs())
        
        # Trace manager
        self.trace_manager = TraceManager()
        
        # Job parallel tuner
        self.tuner = ParallelTuner()
        
        # Scheduler
        if args.policy == "crius":
            # Crius scheduler
            self.scheduler = Scheduler(node_pool, AblationOptions(), 
                                       is_runtime=True,
                                       verbose=args.verbose, 
                                       dummy_test=args.dummy_test,
                                       sched_with_opt=args.sched_with_opt)
        elif args.policy.split("-")[0] == "fcfs":
            # First-come-first-serve series
            self.scheduler = FCFSSched(node_pool, is_allow_relaxed=(args.policy == "fcfs-r"), 
                                       enable_alpa=args.enable_alpa, 
                                       is_runtime=True,
                                       verbose=args.verbose, 
                                       dummy_test=args.dummy_test, 
                                       sched_with_opt=args.sched_with_opt)
        elif args.policy == "gandiva":
            # Gandiva scheduler
            self.scheduler = GandivaSched(node_pool, enable_alpa=args.enable_alpa, 
                                          is_runtime=True, verbose=args.verbose, 
                                          dummy_test=args.dummy_test, 
                                          sched_with_opt=args.sched_with_opt)
        elif args.policy.split("-")[0] == "elasticflow":
            # Elasticflow series
            self.scheduler = ElasticFlowSched(node_pool, enable_alpa=args.enable_alpa, 
                                              is_runtime=True, verbose=args.verbose, 
                                              dummy_test=args.dummy_test, 
                                              sched_with_opt=args.sched_with_opt)
        elif args.policy == "gavel":
            # Gavel scheduler
            self.scheduler = GavelSched(node_pool, enable_alpa=args.enable_alpa, 
                                        is_runtime=True, verbose=args.verbose, 
                                        dummy_test=args.dummy_test, 
                                        sched_with_opt=args.sched_with_opt)
        elif args.policy == "sia":
            # Sia scheduler
            self.scheduler = SiaSched(node_pool, enable_alpa=args.enable_alpa, 
                                        is_runtime=True, verbose=args.verbose, 
                                        dummy_test=args.dummy_test, 
                                        sched_with_opt=args.sched_with_opt)
        else:
            raise NotImplementedError()

        self.end_event_stream = list()                              # Ended event and related performance
        self.rt_global_timer = None                                 # Runtime global timer
        
        # Runtime tables
        self.remained_iter_num_table = dict()                       # Remained iter num of each job                      
        self.job_try_idx_table = dict()                             # Try idx of each job
        self.return_msg_table = dict()                              # Return msg of each operation
        # self.global_perform_table = dict()                        # Iter num and related avg iter time of each job in each try
        self.job_head_node_table = dict()                           # Head node id of each job
        self.job_worker_nodes_table = dict()                        # Worker node ids of each job
        self.job_placement_table = dict()                           # Record the job placement info in the previous round, 
                                                                    # Format: job uuid -> node uuid -> [GPU rank, ...]
        self.job_max_check_times_table = dict()                     # Format: job uuid -> check times without execution
        self.resource_alloc_status = \
            self._init_resource_alloc_status()                      # Runtime GPU allocation status, node uuid -> GPU rank (e.g., 0)
        self.prev_crs_table = \
            self.scheduler.resource_interface.get_crs_table()       # The crs table object in the previous scheduling round
        self.prev_iter_time_table = dict()                          # Iteration time of each job in the previous scheduling round
        self.port_alloc_status = \
            self._init_port_alloc_status()                          # Runtime ray port allocation status of each node, 
                                                                    # Format: node uuid -> port option index
        self.ended_job_info_table = dict()                          # Ended job in this round, job uuid -> job alias
        self.resubmitted_job_id_list = list()                       # Resubmitted job id list due to runtime failure, we 
                                                                    # force them to run without pruning.
        # Metrics
        self.thr_list = list()                                      # Cluster throughput list
        self.jct_table = dict()                                     # JCT table
        self.queuing_time_table = dict()                            # Queuing time table

        # Runtime dummy test
        self.exec_time_table = dict()
        self.resched_time_debt_table = dict()                       # Job uuid -> time debt

    ######################################
    #     Profiling Related Functions    #
    ######################################


    ######################################
    #     Resource Related Functions     #
    ######################################

    def _load_node_configs(self):
        """ Read the json file of cluster spec and parse. """
        node_cfgs = dict()
        cluster_specs = read_json_content(
            # os.path.join(os.path.dirname(os.path.abspath(__file__)), "cluster_spec.json")
            os.path.join(os.path.dirname(os.path.abspath(__file__)), "cluster_spec_32_nodes.json")
        )[0]["gpu_type"]

        for _gpu_type in cluster_specs:
            node_cfgs[_gpu_type] = {
                "node_num": cluster_specs[_gpu_type]["node_num"],
                "ip_addr": cluster_specs[_gpu_type]["ip_addr"],
                "port": cluster_specs[_gpu_type]["port"]
            }
        
        return node_cfgs
    
    def _construct_cluster(self, node_cfgs: dict):
        """ Construct the node resources of the cluster. """
        node_pool = dict()
        node_cnt = 0
        for gpu_type in node_cfgs:
            if gpu_type not in self.existed_gpu_type:
                # Record physical gpu type
                self.existed_gpu_type.append(gpu_type)

            node_num = node_cfgs[gpu_type]["node_num"]
            node_cap = NODE_CAPACITY[gpu_type]
            gpu_specs = {
                "max_mem": gpu_specs_suite[gpu_type].max_mem,
                "max_bw": gpu_specs_suite[gpu_type].max_bw,
                "sm_num": gpu_specs_suite[gpu_type].sm_sum,
            }

            for _i in range(node_num):
                node_cnt += 1
                node_id = str(uuid.uuid1())
                ip_addr = node_cfgs[gpu_type]["ip_addr"][_i]
                port = node_cfgs[gpu_type]["port"][_i]
                # Gpus
                gpu_list = list()
                for _gpu_idx in range(node_cap):
                    _alias = "gpu_" + str(node_cnt).zfill(2) + "_" + str(_gpu_idx + 1).zfill(2)
                    gpu_list.append(
                        GPU(str(uuid.uuid1()), _alias, gpu_type, node_id, gpu_specs)
                    )
                # Node
                node_pool[node_id] = Node(node_id, alias="node_" + str(node_cnt).zfill(2), 
                                          type="none", capacity=NODE_CAPACITY[gpu_type], 
                                          gpu_list=gpu_list, ip_addr=ip_addr, port=port)
        return node_pool

    def _init_resource_alloc_status(self):
        """ Init the resource alloc status based on cluster resources. """
        _table = dict()
        for _nid in self.scheduler.resource_interface.node_pool:
            _table[_nid] = dict()
            for _i in range(self.scheduler.resource_interface.node_pool[_nid].capacity):
                _table[_nid][str(_i)] = IDLE_STATUS
        return _table
    
    ######################################
    #     Scheduler Related Functions    #
    ######################################

    def _parse_sched_decision(self):
        """ 
        Parse the scheduling decision generated by the scheduler and generate a series of operations. 
        There are two types of unit operations:
        - Dispatch operation: Dispatch one job onto target nodes.
        - Suspend operation: Suspend jobs on the target nodes.
        """
        dispatch_operations, suspend_operations = list(), list()

        # # Get current crs table of the scheduler, parse and update
        # crs_table = self.scheduler.resource_interface.get_crs_table()
        # # Format of node_mdf_infos: node uuid -> prev job uuid -> 
        # #                               [[prev gpu status, prev job alias, 
        # #                                new gpu status, new job alias, gpu num] (GPUMdfInfo), ...]
        # _, node_mdf_infos = self.scheduler._parse_crs_table_change(self.prev_crs_table, crs_table, 
        #                                                            self.ended_job_info_table, 
        #                                                            is_runtime=True)
        # self.prev_crs_table = deepcopy(crs_table)
        
        # # Record the uuid of jobs to be suspended/dispatched on each node
        # # Format: Node uuid -> [job uuid 1, job uuid 2, ...]
        # suspend_info_table, dispatch_info_table = dict(), dict()
        
        # # Traverse each node to record suspend/dispatch infos
        # for _node_id in node_mdf_infos:
        #     suspend_info_table[_node_id], dispatch_info_table[_node_id] = list(), list()
        #     for _prev_job_id in node_mdf_infos[_node_id]:
        #         # Records of GPUMdfInfos
        #         gpu_mdf_infos = node_mdf_infos[_node_id][_prev_job_id]
        #         prev_used_job_id = self.scheduler._jau_table[gpu_mdf_infos[0][1]] if gpu_mdf_infos[0][1] != EMPTY_JOB_ALIAS \
        #                                                                     else EMPTY_JOB_ID
        #         assert _prev_job_id == prev_used_job_id

        #         # The following assertion is error, a counter case is: job_1 <- 4 1080ti GPUs, job_2 and job_3 arrive, then:
        #         # (1) job_1 <- 1 a100 GPU; (2) job_2 <- 2 1080ti GPUs; (3) job_3 <- 2 1080ti GPUs
        #         # assert (len(gpu_mdf_infos) == 1) or (prev_used_job_id == EMPTY_JOB_ID)

        #         # Traverse infos to get the changed GPU num of the prev job on this node, 
        #         # in case that the situation above.
        #         changed_gpu_num = 0
        #         for _info in gpu_mdf_infos:
        #             changed_gpu_num += _info[4]

        #         # Traverse mdf infos in case that: 2 idle GPUs -> job_1, 2 idle GPUs -> job_2
        #         for _info in gpu_mdf_infos:
        #             new_used_job_id = self.scheduler._jau_table[_info[3]] if _info[3] != EMPTY_JOB_ALIAS else EMPTY_JOB_ID
                    
        #             # assert prev_used_job_id in self.job_placement_table, \
        #             #     f"Job {self.scheduler.get_job_by_uuid(prev_used_job_id)} should be recorded in placement " + \
        #             #     f"table in the last one scheduling round."
        #             # is_all_removed = (len(self.job_placement_table[prev_used_job_id][_node_id]) 
        #             #                   == changed_gpu_num) if prev_used_job_id != EMPTY_JOB_ID else False
                    
        #             # Whether all GPUs occupied by the prev job are removed
        #             if prev_used_job_id not in self.job_placement_table:
        #                 # This job has been ended or restarted
        #                 if prev_used_job_id != "":
        #                     print(f"[WARN] The target gpu ({self.scheduler.resource_interface.node_pool[_node_id].gpu_type}) is " + 
        #                           f"used by job {prev_used_job_id, self.scheduler.get_job_by_uuid(prev_used_job_id)}, but this job is not recorded " + 
        #                           f"in the job placement table in the last scheduling round.")
        #                 is_all_removed = True
        #             else:
        #                 # Consider both single node and multi nodes cases
        #                 is_all_removed = (len(self.job_placement_table[prev_used_job_id][_node_id]) 
        #                                   == changed_gpu_num) if prev_used_job_id != EMPTY_JOB_ID else False
                    
        #             # Record suspended job
        #             assert prev_used_job_id != FAKE_JOB_ID
        #             if (prev_used_job_id != EMPTY_JOB_ID and 
        #                 prev_used_job_id not in self.ended_job_info_table and 
        #                 prev_used_job_id not in suspend_info_table[_node_id]):
        #                 # Vanilla running job or opportunism job, except for already ended jobs
        #                 suspend_info_table[_node_id].append(prev_used_job_id)
        #                 # Check whether all removed
        #                 if not is_all_removed and prev_used_job_id not in dispatch_info_table[_node_id]:
        #                     # Add to the dispatch table
        #                     dispatch_info_table[_node_id].append(prev_used_job_id)

        #             # Record dispatched job
        #             assert new_used_job_id != FAKE_JOB_ID
        #             if new_used_job_id != EMPTY_JOB_ID and new_used_job_id not in dispatch_info_table[_node_id]:
        #                 # Vanilla running job or opportunism job
        #                 dispatch_info_table[_node_id].append(new_used_job_id)
        #                 # Check whether the new job
        #                 if (new_used_job_id in self.job_placement_table and 
        #                     _node_id in self.job_placement_table[new_used_job_id] and 
        #                     new_used_job_id not in suspend_info_table[_node_id]):
        #                     # This job has already been running on this node (or other nodes) 
        #                     # previously, need to restart it.
        #                     suspend_info_table[_node_id].append(new_used_job_id)
        
        # # Generate operation collection based on recorded suspend/dispatch infos
        # # Need to consider both single node and multi nodes job suspend/dispath.
        
        # # - For suspended case
        # suspend_job_id_buffer = list()
        # for _node_id in suspend_info_table:
        #     for _job_id in suspend_info_table[_node_id]:
        #         if _job_id not in suspend_job_id_buffer:
        #             # Not recorded yet
        #             suspend_job_id_buffer.append(_job_id)
        
        # # - For dispatched case
        # dispatch_job_id_buffer = list()
        # for _node_id in dispatch_info_table:
        #     for _job_id in dispatch_info_table[_node_id]:
        #         if _job_id not in dispatch_job_id_buffer:
        #             # Not recorded yet
        #             dispatch_job_id_buffer.append(_job_id)
        
        # # Analyze and supply generated operations
        # for _op in dispatch_operations:
        #     if (_op.job_id in self.job_placement_table and 
        #         _op.job_id not in suspend_job_id_buffer):
        #         # Rescheduled job and not captured by parsing scheduling decisions.
        #         # For example, job 1 is dispatched on node 1, then in current round,
        #         # node 2 is released and job 1 is scheduled to upscale and occupies
        #         # both node 1 and node 2. In this case, since no gpus of job 1 is 
        #         # reallocated to other jobs or released, job 1 will not be recognized
        #         # as a suspended job.
        #         suspend_job_id_buffer.append(_op.job_id)
        
        # # Generate all suspend operations
        # for _job_id in suspend_job_id_buffer:
        #     suspend_operations.append(
        #         RuntimeOp(type=IS_SUSPEND, job_id=_job_id, 
        #                   node_id_list=self.job_placement_table[_job_id])
        #     )
        
        # # Generate all dispatch operations
        # for _job_id in dispatch_job_id_buffer:
        #     dispatch_operations.append(
        #         RuntimeOp(type=IS_DISPATCH, job_id=_job_id,
        #                   node_id_list=job_to_node_table[_job_id])
        #     )

        print("")
        print("[I][RT] New job resource allocation:")
        for _job in self.scheduler.running_job_queue:
            print(_job.alias, [self.scheduler.resource_interface.node_pool[_nid].alias for _nid in _job.resource_alloc.node_id_list])
        
        print("")

        print("[I][RT] Prev job resource allocation:")
        for _job_id in self.job_placement_table:
            print(self.scheduler.get_job_by_uuid(_job_id).alias, [self.scheduler.resource_interface.node_pool[_nid].alias for _nid in self.job_placement_table[_job_id]])
        
        print("")

        # Generate all suspend operations
        suspend_job_id_buffer = list()
        for _job_id in self.job_placement_table:
            job = self.scheduler.get_job_by_uuid(_job_id)
            if job.status == JOB_COMPLETED_STATUS:
                # This job has been ended, its training process should have been 
                # suspended in _check_job() function.
                continue
            for _node_id in self.job_placement_table[_job_id]:
                if (_job_id not in suspend_job_id_buffer and 
                    (job.resource_alloc is None or 
                     _node_id not in job.resource_alloc.node_id_list or 
                     (len(job.resource_alloc.node_id_list) != 
                      len(self.job_placement_table[job.uuid])) or
                     len(job.resource_alloc.node_to_gpu_table[_node_id]) != 
                     len(self.job_placement_table[job.uuid][_node_id]))):
                    # Cond 1. Allocated resources of the opportunism job is recycled 
                    # Cond 2. Node uuid not in allocated resources of the job
                    # Cond 3. Allocated node num of the job is changed 
                    # Cond 4. Allocated gpu num on this node is changed
                    
                    print(self.scheduler.resource_interface.node_pool[_node_id].alias)
                    if job.resource_alloc is not None:
                        print([self.scheduler.resource_interface.node_pool[_nid].alias for _nid in job.resource_alloc.node_id_list])
                    
                    # This job need to be suspended
                    suspend_operations.append(
                        RuntimeOp(type=IS_SUSPEND, job_id=_job_id, 
                                node_id_list=self.job_placement_table[_job_id])
                    )
                    suspend_job_id_buffer.append(_job_id)
        
        # Generate all dispatch operations
        dispatch_job_id_buffer = list()
        for _job in self.scheduler.running_job_queue:
            for _node_id in _job.resource_alloc.node_id_list:
                if (_job.uuid not in dispatch_job_id_buffer and 
                    (_job.uuid not in self.job_placement_table or 
                     _node_id not in self.job_placement_table[_job.uuid] or 
                     (len(_job.resource_alloc.node_id_list) != 
                      len(self.job_placement_table[_job.uuid])) or
                     (len(_job.resource_alloc.node_to_gpu_table[_node_id]) != 
                      len(self.job_placement_table[_job.uuid][_node_id])))):
                    # Cond 1. New job or newly restarted job
                    # Cond 2. This node is newly allocated to the job
                    # Cond 3. Allocated node num of the job is changed 
                    # Cond 4. Allocated gpu num on this job is changed

                    # This job need to be dispatched
                    dispatch_operations.append(
                        RuntimeOp(type=IS_DISPATCH, job_id=_job.uuid, 
                                node_id_list=_job.resource_alloc.node_id_list)
                    )
                    dispatch_job_id_buffer.append(_job.uuid)

        print("")
        print("[I][RT] Suspend operations:")
        for _op in suspend_operations:
            print(_op.type, 
                  self.scheduler.get_job_by_uuid(_op.job_id).alias, 
                  [self.scheduler.resource_interface.node_pool[_nid].alias for _nid in _op.node_id_list])
        print("")
        print("[I][RT] Dispatch operations:")
        for _op in dispatch_operations:
            print(_op.type, 
                  self.scheduler.get_job_by_uuid(_op.job_id).alias, 
                  [self.scheduler.resource_interface.node_pool[_nid].alias for _nid in _op.node_id_list])
        print("")
        
        return suspend_operations, dispatch_operations

    ######################################
    #      Runtime Related Functions     #
    ######################################

    def _init_port_alloc_status(self):
        """ Init the ray port alloc status. """
        _table = dict()
        for _node_id in self.scheduler.resource_interface.node_pool:
            _table[_node_id] = dict()
            for i in range(len(ray_port_option_list)):
                _table[_node_id][i] = IDLE_STATUS
        return _table

    def _deprecate_job(self, job_id: str):
        """ Remove all runtime records of the job in both scheduler and runtime. """
        job = self.scheduler.get_job_by_uuid(job_id)
        # Send end signal to scheduler
        self._end_job(job_id)
        # # Clear occupied resources
        # crs_table = self.scheduler.resource_interface.get_crs_table()
        # self.scheduler._clear_placement(job_id, crs_table)
        # self.scheduler.resource_interface.apply_sched(crs_table)
        # self.scheduler._clear_placement(job_id, self.prev_crs_table)
        # # Delete previous record in scheduler
        # if job.alias in self.scheduler._jau_table:
        #     self.scheduler._jau_table.pop(job.alias)
        # job.resource_alloc = None
        # Clear records
        assert job_id in self.job_placement_table, \
            f"Job '{job.alias}' should be removed, but not found in job placement table."
        for _node_id in self.job_placement_table[job_id]:
            # Clear GPU rank alloc
            for _gpu_rank in self.job_placement_table[job_id][_node_id]:
                self.resource_alloc_status[_node_id][_gpu_rank] = IDLE_STATUS
            # Clear Ray port alloc
            for _idx in self.port_alloc_status[_node_id]:
                if self.port_alloc_status[_node_id][_idx] == job_id:
                    self.port_alloc_status[_node_id][_idx] = IDLE_STATUS
                    break
        # Clear job try index table
        if job_id in self.job_try_idx_table:
            _ = self.job_try_idx_table.pop(job_id)
        
        # Do not clear

        # # Clear job remained iter num table
        # if job_id in self.remained_iter_num_table:
        #     _ = self.remained_iter_num_table.pop(job_id)
        # else:
        #     print(f"[WARN] Job '{job.alias}' should be removed, but not found in " + 
        #           f"job remained #iter table.")
        
        # Clear job placement table
        _ = self.job_placement_table.pop(job_id)
        # Clear job head node table
        assert job_id in self.job_head_node_table, \
            f"Job '{job.alias}' should be removed, but not found in job head node table."
        _ = self.job_head_node_table.pop(job_id)

    def _resubmit_job(self, job_id: str):
        """ Resubmit the job with runtime error. """
        _job = self.scheduler.get_job_by_uuid(job_id)
        self.resubmitted_job_id_list.append(job_id)
        # Clear occupied resources
        crs_table = self.scheduler.resource_interface.get_crs_table()
        self.scheduler._clear_placement(job_id, crs_table)
        self.scheduler.resource_interface.apply_sched(crs_table)
        self.scheduler._clear_placement(job_id, self.prev_crs_table)
        # Resubmit this job
        assert _job in self.scheduler.running_job_queue, \
            f"A resubmitted job should be in running status before resubmission."
        self.scheduler.running_job_queue.remove(_job)
        if _job in self.scheduler.decision_queue:
            self.scheduler.decision_queue.remove(_job)
        # Update job iter num
        _job.iter_num = _job.remained_iter_num
        # Delete previous record in scheduler
        self.scheduler._jau_table.pop(_job.alias)
        _job.resource_alloc = None
        _job.update_status(JOB_SUBMITTED_STATUS)
        # Clear records
        for _node_id in self.job_placement_table[job_id]:
            # Clear GPU rank alloc
            for _gpu_rank in self.job_placement_table[job_id][_node_id]:
                self.resource_alloc_status[_node_id][_gpu_rank] = IDLE_STATUS
            # Clear Ray port alloc
            for _idx in self.port_alloc_status[_node_id]:
                if self.port_alloc_status[_node_id][_idx] == job_id:
                    self.port_alloc_status[_node_id][_idx] = IDLE_STATUS
                    break
        # Clear job placement table
        _ = self.job_placement_table.pop(job_id)
        _ = self.job_head_node_table.pop(job_id)

        # Do not need to clear remained iter num table, since we do not 
        # reformulate job and direcly resubmit job instead.

        # Resubmit to the scheduler
        self.scheduler.submit_job(_job, _job.sub_time)
        # Resubmitted
        return True

    def _dispatch_job(self, job_id: str, node_to_gpu_table: dict):
        """ Dispatch the training job onto physical nodes. """
        # Job info
        _job = self.scheduler.get_job_by_uuid(job_id)
        assert _job, "Job not found."
        (model_name, param_num) = _job.model_name.split('__')
        # Device info
        node_id_list = list(node_to_gpu_table.keys())
        node_num = len(node_id_list)
        assert _job.resource_quota.gpu_num % node_num == 0, \
            f"Allocated GPU num ({_job.resource_quota.gpu_num}) of job {_job.alias} " + \
            f"cannot be divisible by allocated node num ({node_num})."
        num_devices_per_node = _job.resource_quota.gpu_num // node_num
        # We choose the first node as the head node
        head_node_id = node_id_list[0]
        # Only support homogenous nodes in the same node-to-gpu table
        gpu_type = self.scheduler.resource_interface.node_pool[head_node_id].gpu_type
        # Devices name
        devices_name = f"{node_num}_{gpu_type}"
        # Record
        self.job_head_node_table[job_id] = head_node_id
        self.job_worker_nodes_table[job_id] = node_id_list[1:]

        print("")
        print(f"[I][RT] Dispatching training job '{self.scheduler.get_job_by_uuid(job_id).alias}' on:")
        for _node_id in node_id_list:
            _node = self.scheduler.resource_interface.node_pool[_node_id]
            _type = "Head" if _node_id == head_node_id else "Worker"
            print(f"        - {_type} node: {_node.alias} | IP addr: {_node.ip_addr}")

        # Load generated pruning prompts
        if args.policy == "crius" and job_id not in self.resubmitted_job_id_list:
            prune_prompt = self.tuner.gen_prune_prompt(model_name, param_num, _job.batch_size, 
                                                       node_num, num_devices_per_node)
        else:
            prune_prompt = "none"
        
        # Try index
        try_idx = 1 if job_id not in self.job_try_idx_table else self.job_try_idx_table[job_id] + 1
        self.job_try_idx_table[job_id] = try_idx

        # Ip address, daemon port and ray port of the head node
        head_node_ip_addr = self.scheduler.resource_interface.node_pool[head_node_id].ip_addr
        head_node_daemon_port = self.scheduler.resource_interface.node_pool[head_node_id].port
        head_node_port_dict = None
        # Ip address, daemon port and ray port of worker nodes
        worker_node_ip_addr_table = dict()          # Node uuid -> ip addr
        for _nid in self.job_worker_nodes_table[job_id]:
            worker_node_ip_addr_table[_nid] = self.scheduler.resource_interface.node_pool[_nid].ip_addr
        worker_node_daemon_port_table = dict()      # Node uuid -> daemon port
        for _nid in self.job_worker_nodes_table[job_id]:
            worker_node_daemon_port_table[_nid] = self.scheduler.resource_interface.node_pool[_nid].port
        worker_node_port_dict_table = dict()        # Node uuid -> ray port dict

        # Select ray port for head and worker nodes
        for _node_id in node_to_gpu_table:
            node = self.scheduler.resource_interface.node_pool[_node_id]
            is_head_node = (_node_id == head_node_id)

            # Search the port alloc status to get available port option
            ray_port_dict = None
            assert _node_id in self.port_alloc_status, \
                f"Node uuid of {node.alias} not found in port allocation status table."
            for _idx in self.port_alloc_status[_node_id]:
                if self.port_alloc_status[_node_id][_idx] == IDLE_STATUS:
                    # Apply this port option
                    ray_port_dict = ray_port_option_list[_idx]
                    self.port_alloc_status[_node_id][_idx] = job_id
                    break
            assert ray_port_dict is not None, \
                f"Port selection on node {node.alias} (is_head = {is_head_node}) is failed."
            if is_head_node:
                # Port dict for head node
                head_node_port_dict = ray_port_dict
            else:
                # Port dict for worker node
                worker_node_port_dict_table[_node_id] = ray_port_dict
        
        # Dispatch
        for _node_id in node_to_gpu_table:
            node = self.scheduler.resource_interface.node_pool[_node_id]
            is_head_node = (_node_id == head_node_id)
            # Query url = node ip + daemon port
            ip_addr = node.ip_addr
            port = node.port
            assert ip_addr, \
                f"IP address of the destination node {node.alias} is not properly set."
            url = f"http://{ip_addr}:{port}/query/train"
            print(f"[I][RT] Querying url is: {url}")
            # Worker ip and daemon port
            worker_node_ip_addr = worker_node_ip_addr_table[_node_id] if not is_head_node else None
            worker_node_daemon_port = worker_node_daemon_port_table[_node_id] if not is_head_node else None
            # Ray port dict
            ray_port_dict = head_node_port_dict if is_head_node else worker_node_port_dict_table[_node_id]
            
            # Encapsulate data to bytes
            data = json.dumps({
                "job_id": job_id,
                "try_idx": try_idx,
                "is_head": is_head_node,
                # Node ip and exposed port of daemon process
                "head_node_ip_addr": head_node_ip_addr,
                "head_node_daemon_port": head_node_daemon_port,
                "worker_node_ip_addr": worker_node_ip_addr,
                "worker_node_daemon_port": worker_node_daemon_port,
                # Device info
                "devices_name": devices_name,
                "num_devices_per_node": num_devices_per_node,
                "node_num": node_num,
                # Job info
                "model_name": model_name,
                "param_num": param_num,
                "batch_size": _job.batch_size,
                "iter_num": self.remained_iter_num_table[job_id],
                "prune_prompt": prune_prompt,
                # Visible GPU rank list of this job
                "gpu_rank_list": node_to_gpu_table[_node_id],
                # Ports info of the ray cluster
                "overwrite_network_interface": args.overwrite_net_if,
                "head_node_port": head_node_port_dict["port"],
                "port": ray_port_dict["port"],
                "object_manager_port": ray_port_dict["object_manager_port"],
                "node_manager_port": ray_port_dict["node_manager_port"],
                "ray_client_server_port": ray_port_dict["ray_client_server_port"],
                "min_worker_port": ray_port_dict["min_worker_port"],
                "max_worker_port": ray_port_dict["max_worker_port"]
            })

            print(data)

            # Try send requests to the target node
            _try_cnt = 0
            while _try_cnt < MAX_RETRY_TIMES:
                _try_cnt += 1
                try:
                    if not args.rt_dummy_test:
                        # Request url
                        res = requests.post(url=url, data=data)
                        json_data = json.loads(res.text)
                        print(f"[I][RT] Job: '{_job.alias}' | Try idx: {try_idx} | Return msg is as follows:")
                        print(json_data)
                        self.return_msg_table[_job.alias + "_" + node.alias + "_" + str(try_idx) + "_dph"] = json_data
                    else:
                        # Do not query remote workers, calculate based on profiling data
                        # to simulate runtime orchestration.
                        per_gpu_overhead = RESCHED_OVERHEAD_WITH_PRUNE \
                                                if os.environ.get("SCHED_POLICY", "crius").split("-")[0] == "crius" \
                                                else RESCHED_OVERHEAD_WITHOUT_PRUNE_RT

                        max_tune_time = MAX_RESCHED_OVERHEAD_WITH_PRUNE \
                            if os.environ.get("SCHED_POLICY", "crius").split("-")[0] == "crius" \
                                else MAX_RESCHED_OVERHEAD_WITHOUT_PRUNE_RT
                        self.resched_time_debt_table[job_id] = min(per_gpu_overhead * _job.resource_quota.gpu_num, 
                                                                   max_tune_time)
                        
                        self.exec_time_table[job_id] = 0
                    break
                except Exception as e:
                    print(f"[E] Meet unexpected exception in dispatching job: {e}")
                    traceback.print_exc()
                    if _try_cnt == MAX_RETRY_TIMES:
                        print("[I][RT] Connection failed.")
                        break
                    else:
                        print("[I][RT] Retrying...")
                        sleep(1)
    
    def _suspend_job(self, job_id: str, node_id_list: Sequence[str], 
                     is_suspend_worker_node: bool = False, 
                     end_signal_sended: bool = True):
        """ Suspend the training process onto physical nodes. """
        print("")
        print(f"[I][RT] Suspending running job '{self.scheduler.get_job_by_uuid(job_id).alias}' " + 
              f"(alias) by querying the remote flask server in the worker container...")
        # Job info
        _job = self.scheduler.get_job_by_uuid(job_id)
        assert _job, "Job not found."

        for _node_id in node_id_list:
            node = self.scheduler.resource_interface.node_pool[_node_id]
            # Query url = node ip + daemon port
            ip_addr = node.ip_addr
            port = node.port
            assert ip_addr, \
                f"IP address of the destination node {node.alias} is not properly set."
            url = "http://{}:{}/query/suspend".format(ip_addr, port)
            
            # Encapsulate data and transform from dict to bytes
            if job_id in self.job_head_node_table:
                try_idx = self.job_try_idx_table[job_id]
                is_head_node = (_node_id == self.job_head_node_table[job_id])
            else:
                print(f"[WARN] Job '{_job.alias}' not found in head node table, probably due to " + 
                      f"this is a to-resubmit job (its runtime info has been removed by " + 
                      f"_resubmit_job() before entering this suspending process) or deprecated job " + 
                      f"(training process is hanged due to runtime error (e.g., XLA buffer size " + 
                      f"overflow) that do not stop the process). In this case, " + 
                      f"we takes the first node in node id list as the head node (which is " + 
                      f"default to be true.).")
                try_idx = self.job_try_idx_table[job_id] if job_id in self.job_try_idx_table else -1
                is_head_node = (_node_id == node_id_list[0])
            data = json.dumps(
                {
                    "job_id": job_id, 
                    "try_idx": try_idx, 
                    "is_head": is_head_node
                }
            )

            # Try send requests to the target node
            _try_cnt = 0
            while _try_cnt < MAX_RETRY_TIMES:
                _try_cnt += 1
                try:
                    if not args.rt_dummy_test:
                        # Request url, note that requests.post() is set to block-enabled by default
                        res = requests.post(url=url, data=data)
                        json_data = json.loads(res.text)
                        self.return_msg_table[_job.alias + "_" + node.alias + "_" + str(try_idx) + "_spd"] = json_data
                    else:
                        # Do not query remote workers, calculate based on profiling data
                        # to simulate runtime orchestration.
                        if job_id in self.prev_iter_time_table and self.prev_iter_time_table[job_id] > 0:
                            _iter_time = self.prev_iter_time_table[job_id]
                            _executed_iter_num = self.exec_time_table[job_id] // _iter_time
                        else:
                            _iter_time, _executed_iter_num = 0, 0
                        json_data = {
                            "executed_iter_num": _executed_iter_num,
                            "avg_iter_time": _iter_time,
                            "compilation_time": 0.0,
                            "job_id": job_id,
                        }

                    if is_head_node:
                        # Only display the msg and record end event from the head node
                        executed_iter_num = int(json_data["executed_iter_num"])
                        avg_iter_time = float(json_data["avg_iter_time"])
                        compilation_time = float(json_data["compilation_time"])
                        print(f"[I][RT] A head node of job '{_job.alias}' has been suspended on " + 
                              f"node '{node.alias}' (ip = {node.ip_addr}). Info: | Try idx: {try_idx} | " + 
                              f"Executed iter num: {executed_iter_num} | Avg iter " + 
                              f"time: {avg_iter_time} | Compilation time: {compilation_time} | " + 
                              f"Return msg is as follows:")
                        print(json_data)
                        
                        # Update remained iter num table
                        assert job_id in self.remained_iter_num_table, \
                            f"Job {_job.alias} not found in remained iter num table."
                        self.remained_iter_num_table[job_id] -= executed_iter_num
        
                        # Record end event
                        # FIXME: Here, the info in job object is updated by the scheduler, 
                        # while the node info is on the prev node, so these may be incompatible
                        self.end_event_stream.append(
                            RTEndEvent(IS_END, job_id, _job.alias, _job.model_name, _job.batch_size, self.job_try_idx_table[job_id], 
                                       self.rt_global_timer, node_id_list,
                                       node_alias_list=[self.scheduler.resource_interface.node_pool[_nid].alias for _nid in node_id_list],
                                       gpu_type=_job.resource_quota.gpu_type, gpu_num=_job.resource_quota.gpu_num,
                                       locality=_job.resource_quota.locality, executed_iter_num=executed_iter_num,
                                       remained_iter_num=self.remained_iter_num_table[job_id], 
                                       avg_iter_time=avg_iter_time, compilation_time=compilation_time)
                        )

                        # Check whether ended 
                        if self.remained_iter_num_table[job_id] <= 0 and not end_signal_sended:
                            # Job ended and the end signal is not sended to the scheduler.
                            # Resubmitted job won't execute since its remained iter num is 
                            # larger than 0.
                            self._end_job(job_id)
                    else:
                        print(f"[I][RT] A worker node of job '{_job.alias}' has been suspended " + 
                              f"on node '{node.alias}' (ip = {node.ip_addr}).")
                    
                    break
                except Exception as e:
                    print(f"[E] Meet unexpected exception in suspending jobs: {e}")
                    traceback.print_exc()
                    if _try_cnt == MAX_RETRY_TIMES:
                        print(f"[I][RT] Connection failed. Probably due to runtime OOM error " + 
                              f"caused by XLA buffer size overflow. This job will be directly " + 
                              f"depracated for best-effort avoiding such a crash.")
                        self._end_job(job_id)
                        break
                    else:
                        print("[I][RT] Retrying...")
                        sleep(1)
    
    def _check_job(self, job_id: str, node_id_list: Sequence[str]):
        """ Periodically called to check whether the training job is ended. """
        # Job info
        _job = self.scheduler.get_job_by_uuid(job_id)
        assert _job is not None
        is_job_ended = None     # Whether the job is ended
        # Head node id
        assert job_id in self.job_head_node_table, \
            f"Job {_job.alias} not found in job head node table."
        head_node_id = self.job_head_node_table[job_id]
        assert node_id_list.index(head_node_id) == 0, \
            f"Head node must be the first one in the list."
        # # Head node must be the first one in the list
        # node_id_list.remove(head_node_id)
        # node_id_list = [head_node_id] + node_id_list

        # Query head node to check job status
        head_node = self.scheduler.resource_interface.node_pool[head_node_id]
        
        # Query url = node ip + daemon port
        ip_addr = head_node.ip_addr
        port = head_node.port
        assert ip_addr, \
            f"IP address of the destination node {head_node.alias} is not properly set."
        url = "http://{}:{}/query/check".format(ip_addr, port)
        
        # Encapsulate data and transform from dict to bytes
        try_idx = self.job_try_idx_table[job_id]
        data = json.dumps(
            {
                "job_id": job_id, 
                "try_idx": try_idx, 
            }
        )
        
        # Try send requests to the target node
        _try_cnt = 0
        while _try_cnt < MAX_RETRY_TIMES:
            _try_cnt += 1
            try:
                if not args.rt_dummy_test:
                    # Request url
                    res = requests.post(url=url, data=data)
                    json_data = json.loads(res.text)
                else:
                    # Do not query remote workers, calculate based on profiling data
                    # to simulate runtime orchestration.
                    # Update job time debt or executed iter num
                    if self.resched_time_debt_table[job_id] > 0:
                        # Update time debt, ignoring possible small time piece for execution
                        self.resched_time_debt_table[job_id] -= RUNTIME_CHECK_INTERVAL
                        json_data = {
                            "executed_iter_num": 0,
                            "avg_iter_time": 0.0,
                            "last_iter_time": 0.0,
                            "compilation_time": 0.0,
                            "is_ended": False,
                            "cmd": None,
                            "job_id": job_id,
                            "pid": None,
                            "debug_msg": None
                        }
                    else:
                        # Update iter num
                        _iter_time = self.scheduler._get_iter_time(_job.model_name, _job.batch_size, 
                                                                   _job.resource_quota.gpu_type, 
                                                                   _job.resource_quota.locality)
                        self.exec_time_table[job_id] += RUNTIME_CHECK_INTERVAL
                        _executed_iter_num = self.exec_time_table[job_id] // _iter_time
                        _is_job_ended = _executed_iter_num >= self.remained_iter_num_table[job_id]
                        _avg_iter_time, _last_iter_time = _iter_time, _iter_time
                        json_data = {
                            "executed_iter_num": _executed_iter_num,
                            "avg_iter_time": _avg_iter_time,
                            "last_iter_time": _last_iter_time,
                            "compilation_time": 0.0,
                            "is_ended": _is_job_ended,
                            "cmd": None,
                            "job_id": job_id,
                            "pid": None,
                            "debug_msg": None
                        }

                        # Update previous iter time table
                        if _avg_iter_time > 0:
                            self.prev_iter_time_table[job_id] = _avg_iter_time

                # Check end status and runtime info only in head status
                is_job_ended = bool(json_data["is_ended"])
                # Update job end flag
                executed_iter_num = int(json_data["executed_iter_num"])
                avg_iter_time = float(json_data["avg_iter_time"])
                last_iter_time = float(json_data["last_iter_time"])
                compilation_time = float(json_data["compilation_time"])
                
                if executed_iter_num == 0:
                    # No response
                    if _job.uuid not in self.job_max_check_times_table:
                        self.job_max_check_times_table[_job.uuid] = MAX_CHECK_RESPONSE_TIMES - 1
                    else:
                        self.job_max_check_times_table[_job.uuid] -= 1
                        if self.job_max_check_times_table[_job.uuid] == 0:
                            # Deprecate this job
                            print("-------------------------------")
                            print(f"[WARN] Job Depracated.")
                            print(f"[I][RT] Job: '{_job.alias}' | Try idx: {try_idx} | " + 
                                  f"Allocated nodes: {[self.scheduler.resource_interface.node_pool[_nid].alias for _nid in _job.resource_alloc.node_id_list]} | " +
                                  f"Remained iter num: {self.remained_iter_num_table[_job.uuid]} | "+ 
                                  f"Executed iter num: {executed_iter_num} | " + 
                                  f"Avg iter time: {avg_iter_time} | Compilation time: {compilation_time}.")
                            print("-------------------------------")
                            # self._deprecate_job(job_id)
                            self._end_job(job_id)
                            self._suspend_job(job_id, node_id_list, end_signal_sended=True)
                            return True, 0.0
                else:
                    # Response
                    self.job_max_check_times_table[_job.uuid] = MAX_CHECK_RESPONSE_TIMES

                print(_job.alias)
                
                print(f"[I][RT] Job: '{_job.alias}' | Try idx: {try_idx} | " + 
                      f"Allocated nodes: {[self.scheduler.resource_interface.node_pool[_nid].alias for _nid in _job.resource_alloc.node_id_list]} | " +
                      f"Remained iter num: {self.remained_iter_num_table[_job.uuid]} | "+ 
                      f"Executed iter num: {executed_iter_num} | " + 
                      f"Avg iter time: {avg_iter_time} | Compilation time: {compilation_time} | " + 
                      f"Return msg is as follows:")
                print(json_data)

                # Calculate thr
                thr = round(_job.batch_size / last_iter_time, PREC) if _job and last_iter_time > 0.0 else 0.0
                
                if is_job_ended:
                    # Job end
                    self.return_msg_table[_job.alias + '_' + head_node.alias + '_' + str(try_idx) + '_c'] = json_data
                    # Update remained iter num table
                    assert job_id in self.remained_iter_num_table, \
                        f"Job {_job.alias} not found in remained iter num table."
                    # Record end event
                    is_resubmit = False
                    if self.remained_iter_num_table[job_id] > executed_iter_num:
                        # In this case, the training job may occur into (1) 'no solution' error due to OOM or 
                        # (2) runtime OOM error (executed iter num > 0).
                        end_event = RTEndEvent(IS_ERROR, job_id, _job.alias, _job.model_name, 
                                               _job.batch_size, self.job_try_idx_table[job_id], 
                                               self.rt_global_timer, node_id_list,
                                               node_alias_list=[
                                                    self.scheduler.resource_interface.node_pool[_node_id].alias 
                                                        for _node_id in node_id_list
                                                ],
                                               gpu_type=_job.resource_quota.gpu_type, 
                                               gpu_num=_job.resource_quota.gpu_num,
                                               locality=_job.resource_quota.locality, 
                                               executed_iter_num=executed_iter_num,
                                               remained_iter_num=self.remained_iter_num_table[job_id], 
                                               avg_iter_time=avg_iter_time, 
                                               compilation_time=compilation_time)
                        # We should re-submit this job
                        if job_id not in self.resubmitted_job_id_list:
                            print(f"[I][RT] Resubmitting job '{_job.alias}'...")
                            is_resubmit = self._resubmit_job(job_id)
                    else:
                        # Normal end job
                        end_event = RTEndEvent(IS_END, job_id, _job.alias, _job.model_name, 
                                               _job.batch_size, self.job_try_idx_table[job_id], 
                                               self.rt_global_timer, node_id_list,
                                               node_alias_list=[
                                                   self.scheduler.resource_interface.node_pool[_node_id].alias 
                                                        for _node_id in node_id_list
                                               ],
                                               gpu_type=_job.resource_quota.gpu_type, 
                                               gpu_num=_job.resource_quota.gpu_num,
                                               locality=_job.resource_quota.locality, 
                                               executed_iter_num=executed_iter_num,
                                               remained_iter_num=0, 
                                               avg_iter_time=avg_iter_time, 
                                               compilation_time=compilation_time)
                    
                    self.end_event_stream.append(end_event)
                    if not is_resubmit:
                        # Job ended and not resubmitted, otherwise job queues in the 
                        # scheduler has been updated in _resubmit_job().
                        self._end_job(job_id)
                        # Suspend on all workers without sending end signals to the 
                        # scheduler.

                    self._suspend_job(job_id, node_id_list, end_signal_sended=True)
                
                # Return whether ended
                return is_job_ended, thr
            except Exception as e:
                print(f"[E] Meet unexpected exception in checking job status: {e}")
                traceback.print_exc()
                if _try_cnt == MAX_RETRY_TIMES:
                    print("[I][RT] Connection failed.")
                    break
                else:
                    print("[I][RT] Retrying...")
                    sleep(1)

    def _end_job(self, job_id: str):
        """ Send ending signal to the cluster scheduler. """
        # Update job status in the scheduler
        job = self.scheduler.get_job_by_uuid(job_id)
        if job.status == JOB_RUNNING_STATUS:
            self.scheduler._end_job(job)

    ######################################
    #       Job Related Functions        #
    ######################################

    def _formulate_job(self, event: ArrivalEvent):
        """ Formulate job object based on the event. """
        assert event.job_gpu_type in self.existed_gpu_type, \
            f"Required GPU type {event.job_gpu_type} is not existed in the cluster " + \
            f"(Available GPU types: {self.existed_gpu_type})."
        # Job info
        _job_id = event.job_id
        _job_alias = event.job_alias
        _resource_quota = ResourceQuota(_job_id, event.job_gpu_num, event.job_gpu_type)
        _deadline = event.deadline if args.policy != "elasticflow-l" else LOOSEN_DEADLINE
        # Record remained iter num
        self.remained_iter_num_table[_job_id] = event.iter_num

        return Job(
            {
                "job_id": _job_id,
                "alias": _job_alias,
                "user_id": "default_user",
                "vc_id": "default_vc",
                "sub_time": event.sub_time,
                "deadline": _deadline,
                "iter_num": event.iter_num, 
                "resource_quota": _resource_quota,
                "model_name": event.model_name,
                "batch_size": event.batch_size,
            }
        )
    
    ######################################
    #        Top-level Functions         #
    ######################################

    def _check_status(self):
        """ Check runtime status of all running jobs. """
        # No need to re-schedule, need to check the runtime status of all running jobs 
        job_id_list = [_job.uuid for _job in self.scheduler.running_job_queue]
        # Cluster throughput 
        cluster_thr = 0.0
        for _job_id in job_id_list:
            job = self.scheduler.get_job_by_uuid(_job_id)
            is_ended, thr = crius_runtime._check_job(_job_id, list(self.job_placement_table[_job_id].keys()))
            # Update thr
            cluster_thr += thr
            if is_ended:
                if thr > 0:
                    # Ended
                    print(f"[I][RT] Job '{self.scheduler.get_job_by_uuid(_job_id).alias}' has been ended.")
                    # Update jct table
                    self.jct_table[_job_id] = [job.alias, self.rt_global_timer - job.sub_time]
                else:
                    # Deprecated
                    print(f"[I][RT] Job '{self.scheduler.get_job_by_uuid(_job_id).alias}' has been deprecated.")
        
        # Record cluster thr
        self.thr_list.append(cluster_thr)
        # Update queuing time table
        for _job in self.scheduler.pending_job_queue:
            if _job.uuid not in self.queuing_time_table:
                self.queuing_time_table[_job.uuid] = RUNTIME_CHECK_INTERVAL
            else:
                self.queuing_time_table[_job.uuid] += RUNTIME_CHECK_INTERVAL
        
        # Save thr list
        _file_path = f"{CUR_PATH}/{args.plot_dir}/{args.policy}_thr.npy"
        np.save(_file_path, self.thr_list)

        # Save jct table
        _json_path = f"{CUR_PATH}/{args.plot_dir}/{args.policy}_jct.json"
        save_as_json(_json_path, [self.jct_table])

        # Save queuing time table
        _json_path = f"{CUR_PATH}/{args.plot_dir}/{args.policy}_queuing_time.json"
        save_as_json(_json_path, [self.queuing_time_table])

        # Save end event stream
        json_list = list()
        for _event in self.end_event_stream:
            json_list.append(translate_namedtuple_to_dict(_event))
        _json_path = f"{CUR_PATH}/{args.plot_dir}/{args.policy}_end_event.json"
        save_as_json(_json_path, json_list)

    def _schedule(self):
        """ Runtime schedule function. """
        # Step 1. Call scheduler
        if args.policy.split("-")[0] != "elasticflow":
            self.ended_job_info_table = self.scheduler.runtime_schedule()
        else:
            self.ended_job_info_table = self.scheduler.runtime_schedule(self.rt_global_timer)

        # Step 2. Parse scheduling decision
        suspend_operations, dispatch_operations = self._parse_sched_decision()

        # Step 3. Remove all records of the ended jobs since their occupying info has been removed
        for _job_id in list(self.ended_job_info_table.keys()):
            job = self.scheduler.get_job_by_uuid(_job_id)
            if not job:
                continue

            print(f"[I][RT] Ended job {job.alias} is removed from runtime records.")
            if _job_id not in self.job_placement_table:
                print(f"[WARN] Ended job {job.alias} is not found in job placement table, " + 
                      f"this should not happened.")
                continue
            
            for _node_id in self.job_placement_table[_job_id]:
                # Clear GPU rank alloc
                for _gpu_rank in self.job_placement_table[_job_id][_node_id]:
                    self.resource_alloc_status[_node_id][_gpu_rank] = IDLE_STATUS
                # Clear Ray port alloc
                for _idx in self.port_alloc_status[_node_id]:
                    if self.port_alloc_status[_node_id][_idx] == _job_id:
                        self.port_alloc_status[_node_id][_idx] = IDLE_STATUS
                        break
            # Clear job records
            if _job_id in self.job_try_idx_table:
                # Try index table
                _ = self.job_try_idx_table.pop(_job_id)
            if _job_id in self.remained_iter_num_table:
                # Remained iter num table
                _ = self.remained_iter_num_table.pop(_job_id)
            if _job_id in self.job_placement_table:
                # Job placement table
                _ = self.job_placement_table.pop(_job_id)    
            if _job_id in self.job_head_node_table:
                # Job head node table
                _ = self.job_head_node_table.pop(_job_id)
        
        # Step 4. Perform all suspend operations (can't be an ended job) in a block-style
        for _op in suspend_operations:
            assert _op.type == IS_SUSPEND, f"Mismatched operation type: {_op.type} <-> {IS_SUSPEND}."
            _job = self.scheduler.get_job_by_uuid(_op.job_id)
            assert _job, f"Job not found. If this job is ended and cleared from ended job queue " + \
                         f"(in runtime_schedule()), it should be still registered in " + \
                         f"scheduler.job_registry until removed in the next scheduling round " + \
                         f"in _release_resources_and_update_decision_queue()."
            # Allocated node id list
            node_id_list = _op.node_id_list
            # Add all allocated nodes of this job
            for _node_id in self.job_placement_table[_op.job_id]:
                if _node_id not in node_id_list:
                    node_id_list.append(_node_id)
            # Launch the suspend function, block this main process until the target 
            # remote training process is killed.
            self._suspend_job(_op.job_id, node_id_list)

            # Release rank occupation
            assert _op.job_id in self.job_placement_table, \
                "Job not found in placement table."
            for _node_id in node_id_list:
                for _gpu_rank in self.job_placement_table[_op.job_id][_node_id]:
                    self.resource_alloc_status[_node_id][_gpu_rank] = IDLE_STATUS
            # Clear job placement table
            _ = self.job_placement_table.pop(_op.job_id)

            # Release port occupation
            for _node_id in node_id_list:
                for _idx in self.port_alloc_status[_node_id]:
                    if self.port_alloc_status[_node_id][_idx] == _op.job_id:
                        self.port_alloc_status[_node_id][_idx] = IDLE_STATUS
                        break
        
        # Step 5. Perform all dispatch operations in a non-block style
        for _op in dispatch_operations:
            assert _op.type == IS_DISPATCH, f"Mismatched operation type: {_op.type} <-> {IS_DISPATCH}."
            _job = self.scheduler.get_job_by_uuid(_op.job_id)
            assert _job, "Job not found."
            # Node-GPU mapping table
            node_to_gpu_table = dict()

            for _node_id in _op.node_id_list:
                # Per-gpu quota
                pn_quota = _job.resource_quota.gpu_num // len(_op.node_id_list)
                # Search the node to get enough gpu ranks to allocate
                gpu_rank_list = list()
                assert _node_id in self.resource_alloc_status, \
                    f"Node uuid not found in resource alloc status table."
                for _gpu_rank in self.resource_alloc_status[_node_id]:
                    if (self.resource_alloc_status[_node_id][_gpu_rank] == IDLE_STATUS or 
                        (_op.job_id in self.job_placement_table and 
                         _node_id in self.job_placement_table[_op.job_id] and 
                         _gpu_rank in self.job_placement_table[_op.job_id][_node_id])):
                        # Allocable gpu rank
                        gpu_rank_list.append(_gpu_rank)
                        self.resource_alloc_status[_node_id][_gpu_rank] = USED_STATUS
                        # Check if enough
                        if len(gpu_rank_list) == pn_quota:
                            break
                assert len(gpu_rank_list) == pn_quota, \
                    f"Required GPU num ({pn_quota}) is not sufficient on the " + \
                    f"target node {self.scheduler.resource_interface.node_pool[_node_id].alias}."
                # Record
                node_to_gpu_table[_node_id] = gpu_rank_list
            
            # Update job placement table
            if _op.job_id not in self.job_placement_table:
                self.job_placement_table[_op.job_id] = dict()
            self.job_placement_table[_op.job_id] = node_to_gpu_table

            # Launch the dispatch function, do not block the main process
            self._dispatch_job(_op.job_id, node_to_gpu_table)
        
        # Step 6. Clear ended job info table
        self.ended_job_info_table = dict()
    
    def _shutdown(self):
        """ Functions when Crius system is shutdown. """
        # Save thr list
        _file_path = f"{CUR_PATH}/{args.plot_dir}/{args.policy}_thr.npy"
        np.save(_file_path, self.thr_list)

        # Save jct table
        _json_path = f"{CUR_PATH}/{args.plot_dir}/{args.policy}_jct.json"
        save_as_json(_json_path, [self.jct_table])

        # Save queuing time table
        _json_path = f"{CUR_PATH}/{args.plot_dir}/{args.policy}_queuing_time.json"
        save_as_json(_json_path, [self.queuing_time_table])

        # Save end event stream
        json_list = list()
        for _event in self.end_event_stream:
            json_list.append(translate_namedtuple_to_dict(_event))
        _json_path = f"{CUR_PATH}/{args.plot_dir}/{args.policy}_end_event.json"
        save_as_json(_json_path, json_list)
        
        # Since the system is shutdown, suspend all running jobs
        _job_id_list = [_job.uuid for _job in self.scheduler.running_job_queue]
        for _job_id in _job_id_list:
            node_id_list=list(self.job_placement_table[_job_id].keys())
            # Launch the suspend function, block this main process until 
            # the target remote training process is killed.
            self._suspend_job(_job_id, node_id_list)
            # Release rank occupation
            assert _job_id in self.job_placement_table, \
                "Job uuid not found in job placement table."
            for _node_id in node_id_list:
                for _gpu_rank in self.job_placement_table[_job_id][_node_id]:
                    self.resource_alloc_status[_node_id][_gpu_rank] = IDLE_STATUS
            # Clear job placement table
            _ = self.job_placement_table.pop(_job_id)
            # Release port occupation
            for _node_id in node_id_list:
                for _idx in self.port_alloc_status[_node_id]:
                    if self.port_alloc_status[_node_id][_idx] == _job_id:
                        self.port_alloc_status[_node_id][_idx] = IDLE_STATUS
            print(f"[I][RT] Job '{self.scheduler.get_job_by_uuid(_job_id).alias}' has been ended on:")
            for _node_id in node_id_list:
                print(" - Node alias:", self.scheduler.resource_interface.node_pool[_node_id].alias)

    def run(self):
        """ The entrypoint to run the Crius runtime system. """
        # Job arrival workload
        arrival_trace = self.trace_manager.load_trace(args.trace_name, is_runtime=True)
        # Init global timer
        init_timestamp = arrival_trace[0].sub_time + RUNTIME_SCHEDULING_INTERVAL
        self.rt_global_timer = init_timestamp
        # Job cnt
        global_job_cnt = 0
        # Scheduling round index
        sched_rnd_idx = 0
        # Check idx in one round
        chk_idx = 0
        # Is system shutdown
        is_system_shutdown = False

        print("[I][RT] Cluster resources:")
        for _node_id in self.scheduler.resource_interface.node_pool:
            node = self.scheduler.resource_interface.node_pool[_node_id]
            print(f"        - Node alias: {node.alias} | Node ip addr: {node.ip_addr} | " + 
                  f"Node daemon port: {node.port} | GPU type: {node.gpu_type} | " + 
                  f"Node capacity: {node.capacity}")
        
        # Main loop
        while not is_system_shutdown:
            # Submit jobs
            while (global_job_cnt < len(arrival_trace) and 
                   self.rt_global_timer >= arrival_trace[global_job_cnt].sub_time):
                if global_job_cnt < len(arrival_trace):
                    # Formulate job object and submit
                    _job = self._formulate_job(arrival_trace[global_job_cnt])
                    self.scheduler.submit_job(_job, _job.sub_time)
                    # Update job cnt
                    global_job_cnt += 1
                    print("")
                    print(f"[I][RT] Job '{_job.alias}' submitted | Timestamp: {_job.sub_time} | " + 
                          f"Model type: {_job.model_name}")

            # Attempt re-schedule or check status
            if (self.rt_global_timer - init_timestamp) % RUNTIME_SCHEDULING_INTERVAL == 0:
                # Update scheduling round idx
                sched_rnd_idx += 1
                chk_idx = 0
                # Need Schedule
                print("")
                print("")
                print("############################################################")
                print("#          Event-stream scheduling round:  %-05s           #" % (sched_rnd_idx))
                print("############################################################")
                # Schedule
                self._schedule()

                # Exit point 
                if (sched_rnd_idx > args.max_sched_round or 
                    (global_job_cnt == len(arrival_trace) and 
                     len(self.scheduler.submit_init_job_queue) == 0 and 
                     len(self.scheduler.running_job_queue) == 0)):
                    # Exit the scheduling system when there is no new job or running job
                    is_system_shutdown = True
                    continue
            
            elif (self.rt_global_timer - init_timestamp) % RUNTIME_CHECK_INTERVAL == 0:
                # Update check idx
                chk_idx += 1
                print("\n")
                print("-----------------------------------------------------------------------")
                print(f"[I][RT] Checking runtime status of all running jobs " + 
                      f"(Sched round idx: {sched_rnd_idx} / {args.max_sched_round}  | " + 
                      f"Check idx: {chk_idx} / {RUNTIME_SCHEDULING_INTERVAL // RUNTIME_CHECK_INTERVAL}).")
                self._check_status()
            
            if not args.rt_dummy_test:
                # Sleep for RUNTIME_CHECK_INTERVAL sec, ignoring the operation cost
                sleep(RUNTIME_CHECK_INTERVAL)
            else:
                sleep(0.1)
            # Update global timer
            self.rt_global_timer += RUNTIME_CHECK_INTERVAL
        
        print("")
        print("")
        print("############################################################")
        print("#                 Crius system is shutdown                 #")
        print("############################################################")
        # System shutdown
        self._shutdown()


if __name__ == "__main__":
    if not args.visual:
        # Crius runtime
        crius_runtime = CriusRuntime()
        # Run
        crius_runtime.run()
        # # Run test
        # crius_runtime.run_test()
    else:
        if args.visualized_metric == "thr":
            plot_cluster_thr(work_dir=f'./runtime/{args.plot_dir}')
        elif args.visualized_metric == "queuing_time":
            # Visualize the queuing time
            plot_queuing_time(f"./runtime/{args.plot_dir}")
        elif args.visualized_metric == "jct":
            # Visualize and cal jct
            plot_jct(work_dir=f"./runtime/{args.plot_dir}")
        else:
            NotImplementedError()
